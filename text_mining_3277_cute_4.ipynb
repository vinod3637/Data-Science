{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text from 'news.ycombinator.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary library's for  text mining\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install urllib\n",
    "import urllib.request as url \n",
    "\n",
    "#!pip install bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_web1(link):\n",
    "    htm = url.urlopen(link).read()\n",
    "    soup=bs(htm,'html.parser')\n",
    "    web = {'title':[],'url1':[], 'domain':[],\"votes\":[],\"comments\":[],\"user\":[],\"date\":[]}\n",
    "    for link in soup.findAll(\"td\",class_=\"title\")[1::2]:\n",
    "        try:\n",
    "            web[\"title\"].append(link.a.text)\n",
    "        except:\n",
    "            web[\"title\"].append(\"missing\")\n",
    "        try:\n",
    "            web[\"url1\"].append(link.a[\"href\"])\n",
    "        except:\n",
    "            web[\"url1\"].append(\"missing\")\n",
    "        try:\n",
    "            web[\"domain\"].append(link.span.text.replace('(',\"\").replace(')',''))\n",
    "        except:\n",
    "            web[\"domain\"].append(\"missing\")\n",
    "\n",
    "    for link in soup.findAll(\"td\",class_=\"subtext\"):\n",
    "        try:\n",
    "            web[\"votes\"].append(link.span.text)\n",
    "        except:\n",
    "            web[\"votes\"].append(\"missing\")\n",
    "        try:\n",
    "            web[\"comments\"].append(link.findAll(\"a\")[-1].text)\n",
    "        except:\n",
    "            web[\"comments\"].append(\"missing\")\n",
    "        try:\n",
    "            web[\"user\"].append(link.a.text)\n",
    "        except:\n",
    "            web[\"user\"].append(\"missing\")\n",
    "        try:\n",
    "            web[\"date\"].append(link.findAll(\"span\")[1].text)\n",
    "        except:\n",
    "            web[\"date\"].append(\"missing\")\n",
    "        \n",
    "    print(len(web[\"title\"]))\n",
    "    print(len(web[\"url1\"]))\n",
    "    print(len(web[\"domain\"]))\n",
    "    print(len(web[\"votes\"]))\n",
    "    print(len(web[\"date\"]))\n",
    "    print(len(web[\"comments\"]))\n",
    "    print(len(web[\"user\"]))\n",
    "\n",
    "            \n",
    "    blog_list1 = pd.DataFrame(web)\n",
    "    return blog_list1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html=\"https://news.ycombinator.com/front\"\n",
    "# a=extract_web1(html)\n",
    "# html1=\"https://news.ycombinator.com/front?day=2019-12-09&p=2\"\n",
    "# b=extract_web1(html1)\n",
    "# html2=\"https://news.ycombinator.com/front?day=2019-12-09&p=3\"\n",
    "# c=extract_web1(html2)\n",
    "# html3=\"https://news.ycombinator.com/front?day=2019-12-09&p=4\"\n",
    "# d=extract_web1(html3)\n",
    "# html4=\"https://news.ycombinator.com/front?day=2019-12-08\"\n",
    "# e=extract_web1(html4)\n",
    "# html5=\"https://news.ycombinator.com/front?day=2019-12-08&p=2\"\n",
    "# f=extract_web1(html5)\n",
    "# html6=\"https://news.ycombinator.com/front?day=2019-12-08&p=3\"\n",
    "# g=extract_web1(html6)\n",
    "# html7=\"https://news.ycombinator.com/front?day=2019-12-07\"\n",
    "# h=extract_web1(html7)\n",
    "# html8=\"https://news.ycombinator.com/front?day=2019-12-07&p=2\"\n",
    "# j=extract_web1(html8)\n",
    "# html9=\"https://news.ycombinator.com/front?day=2019-12-07&p=3\"\n",
    "# k=extract_web1(html9)\n",
    "# html10=\"https://news.ycombinator.com/front?day=2019-12-06\"\n",
    "# l=extract_web1(html10)\n",
    "# html11=\"https://news.ycombinator.com/front?day=2019-12-06&p=2\"\n",
    "# m=extract_web1(html11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=['https://news.ycombinator.com/newest','https://news.ycombinator.com/news?p=2','https://news.ycombinator.com/news?p=3','https://news.ycombinator.com/news?p=4','https://news.ycombinator.com/news?p=5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for i in links:\n",
    "    a.append(extract_web1(i))\n",
    "bigdata1=pd.concat(a,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url1</th>\n",
       "      <th>domain</th>\n",
       "      <th>votes</th>\n",
       "      <th>comments</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graph Drawing</td>\n",
       "      <td>http://graphdrawing.org</td>\n",
       "      <td>graphdrawing.org</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>mpiedrav</td>\n",
       "      <td>1 minute ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automation trends for 2020</td>\n",
       "      <td>https://www.thinkautomation.com/just-for-fun/a...</td>\n",
       "      <td>thinkautomation.com</td>\n",
       "      <td>2 points</td>\n",
       "      <td>discuss</td>\n",
       "      <td>roxyabercrombie</td>\n",
       "      <td>6 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Return to Venus and What It Means for Earth</td>\n",
       "      <td>https://www.nasa.gov/feature/jpl/the-return-to...</td>\n",
       "      <td>nasa.gov</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>jelliclesfarm</td>\n",
       "      <td>8 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From iPhone to NT Authority\\System</td>\n",
       "      <td>https://decoder.cloud/2019/12/12/from-iphone-t...</td>\n",
       "      <td>decoder.cloud</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>miles</td>\n",
       "      <td>12 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Statistical paradoxes for sharping your Bayesi...</td>\n",
       "      <td>https://gfrison.com/2019/statistical-puzzle-ba...</td>\n",
       "      <td>gfrison.com</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>gfrison1</td>\n",
       "      <td>12 minutes ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Boeing removed lightning protection from 787 a...</td>\n",
       "      <td>https://www.businessinsider.com/boeing-removed...</td>\n",
       "      <td>businessinsider.com</td>\n",
       "      <td>22 points</td>\n",
       "      <td>6 comments</td>\n",
       "      <td>toomuchtodo</td>\n",
       "      <td>12 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>A Rocket to Nowhere (2005)</td>\n",
       "      <td>https://idlewords.com/2005/08/a_rocket_to_nowh...</td>\n",
       "      <td>idlewords.com</td>\n",
       "      <td>80 points</td>\n",
       "      <td>14 comments</td>\n",
       "      <td>kick</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Ask HN: 2019 Winter Reading List?</td>\n",
       "      <td>item?id=21777109</td>\n",
       "      <td>missing</td>\n",
       "      <td>12 points</td>\n",
       "      <td>6 comments</td>\n",
       "      <td>obilgic</td>\n",
       "      <td>11 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Rambler (Sberbank) Claims Nginx (F5 Networks) ...</td>\n",
       "      <td>https://twitter.com/AntNesterov/statuses/12050...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>10 points</td>\n",
       "      <td>discuss</td>\n",
       "      <td>koustubhs</td>\n",
       "      <td>12 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Founder came back after 8 years to rewrite Fla...</td>\n",
       "      <td>http://pixlr.com/e</td>\n",
       "      <td>pixlr.com</td>\n",
       "      <td>311 points</td>\n",
       "      <td>92 comments</td>\n",
       "      <td>poniko</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                                        Graph Drawing   \n",
       "1                           Automation trends for 2020   \n",
       "2      The Return to Venus and What It Means for Earth   \n",
       "3                   From iPhone to NT Authority\\System   \n",
       "4    Statistical paradoxes for sharping your Bayesi...   \n",
       "..                                                 ...   \n",
       "175  Boeing removed lightning protection from 787 a...   \n",
       "176                         A Rocket to Nowhere (2005)   \n",
       "177                  Ask HN: 2019 Winter Reading List?   \n",
       "178  Rambler (Sberbank) Claims Nginx (F5 Networks) ...   \n",
       "179  Founder came back after 8 years to rewrite Fla...   \n",
       "\n",
       "                                                  url1                domain  \\\n",
       "0                              http://graphdrawing.org      graphdrawing.org   \n",
       "1    https://www.thinkautomation.com/just-for-fun/a...   thinkautomation.com   \n",
       "2    https://www.nasa.gov/feature/jpl/the-return-to...              nasa.gov   \n",
       "3    https://decoder.cloud/2019/12/12/from-iphone-t...         decoder.cloud   \n",
       "4    https://gfrison.com/2019/statistical-puzzle-ba...           gfrison.com   \n",
       "..                                                 ...                   ...   \n",
       "175  https://www.businessinsider.com/boeing-removed...   businessinsider.com   \n",
       "176  https://idlewords.com/2005/08/a_rocket_to_nowh...         idlewords.com   \n",
       "177                                   item?id=21777109               missing   \n",
       "178  https://twitter.com/AntNesterov/statuses/12050...           twitter.com   \n",
       "179                                 http://pixlr.com/e             pixlr.com   \n",
       "\n",
       "          votes     comments             user            date  \n",
       "0       1 point      discuss         mpiedrav    1 minute ago  \n",
       "1      2 points      discuss  roxyabercrombie   6 minutes ago  \n",
       "2       1 point      discuss    jelliclesfarm   8 minutes ago  \n",
       "3       1 point      discuss            miles  12 minutes ago  \n",
       "4       1 point      discuss         gfrison1  12 minutes ago  \n",
       "..          ...          ...              ...             ...  \n",
       "175   22 points   6 comments      toomuchtodo    12 hours ago  \n",
       "176   80 points  14 comments             kick       1 day ago  \n",
       "177   12 points   6 comments          obilgic    11 hours ago  \n",
       "178   10 points      discuss        koustubhs    12 hours ago  \n",
       "179  311 points  92 comments           poniko      2 days ago  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdata = pd.concat([a,b,c,d,e,f,g,h,j,k,l,m], ignore_index=True, sort =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdata1=pd.concat([a,b,c,d,e,f,g,h,j], ignore_index=True, sort =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exctracing the text from every blog link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article(link):\n",
    "    body=[]\n",
    "    try:\n",
    "        html = url.urlopen(link).read()\n",
    "        soup = bs(html, 'html.parser')\n",
    "        for i in soup.find_all('p'):\n",
    "            try:\n",
    "                body.append(i.text)\n",
    "            except:\n",
    "                body.append(\"missing\")\n",
    "        body_str= ' '.join(body)\n",
    "        return(body_str)\n",
    "    except:\n",
    "        fa = \"no text Available\"\n",
    "        body.append(fa)\n",
    "        return(body[0])\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdata['body'] =bigdata['url1'].apply(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "bigdata1['body'] =bigdata1['url1'].apply(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url1</th>\n",
       "      <th>domain</th>\n",
       "      <th>votes</th>\n",
       "      <th>comments</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graph Drawing</td>\n",
       "      <td>http://graphdrawing.org</td>\n",
       "      <td>graphdrawing.org</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>mpiedrav</td>\n",
       "      <td>1 minute ago</td>\n",
       "      <td>\\nGraph Drawing is concerned with the geometri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automation trends for 2020</td>\n",
       "      <td>https://www.thinkautomation.com/just-for-fun/a...</td>\n",
       "      <td>thinkautomation.com</td>\n",
       "      <td>2 points</td>\n",
       "      <td>discuss</td>\n",
       "      <td>roxyabercrombie</td>\n",
       "      <td>6 minutes ago</td>\n",
       "      <td>2019 saw automation continue its march into of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Return to Venus and What It Means for Earth</td>\n",
       "      <td>https://www.nasa.gov/feature/jpl/the-return-to...</td>\n",
       "      <td>nasa.gov</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>jelliclesfarm</td>\n",
       "      <td>8 minutes ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From iPhone to NT Authority\\System</td>\n",
       "      <td>https://decoder.cloud/2019/12/12/from-iphone-t...</td>\n",
       "      <td>decoder.cloud</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>miles</td>\n",
       "      <td>12 minutes ago</td>\n",
       "      <td>As promised in my previous post , I will show ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Statistical paradoxes for sharping your Bayesi...</td>\n",
       "      <td>https://gfrison.com/2019/statistical-puzzle-ba...</td>\n",
       "      <td>gfrison.com</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>gfrison1</td>\n",
       "      <td>12 minutes ago</td>\n",
       "      <td>no text Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Boeing removed lightning protection from 787 a...</td>\n",
       "      <td>https://www.businessinsider.com/boeing-removed...</td>\n",
       "      <td>businessinsider.com</td>\n",
       "      <td>22 points</td>\n",
       "      <td>6 comments</td>\n",
       "      <td>toomuchtodo</td>\n",
       "      <td>12 hours ago</td>\n",
       "      <td>no text Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>A Rocket to Nowhere (2005)</td>\n",
       "      <td>https://idlewords.com/2005/08/a_rocket_to_nowh...</td>\n",
       "      <td>idlewords.com</td>\n",
       "      <td>80 points</td>\n",
       "      <td>14 comments</td>\n",
       "      <td>kick</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>A Rocket To Nowhere \\n\\n\\n\\r\\nThe Space Shuttl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Ask HN: 2019 Winter Reading List?</td>\n",
       "      <td>item?id=21777109</td>\n",
       "      <td>missing</td>\n",
       "      <td>12 points</td>\n",
       "      <td>6 comments</td>\n",
       "      <td>obilgic</td>\n",
       "      <td>11 hours ago</td>\n",
       "      <td>no text Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Rambler (Sberbank) Claims Nginx (F5 Networks) ...</td>\n",
       "      <td>https://twitter.com/AntNesterov/statuses/12050...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>10 points</td>\n",
       "      <td>discuss</td>\n",
       "      <td>koustubhs</td>\n",
       "      <td>12 hours ago</td>\n",
       "      <td>We've detected that JavaScript is disabled in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Founder came back after 8 years to rewrite Fla...</td>\n",
       "      <td>http://pixlr.com/e</td>\n",
       "      <td>pixlr.com</td>\n",
       "      <td>311 points</td>\n",
       "      <td>92 comments</td>\n",
       "      <td>poniko</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>You have no images in your history, to start e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                                        Graph Drawing   \n",
       "1                           Automation trends for 2020   \n",
       "2      The Return to Venus and What It Means for Earth   \n",
       "3                   From iPhone to NT Authority\\System   \n",
       "4    Statistical paradoxes for sharping your Bayesi...   \n",
       "..                                                 ...   \n",
       "175  Boeing removed lightning protection from 787 a...   \n",
       "176                         A Rocket to Nowhere (2005)   \n",
       "177                  Ask HN: 2019 Winter Reading List?   \n",
       "178  Rambler (Sberbank) Claims Nginx (F5 Networks) ...   \n",
       "179  Founder came back after 8 years to rewrite Fla...   \n",
       "\n",
       "                                                  url1                domain  \\\n",
       "0                              http://graphdrawing.org      graphdrawing.org   \n",
       "1    https://www.thinkautomation.com/just-for-fun/a...   thinkautomation.com   \n",
       "2    https://www.nasa.gov/feature/jpl/the-return-to...              nasa.gov   \n",
       "3    https://decoder.cloud/2019/12/12/from-iphone-t...         decoder.cloud   \n",
       "4    https://gfrison.com/2019/statistical-puzzle-ba...           gfrison.com   \n",
       "..                                                 ...                   ...   \n",
       "175  https://www.businessinsider.com/boeing-removed...   businessinsider.com   \n",
       "176  https://idlewords.com/2005/08/a_rocket_to_nowh...         idlewords.com   \n",
       "177                                   item?id=21777109               missing   \n",
       "178  https://twitter.com/AntNesterov/statuses/12050...           twitter.com   \n",
       "179                                 http://pixlr.com/e             pixlr.com   \n",
       "\n",
       "          votes     comments             user            date  \\\n",
       "0       1 point      discuss         mpiedrav    1 minute ago   \n",
       "1      2 points      discuss  roxyabercrombie   6 minutes ago   \n",
       "2       1 point      discuss    jelliclesfarm   8 minutes ago   \n",
       "3       1 point      discuss            miles  12 minutes ago   \n",
       "4       1 point      discuss         gfrison1  12 minutes ago   \n",
       "..          ...          ...              ...             ...   \n",
       "175   22 points   6 comments      toomuchtodo    12 hours ago   \n",
       "176   80 points  14 comments             kick       1 day ago   \n",
       "177   12 points   6 comments          obilgic    11 hours ago   \n",
       "178   10 points      discuss        koustubhs    12 hours ago   \n",
       "179  311 points  92 comments           poniko      2 days ago   \n",
       "\n",
       "                                                  body  \n",
       "0    \\nGraph Drawing is concerned with the geometri...  \n",
       "1    2019 saw automation continue its march into of...  \n",
       "2                                                       \n",
       "3    As promised in my previous post , I will show ...  \n",
       "4                                    no text Available  \n",
       "..                                                 ...  \n",
       "175                                  no text Available  \n",
       "176  A Rocket To Nowhere \\n\\n\\n\\r\\nThe Space Shuttl...  \n",
       "177                                  no text Available  \n",
       "178  We've detected that JavaScript is disabled in ...  \n",
       "179  You have no images in your history, to start e...  \n",
       "\n",
       "[180 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(bigdata1).to_csv(\"news2.csv\", columns=['title', 'url', 'domain', 'votes',\"comments\",\"user\",\"date\",\"body\"], header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "url1        0\n",
       "domain      0\n",
       "votes       0\n",
       "comments    0\n",
       "user        0\n",
       "date        0\n",
       "body        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic cleaning of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the total text into lower case\n",
    "bigdata1[\"body\"]=[i.strip().lower() for i in bigdata1[\"body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for expanding the words\n",
    "def expand_contractions(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata1['body'] = [expand_contractions(re.sub('’', \"'\", text)) for text in bigdata1['body']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing spaces and 'xao',html tags in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def scrub_words(text):\n",
    "    #Replace \\xao characters in text\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    \n",
    "    #Replace non ascii / not words and digits\n",
    "    text = re.sub(\"(\\\\W|\\\\d)\",' ',text)\n",
    "    \n",
    "    #Replace new line characters and following text untill space\n",
    "    text = re.sub('\\n(\\w*?)[\\s]', '', text)\n",
    "    \n",
    "    #Remove html markup\n",
    "    text = re.sub(\"<.*?>\", ' ', text)\n",
    "    \n",
    "    #Remove extra spaces from the text\n",
    "    text = re.sub(\"\\s+\", ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      graph drawing is concerned with the geometric ...\n",
       "1       saw automation continue its march into office...\n",
       "2                                                       \n",
       "3      as promised in my previous post i will show yo...\n",
       "4                                      no text available\n",
       "                             ...                        \n",
       "175                                    no text available\n",
       "176    a rocket to nowhere the space shuttle discover...\n",
       "177                                    no text available\n",
       "178    we have detected that javascript is disabled i...\n",
       "179    you have no images in your history to start ed...\n",
       "Name: body, Length: 180, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1['body'] = [scrub_words(i) for i in bigdata1['body']]\n",
    "bigdata1['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    #https://docs.python.org/2/library/unicodedata.html\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1['body'] = [remove_accented_chars(text) for text in bigdata1['body']]\n",
    "bigdata1['body'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1061.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1638.397339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1292.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8764.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_count\n",
       "count   180.000000\n",
       "mean   1061.811111\n",
       "std    1638.397339\n",
       "min       1.000000\n",
       "25%       3.000000\n",
       "50%     462.000000\n",
       "75%    1292.750000\n",
       "max    8764.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the words in every text\n",
    "bigdata1['word_count'] = [len(text.split(' ')) for text in bigdata1['body']]\n",
    "pd.DataFrame(bigdata1['word_count']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 9)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first quartile value of words_count attribute is 3.0\n",
      "The shape of trimmed blogs dataframe is (123, 9)\n"
     ]
    }
   ],
   "source": [
    "## Getting the first quartile value\n",
    "q1 = np.percentile(bigdata1.word_count, 25)\n",
    "print(f\"The first quartile value of words_count attribute is {q1}\")\n",
    "bigdata1 = bigdata1[bigdata1['word_count'] > q1]\n",
    "print(f\"The shape of trimmed blogs dataframe is {bigdata1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords, stemming, and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load nltk's SnowballStemmer as variable 'stemmer'\n",
    "#from nltk import nlp\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load spacy's English stopwords as variable called 'stopwords'\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(doc,remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        tokens =[word.lemma_ for word in doc if not word.is_stop]\n",
    "    else:\n",
    "        tokens =[word.lemma_ for word in doc]\n",
    "    filtered_tokens=[]\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]',token):\n",
    "            filtered_tokens.append(token)\n",
    "    lst = [t for t in filtered_tokens]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['promise',\n",
       " 'previous',\n",
       " 'post',\n",
       " 'exploit',\n",
       " 'printconfig',\n",
       " 'dll',\n",
       " 'real',\n",
       " 'world',\n",
       " 'example',\n",
       " 'apple',\n",
       " 's',\n",
       " 'iphone',\n",
       " 'read',\n",
       " 'sorry',\n",
       " 'tl',\n",
       " 'dr',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'business',\n",
       " 'partner',\n",
       " 'padovah',\n",
       " 'ck',\n",
       " 'look',\n",
       " 'possible',\n",
       " 'privileged',\n",
       " 'file',\n",
       " 'operation',\n",
       " 'exploitable',\n",
       " 'hardlink',\n",
       " 'point',\n",
       " 'directory',\n",
       " 'c',\n",
       " 'programdata',\n",
       " 'apple',\n",
       " 'lockdown',\n",
       " 'catch',\n",
       " 'attention',\n",
       " 'folder',\n",
       " 'apple',\n",
       " 'mobile',\n",
       " 'device',\n",
       " 'service',\n",
       " 'instal',\n",
       " 'itunes',\n",
       " 'software',\n",
       " 'service',\n",
       " 'run',\n",
       " 'local',\n",
       " 'system',\n",
       " 'privilege',\n",
       " 'responsible',\n",
       " 'handle',\n",
       " 'communication',\n",
       " 'usb',\n",
       " 'port',\n",
       " 'apple',\n",
       " 'device',\n",
       " 'iphone',\n",
       " 'ipad',\n",
       " 'etc',\n",
       " 'standard',\n",
       " 'user',\n",
       " 'add',\n",
       " 'file',\n",
       " 'directory',\n",
       " 'time',\n",
       " 'new',\n",
       " 'device',\n",
       " 'plug',\n",
       " 'driver',\n",
       " 'write',\n",
       " 'pair',\n",
       " 'certificate',\n",
       " 'file',\n",
       " 'directory',\n",
       " 'form',\n",
       " 'udid',\n",
       " 'plist',\n",
       " 'udid',\n",
       " 'universal',\n",
       " 'would',\n",
       " 'apple',\n",
       " 'device',\n",
       " 'let',\n",
       " 'plugin',\n",
       " 'apple',\n",
       " 'device',\n",
       " 'pair',\n",
       " 'certificate',\n",
       " 'generate',\n",
       " 'permission',\n",
       " 'set',\n",
       " 'file',\n",
       " 'following',\n",
       " 'user',\n",
       " 'read',\n",
       " 'access',\n",
       " 'file',\n",
       " 'come',\n",
       " 'funny',\n",
       " 'unplug',\n",
       " 'device',\n",
       " 'plugin',\n",
       " 'magic',\n",
       " 'happen',\n",
       " 'grant',\n",
       " 'user',\n",
       " 'control',\n",
       " 'file',\n",
       " 'observe',\n",
       " 'strange',\n",
       " 'behavior',\n",
       " 'procmon',\n",
       " 'tool',\n",
       " 'sysinternal',\n",
       " 'setsecurity',\n",
       " 'elevated',\n",
       " 'context',\n",
       " 'system',\n",
       " 'grant',\n",
       " 'control',\n",
       " 'user',\n",
       " 'resource',\n",
       " 'question',\n",
       " 'operation',\n",
       " 'exploitable',\n",
       " 'yes',\n",
       " 'get',\n",
       " 'enter',\n",
       " 'native',\n",
       " 'hardlinks',\n",
       " 'standard',\n",
       " 'window',\n",
       " 'user',\n",
       " 'need',\n",
       " 'special',\n",
       " 'privilege',\n",
       " 'create',\n",
       " 'type',\n",
       " 'link',\n",
       " 'use',\n",
       " 'forshaw',\n",
       " 's',\n",
       " 'utiltie',\n",
       " 'manage',\n",
       " 'set',\n",
       " 'native',\n",
       " 'hardlink',\n",
       " 'file',\n",
       " 'let',\n",
       " 'point',\n",
       " 'resource',\n",
       " 'system',\n",
       " 'control',\n",
       " 'go',\n",
       " 'set',\n",
       " 'hardlink',\n",
       " 'udid',\n",
       " 'plist',\n",
       " 'file',\n",
       " 'license',\n",
       " 'rtf',\n",
       " 'locate',\n",
       " 'system',\n",
       " 'folder',\n",
       " 'need',\n",
       " 'plugin',\n",
       " 'apple',\n",
       " 'device',\n",
       " 'order',\n",
       " 'alter',\n",
       " 'permission',\n",
       " 'destination',\n",
       " 'file',\n",
       " 'yes',\n",
       " 'work',\n",
       " 'point',\n",
       " 'piece',\n",
       " 'puzzle',\n",
       " 'need',\n",
       " 'change',\n",
       " 'destination',\n",
       " 'file',\n",
       " 'printconfig',\n",
       " 'dll',\n",
       " 'overwrite',\n",
       " 'dll',\n",
       " 'start',\n",
       " 'xps',\n",
       " 'print',\n",
       " 'job',\n",
       " 'finally',\n",
       " 'enjoy',\n",
       " 'system',\n",
       " 'shell',\n",
       " 'exercise',\n",
       " 'leave',\n",
       " 'reader',\n",
       " 'watch',\n",
       " 'video',\n",
       " 'poc',\n",
       " 'generic',\n",
       " 'hardlink',\n",
       " 'abuse',\n",
       " 'work',\n",
       " 'future',\n",
       " 'release',\n",
       " 'window',\n",
       " 'late',\n",
       " 'insider',\n",
       " 'preview',\n",
       " 'ms',\n",
       " 'add',\n",
       " 'supplementary',\n",
       " 'check',\n",
       " 'write',\n",
       " 'access',\n",
       " 'destination',\n",
       " 'file',\n",
       " 'access',\n",
       " 'deny',\n",
       " 'error',\n",
       " 'try',\n",
       " 'create',\n",
       " 'hardlink',\n",
       " 'miss',\n",
       " 'blur',\n",
       " 'udid',\n",
       " 'screenshot',\n",
       " 'likelike',\n",
       " 'thank',\n",
       " 'fix',\n",
       " 'likelike',\n",
       " 'fill',\n",
       " 'detail',\n",
       " 'click',\n",
       " 'icon',\n",
       " 'log',\n",
       " 'comment',\n",
       " 'wordpress',\n",
       " 'com',\n",
       " 'account',\n",
       " 'log',\n",
       " 'change',\n",
       " 'comment',\n",
       " 'google',\n",
       " 'account',\n",
       " 'log',\n",
       " 'change',\n",
       " 'comment',\n",
       " 'twitter',\n",
       " 'account',\n",
       " 'log',\n",
       " 'change',\n",
       " 'comment',\n",
       " 'facebook',\n",
       " 'account',\n",
       " 'log',\n",
       " 'change',\n",
       " 'connect',\n",
       " 's',\n",
       " 'notify',\n",
       " 'new',\n",
       " 'comment',\n",
       " 'email',\n",
       " 'notify',\n",
       " 'new',\n",
       " 'post',\n",
       " 'email']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_lemmatize(nlp(bigdata1['body'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_lemmatized = []\n",
    "vocab_lemmatized = []\n",
    "for text in bigdata1['body']:\n",
    "    doc = nlp(text)\n",
    "    word_lemmatized=tokenize_lemmatize(doc)\n",
    "    vocab_lemmatized.extend(word_lemmatized)\n",
    "    clean_text_lemmatized.append(word_lemmatized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'text_lemmatized':[]}\n",
    "for i in clean_text_lemmatized:\n",
    "    d['text_lemmatized'].append(' '.join(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "bigdata1['text_lemmatized']=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url1</th>\n",
       "      <th>domain</th>\n",
       "      <th>votes</th>\n",
       "      <th>comments</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>word_count</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graph Drawing</td>\n",
       "      <td>http://graphdrawing.org</td>\n",
       "      <td>graphdrawing.org</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>mpiedrav</td>\n",
       "      <td>1 minute ago</td>\n",
       "      <td>graph drawing is concerned with the geometric ...</td>\n",
       "      <td>89</td>\n",
       "      <td>graph drawing concerned geometric representati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automation trends for 2020</td>\n",
       "      <td>https://www.thinkautomation.com/just-for-fun/a...</td>\n",
       "      <td>thinkautomation.com</td>\n",
       "      <td>2 points</td>\n",
       "      <td>discuss</td>\n",
       "      <td>roxyabercrombie</td>\n",
       "      <td>6 minutes ago</td>\n",
       "      <td>saw automation continue its march into office...</td>\n",
       "      <td>741</td>\n",
       "      <td>see automation continue march office workflow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From iPhone to NT Authority\\System</td>\n",
       "      <td>https://decoder.cloud/2019/12/12/from-iphone-t...</td>\n",
       "      <td>decoder.cloud</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>miles</td>\n",
       "      <td>12 minutes ago</td>\n",
       "      <td>as promised in my previous post i will show yo...</td>\n",
       "      <td>550</td>\n",
       "      <td>help nethack read information page rec game ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NetHack 3.6.3</td>\n",
       "      <td>https://www.nethack.org/</td>\n",
       "      <td>nethack.org</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>paraiuspau</td>\n",
       "      <td>15 minutes ago</td>\n",
       "      <td>help what is nethack how do i get it read our ...</td>\n",
       "      <td>2407</td>\n",
       "      <td>mean develop application usually mean produce ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Serendipity – The Open Source Customer Engagem...</td>\n",
       "      <td>https://github.com/Robinyo/serendipity</td>\n",
       "      <td>github.com</td>\n",
       "      <td>1 point</td>\n",
       "      <td>discuss</td>\n",
       "      <td>Robinyo</td>\n",
       "      <td>17 minutes ago</td>\n",
       "      <td>github is home to over million developers work...</td>\n",
       "      <td>204</td>\n",
       "      <td>allhide author affiliation computer imaging te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                      Graph Drawing   \n",
       "1                         Automation trends for 2020   \n",
       "3                 From iPhone to NT Authority\\System   \n",
       "5                                      NetHack 3.6.3   \n",
       "6  Serendipity – The Open Source Customer Engagem...   \n",
       "\n",
       "                                                url1                domain  \\\n",
       "0                            http://graphdrawing.org      graphdrawing.org   \n",
       "1  https://www.thinkautomation.com/just-for-fun/a...   thinkautomation.com   \n",
       "3  https://decoder.cloud/2019/12/12/from-iphone-t...         decoder.cloud   \n",
       "5                           https://www.nethack.org/           nethack.org   \n",
       "6             https://github.com/Robinyo/serendipity            github.com   \n",
       "\n",
       "      votes comments             user            date  \\\n",
       "0   1 point  discuss         mpiedrav    1 minute ago   \n",
       "1  2 points  discuss  roxyabercrombie   6 minutes ago   \n",
       "3   1 point  discuss            miles  12 minutes ago   \n",
       "5   1 point  discuss       paraiuspau  15 minutes ago   \n",
       "6   1 point  discuss          Robinyo  17 minutes ago   \n",
       "\n",
       "                                                body  word_count  \\\n",
       "0  graph drawing is concerned with the geometric ...          89   \n",
       "1   saw automation continue its march into office...         741   \n",
       "3  as promised in my previous post i will show yo...         550   \n",
       "5  help what is nethack how do i get it read our ...        2407   \n",
       "6  github is home to over million developers work...         204   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  graph drawing concerned geometric representati...  \n",
       "1  see automation continue march office workflow ...  \n",
       "3  help nethack read information page rec game ro...  \n",
       "5  mean develop application usually mean produce ...  \n",
       "6  allhide author affiliation computer imaging te...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdata1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 243984)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(bigdata1[\"body\"])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 qoa</th>\n",
       "      <th>12 qoa cg</th>\n",
       "      <th>12 years</th>\n",
       "      <th>12 years of</th>\n",
       "      <th>16 chance</th>\n",
       "      <th>16 chance of</th>\n",
       "      <th>___________ required</th>\n",
       "      <th>___________ required allowed</th>\n",
       "      <th>_c tayp</th>\n",
       "      <th>_c tayp kt</th>\n",
       "      <th>...</th>\n",
       "      <th>zweig killed</th>\n",
       "      <th>zweig killed himself</th>\n",
       "      <th>zxn ei</th>\n",
       "      <th>zxn ei afys</th>\n",
       "      <th>zxu wu</th>\n",
       "      <th>zxu wu ut</th>\n",
       "      <th>zy xlf</th>\n",
       "      <th>zy xlf es</th>\n",
       "      <th>zz tz</th>\n",
       "      <th>zz tz ip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 243984 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     12 qoa  12 qoa cg  12 years  12 years of  16 chance  16 chance of  \\\n",
       "0       0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "1       0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "2       0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "3       0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "4       0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "..      ...        ...       ...          ...        ...           ...   \n",
       "118     0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "119     0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "120     0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "121     0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "122     0.0        0.0       0.0          0.0        0.0           0.0   \n",
       "\n",
       "     ___________ required  ___________ required allowed  _c tayp  _c tayp kt  \\\n",
       "0                     0.0                           0.0      0.0         0.0   \n",
       "1                     0.0                           0.0      0.0         0.0   \n",
       "2                     0.0                           0.0      0.0         0.0   \n",
       "3                     0.0                           0.0      0.0         0.0   \n",
       "4                     0.0                           0.0      0.0         0.0   \n",
       "..                    ...                           ...      ...         ...   \n",
       "118                   0.0                           0.0      0.0         0.0   \n",
       "119                   0.0                           0.0      0.0         0.0   \n",
       "120                   0.0                           0.0      0.0         0.0   \n",
       "121                   0.0                           0.0      0.0         0.0   \n",
       "122                   0.0                           0.0      0.0         0.0   \n",
       "\n",
       "     ...  zweig killed  zweig killed himself  zxn ei  zxn ei afys  zxu wu  \\\n",
       "0    ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "1    ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "2    ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "3    ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "4    ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "..   ...           ...                   ...     ...          ...     ...   \n",
       "118  ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "119  ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "120  ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "121  ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "122  ...           0.0                   0.0     0.0          0.0     0.0   \n",
       "\n",
       "     zxu wu ut  zy xlf  zy xlf es  zz tz  zz tz ip  \n",
       "0          0.0     0.0        0.0    0.0       0.0  \n",
       "1          0.0     0.0        0.0    0.0       0.0  \n",
       "2          0.0     0.0        0.0    0.0       0.0  \n",
       "3          0.0     0.0        0.0    0.0       0.0  \n",
       "4          0.0     0.0        0.0    0.0       0.0  \n",
       "..         ...     ...        ...    ...       ...  \n",
       "118        0.0     0.0        0.0    0.0       0.0  \n",
       "119        0.0     0.0        0.0    0.0       0.0  \n",
       "120        0.0     0.0        0.0    0.0       0.0  \n",
       "121        0.0     0.0        0.0    0.0       0.0  \n",
       "122        0.0     0.0        0.0    0.0       0.0  \n",
       "\n",
       "[123 rows x 243984 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_matrix.todense(),columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=10,algorithm='randomized',n_iter=20,random_state=143)\n",
    "lsa.fit(tfidf_matrix)\n",
    "lsa_data=lsa.transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.39402953e-16, -1.64726944e-16,  2.42947208e-05, ...,\n",
       "        -1.64760942e-16, -1.64760942e-16, -1.64760942e-16],\n",
       "       [-1.01551426e-12, -1.01548054e-12,  2.78501704e-04, ...,\n",
       "        -1.01537398e-12, -1.01537398e-12, -1.01537398e-12],\n",
       "       [-9.54934947e-10, -9.54939071e-10,  8.00986381e-04, ...,\n",
       "        -9.54935075e-10, -9.54935075e-10, -9.54935075e-10],\n",
       "       ...,\n",
       "       [-7.84896822e-04, -7.84896822e-04,  2.81621773e-04, ...,\n",
       "        -7.84896822e-04, -7.84896822e-04, -7.84896822e-04],\n",
       "       [ 1.24144024e-03,  1.24144024e-03, -5.40759887e-04, ...,\n",
       "         1.24144024e-03,  1.24144024e-03,  1.24144024e-03],\n",
       "       [-2.76314775e-04, -2.76314775e-04,  9.77828864e-04, ...,\n",
       "        -2.76314775e-04, -2.76314775e-04, -2.76314775e-04]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_ # lsa.components_ is a V^T matrix, V: Movie to concept V^T: concept to movie matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00142612,  0.01122074,  0.03354446, ...,  0.03106636,\n",
       "         0.04903191, -0.08199161],\n",
       "       [ 0.00471203,  0.03418047,  0.0835479 , ...,  0.06808441,\n",
       "        -0.01916878, -0.04090495],\n",
       "       [ 0.00985867,  0.04532228,  0.10720597, ..., -0.00630032,\n",
       "        -0.00375414,  0.01712313],\n",
       "       ...,\n",
       "       [ 0.01107967,  0.06966618,  0.16577066, ...,  0.01706075,\n",
       "         0.01425292, -0.14176321],\n",
       "       [ 0.03039409,  0.85634167, -0.29921736, ...,  0.00158168,\n",
       "         0.0042637 , -0.02097842],\n",
       "       [ 0.00305996,  0.01671771,  0.03342064, ...,  0.01132543,\n",
       "        -0.00555301,  0.00754415]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_data # U matrix U: user to concept matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Concept 0\n",
      "again go\n",
      "again go back\n",
      "and try again\n",
      "happens download\n",
      "if nothing happens\n",
      "nothing happens\n",
      "nothing happens download\n",
      "try again go\n",
      "and try\n",
      "if nothing\n",
      "go back\n",
      "try again\n",
      "back if\n",
      "back if nothing\n",
      "go back if\n",
      "\n",
      "\n",
      "Concept 1\n",
      "twitter com\n",
      "pic twitter\n",
      "pic twitter com\n",
      "anton nesterov\n",
      "your website\n",
      "learn more\n",
      "of the\n",
      "to your\n",
      "in the\n",
      "nginx inc\n",
      "add this\n",
      "below learn\n",
      "below learn more\n",
      "by copying\n",
      "by copying the\n",
      "\n",
      "\n",
      "Concept 2\n",
      "of the\n",
      "in the\n",
      "it is\n",
      "to the\n",
      "and the\n",
      "on the\n",
      "if you\n",
      "do not\n",
      "at the\n",
      "you can\n",
      "you have\n",
      "this is\n",
      "for the\n",
      "to be\n",
      "is not\n",
      "\n",
      "\n",
      "Concept 3\n",
      "the repository\n",
      "affected tests\n",
      "of tests\n",
      "the set\n",
      "the set of\n",
      "set of\n",
      "continuous integration\n",
      "deployment system\n",
      "every commit\n",
      "in the repository\n",
      "of affected\n",
      "of affected tests\n",
      "particular commit\n",
      "ping_server sqfs\n",
      "release tests\n",
      "\n",
      "\n",
      "Concept 4\n",
      "courses on\n",
      "the repository\n",
      "on javascript\n",
      "has lot\n",
      "affected tests\n",
      "of tests\n",
      "the set\n",
      "the set of\n",
      "are made by\n",
      "courses on javascript\n",
      "have to pay\n",
      "javascript or\n",
      "lot of great\n",
      "resources on\n",
      "the courses\n",
      "\n",
      "\n",
      "Concept 5\n",
      "account log\n",
      "account log out\n",
      "are commenting\n",
      "are commenting using\n",
      "commenting using\n",
      "commenting using your\n",
      "log out\n",
      "log out change\n",
      "out change\n",
      "using your\n",
      "you are commenting\n",
      "change you\n",
      "change you are\n",
      "out change you\n",
      "google sheets\n",
      "\n",
      "\n",
      "Concept 6\n",
      "docker slim\n",
      "if you\n",
      "xbox series\n",
      "you can\n",
      "you will\n",
      "you have\n",
      "want to\n",
      "http probe\n",
      "we are\n",
      "you are\n",
      "you need\n",
      "you want\n",
      "the docker\n",
      "you to\n",
      "that you\n",
      "\n",
      "\n",
      "Concept 7\n",
      "xbox series\n",
      "xbox one\n",
      "the xbox\n",
      "with xbox\n",
      "new xbox\n",
      "the new xbox\n",
      "the new\n",
      "the game\n",
      "with xbox series\n",
      "xbox game\n",
      "game awards\n",
      "the game awards\n",
      "verified sms\n",
      "game studios\n",
      "series the\n",
      "\n",
      "\n",
      "Concept 8\n",
      "verified sms\n",
      "messages with\n",
      "your messages\n",
      "google said\n",
      "rolling out\n",
      "in messages\n",
      "with verified\n",
      "identity of\n",
      "of the\n",
      "can be\n",
      "enroll in\n",
      "google said it\n",
      "sms google\n",
      "sms google said\n",
      "verified sms google\n",
      "\n",
      "\n",
      "Concept 9\n",
      "verified sms\n",
      "messages with\n",
      "your messages\n",
      "google said\n",
      "points points\n",
      "rolling out\n",
      "in messages\n",
      "with verified\n",
      "identity of\n",
      "he was\n",
      "the business\n",
      "it was\n",
      "enroll in\n",
      "google said it\n",
      "sms google\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate(lsa.components_):\n",
    "    #print(f\"The component is{comp} and shape is{comp.shape}\")\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[0:15]\n",
    "    print(\"\\n\")\n",
    "    print('Concept', i)\n",
    "    for term in sorted_terms:\n",
    "        print(term[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = ['concept {}'.format(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 qoa</th>\n",
       "      <th>12 qoa cg</th>\n",
       "      <th>12 years</th>\n",
       "      <th>12 years of</th>\n",
       "      <th>16 chance</th>\n",
       "      <th>16 chance of</th>\n",
       "      <th>___________ required</th>\n",
       "      <th>___________ required allowed</th>\n",
       "      <th>_c tayp</th>\n",
       "      <th>_c tayp kt</th>\n",
       "      <th>...</th>\n",
       "      <th>zweig killed</th>\n",
       "      <th>zweig killed himself</th>\n",
       "      <th>zxn ei</th>\n",
       "      <th>zxn ei afys</th>\n",
       "      <th>zxu wu</th>\n",
       "      <th>zxu wu ut</th>\n",
       "      <th>zy xlf</th>\n",
       "      <th>zy xlf es</th>\n",
       "      <th>zz tz</th>\n",
       "      <th>zz tz ip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>concept 0</th>\n",
       "      <td>-2.394030e-16</td>\n",
       "      <td>-1.647269e-16</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "      <td>-1.647609e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 1</th>\n",
       "      <td>-1.015514e-12</td>\n",
       "      <td>-1.015481e-12</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "      <td>-1.015374e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 2</th>\n",
       "      <td>-9.549349e-10</td>\n",
       "      <td>-9.549391e-10</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "      <td>-9.549351e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 3</th>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "      <td>1.205079e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 4</th>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>-0.000709</td>\n",
       "      <td>-0.000709</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "      <td>2.378178e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 5</th>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "      <td>3.556571e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 6</th>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>-0.002541</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "      <td>-1.763097e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 7</th>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>-0.001351</td>\n",
       "      <td>-0.001351</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "      <td>-7.848968e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 8</th>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "      <td>1.241440e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept 9</th>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "      <td>-2.763148e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 243984 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 12 qoa     12 qoa cg  12 years  12 years of  16 chance  \\\n",
       "concept 0 -2.394030e-16 -1.647269e-16  0.000024     0.000024   0.000037   \n",
       "concept 1 -1.015514e-12 -1.015481e-12  0.000279     0.000279   0.000404   \n",
       "concept 2 -9.549349e-10 -9.549391e-10  0.000801     0.000801   0.001167   \n",
       "concept 3  1.205079e-08  1.205079e-08  0.000024     0.000024  -0.000041   \n",
       "concept 4  2.378178e-08  2.378178e-08 -0.000709    -0.000709  -0.000856   \n",
       "concept 5  3.556571e-05  3.556571e-05 -0.000405    -0.000405  -0.000139   \n",
       "concept 6 -1.763097e-04 -1.763097e-04 -0.001681    -0.001681   0.001214   \n",
       "concept 7 -7.848968e-04 -7.848968e-04  0.000282     0.000282  -0.001351   \n",
       "concept 8  1.241440e-03  1.241440e-03 -0.000541    -0.000541  -0.001116   \n",
       "concept 9 -2.763148e-04 -2.763148e-04  0.000978     0.000978   0.001187   \n",
       "\n",
       "           16 chance of  ___________ required  ___________ required allowed  \\\n",
       "concept 0      0.000037              0.000031                      0.000031   \n",
       "concept 1      0.000404              0.000317                      0.000317   \n",
       "concept 2      0.001167              0.000954                      0.000954   \n",
       "concept 3     -0.000041             -0.000016                     -0.000016   \n",
       "concept 4     -0.000856             -0.000790                     -0.000790   \n",
       "concept 5     -0.000139             -0.000183                     -0.000183   \n",
       "concept 6      0.001214              0.000153                      0.000153   \n",
       "concept 7     -0.001351             -0.000612                     -0.000612   \n",
       "concept 8     -0.001116              0.000073                      0.000073   \n",
       "concept 9      0.001187              0.000627                      0.000627   \n",
       "\n",
       "                _c tayp    _c tayp kt  ...  zweig killed  \\\n",
       "concept 0 -1.647609e-16 -1.647609e-16  ...      0.000029   \n",
       "concept 1 -1.015374e-12 -1.015374e-12  ...      0.000303   \n",
       "concept 2 -9.549351e-10 -9.549351e-10  ...      0.000920   \n",
       "concept 3  1.205079e-08  1.205079e-08  ...      0.000028   \n",
       "concept 4  2.378178e-08  2.378178e-08  ...     -0.000809   \n",
       "concept 5  3.556571e-05  3.556571e-05  ...     -0.000507   \n",
       "concept 6 -1.763097e-04 -1.763097e-04  ...     -0.002541   \n",
       "concept 7 -7.848968e-04 -7.848968e-04  ...      0.000090   \n",
       "concept 8  1.241440e-03  1.241440e-03  ...     -0.000374   \n",
       "concept 9 -2.763148e-04 -2.763148e-04  ...      0.000580   \n",
       "\n",
       "           zweig killed himself        zxn ei   zxn ei afys        zxu wu  \\\n",
       "concept 0              0.000029 -1.647609e-16 -1.647609e-16 -1.647609e-16   \n",
       "concept 1              0.000303 -1.015374e-12 -1.015374e-12 -1.015374e-12   \n",
       "concept 2              0.000920 -9.549351e-10 -9.549351e-10 -9.549351e-10   \n",
       "concept 3              0.000028  1.205079e-08  1.205079e-08  1.205079e-08   \n",
       "concept 4             -0.000809  2.378178e-08  2.378178e-08  2.378178e-08   \n",
       "concept 5             -0.000507  3.556571e-05  3.556571e-05  3.556571e-05   \n",
       "concept 6             -0.002541 -1.763097e-04 -1.763097e-04 -1.763097e-04   \n",
       "concept 7              0.000090 -7.848968e-04 -7.848968e-04 -7.848968e-04   \n",
       "concept 8             -0.000374  1.241440e-03  1.241440e-03  1.241440e-03   \n",
       "concept 9              0.000580 -2.763148e-04 -2.763148e-04 -2.763148e-04   \n",
       "\n",
       "              zxu wu ut        zy xlf     zy xlf es         zz tz  \\\n",
       "concept 0 -1.647609e-16 -1.647609e-16 -1.647609e-16 -1.647609e-16   \n",
       "concept 1 -1.015374e-12 -1.015374e-12 -1.015374e-12 -1.015374e-12   \n",
       "concept 2 -9.549351e-10 -9.549351e-10 -9.549351e-10 -9.549351e-10   \n",
       "concept 3  1.205079e-08  1.205079e-08  1.205079e-08  1.205079e-08   \n",
       "concept 4  2.378178e-08  2.378178e-08  2.378178e-08  2.378178e-08   \n",
       "concept 5  3.556571e-05  3.556571e-05  3.556571e-05  3.556571e-05   \n",
       "concept 6 -1.763097e-04 -1.763097e-04 -1.763097e-04 -1.763097e-04   \n",
       "concept 7 -7.848968e-04 -7.848968e-04 -7.848968e-04 -7.848968e-04   \n",
       "concept 8  1.241440e-03  1.241440e-03  1.241440e-03  1.241440e-03   \n",
       "concept 9 -2.763148e-04 -2.763148e-04 -2.763148e-04 -2.763148e-04   \n",
       "\n",
       "               zz tz ip  \n",
       "concept 0 -1.647609e-16  \n",
       "concept 1 -1.015374e-12  \n",
       "concept 2 -9.549351e-10  \n",
       "concept 3  1.205079e-08  \n",
       "concept 4  2.378178e-08  \n",
       "concept 5  3.556571e-05  \n",
       "concept 6 -1.763097e-04  \n",
       "concept 7 -7.848968e-04  \n",
       "concept 8  1.241440e-03  \n",
       "concept 9 -2.763148e-04  \n",
       "\n",
       "[10 rows x 243984 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concept to term matrix(V)\n",
    "pd.DataFrame(lsa.components_,columns=terms,index=concepts)  # lsa.components_ is a V^T matrix, V: Movie to concept V^T: concept to movie matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=pd.DataFrame(lsa_data,columns=concepts,index=bigdata1['body']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "concept 0\n",
      "your custom text here was a really big year for me it was also a really difficult year but i think the challenges i faced gave me more clarity and determination than i would have otherwise had here are some of the things i am most proud of finishing my memoiri officially started writing my memoir whistleblower back in october and finished it a few months ago writing it was the hardest thing i have ever done i wanted to quit so many times during the writing process because finishing it meant i had to dig into some of the most painful memories from my life and process them and figure out how i felt about them and decide what i wanted to say about them i re opened a lot of old wounds to write the story of my life and while i am so glad that i did i feel like those wounds are still open and will not close until the book is out in the world and in people s hands writing it also meant i had to speak up about more injustice and mistreatment it meant i had to put myself back in the hot seat again and prepare for all of the retaliation and fallout and backlash that i will inevitably experience when the book is published i am not ready for any of that and there is really no way to prepare for it but i know that i am doing the right thing i am so proud of the book i have written and i truly believe it will make a difference in this world launching and running op eds from the futureop eds from the future is one of my absolute favorite things i have ever worked on i still cannot believe i started a speculative fiction series at the new york times and that it is still running and i am running it and that some of the greatest writers of our time have written pieces for it every time i think about it and work on it it fills me with joy finding balance between work life and being a momone of my biggest challenges this year was trying to balance working full time at the new york times writing my memoir and several other books and projects see more details below in my spare time i did not take book leave at all taking care of myself and spending time on the other things i love to do music reading exercise etc and making sure i was spending good quality time with my daughter and my husband i never wanted to feel like any of the most important aspects of my life my work my marriage my toddler were being shortchanged or that i had to make any sacrifices or compromises that i would later regret while i do wish i had much more time for writing and much more time to spend with my husband and daughter not to mention with friends and family i feel like at the end of every day when i go to sleep my heart is full i feel like i have done meaningful work and that i have spent meaningful time with the ones i love and that is really all i can ask of life becoming a better writer and writing a lot this year was transformative for me as far as my writing skills are concerned i got to the point where i can now sit down and knock out good words in one sitting even when completely exhausted at the end of a long workday my day job as an editor made all the difference here since i am so used to thinking of writing and editing as work i no longer get writer s block and writing has lost most of its mythical quality which is a good thing as far as i am concerned in addition to finishing my memoir i also wrote a couple of pieces for the times two novels which i am currently revising and one very joyful screenplay how much i learnedin addition to my usual weird learning habits i learned a great deal on the job at the times and took two classes at asu one economics class and one graduate philosophy class and three screenwriting classes at the new school it was a blast going back to school and taking classes merely for fun and for the joy of learning but it was ridiculously hard to juggle on top of all of my other responsibilities getting into better shapethis was a super active year for me i rode horses took tennis lessons ran and ran and ran hiked did lots of yoga and pilates and most recently started working with a personal trainer i would never been so active before in my life mostly because of issues with illness over the years i dealt with some really bad chronic pain this year too but i was very determined to not let any of that that stop me and i mostly succeeded reaching my reading goalevery year i try to read books one each week this year i not only met my goal i flew past it i am not sure how i read so many books this year because i did not have much extra time at all i usually read for about minutes to an hour every evening right before bed and somehow that did the trick see my reading list here https www susanjfowler com reading list here are some of the biggest lessons i learned i writing is not magic it is hard worki used to struggle to get my thoughts onto the page because the first time i put them down they were complete crap the second time i put them down they were also crap and the third time and the fourth time and the fifth time but each time what i was writing got a little bit better and eventually i realized that by approximately the fiftieth time i revised something i would have something really good therefore if i wanted to write something worth reading all i had to do was put in the time and effort to revise it enough times this is a general lesson that i think applies to most things if you want to get better at something you have to do it over and over and over again and incrementally improve with each new try given enough time you can take something from crap to good and maybe even to great ii trust yourself more i am the kind of person who picks apart every thought i have and overanalyzes every decision i always try so hard to make sure i am doing the right things for the right reasons period this unfortunately means that sometimes i do not listen to myself when i have a gut reaction that something is bad not right or off i will go along with it and or not say anything about it until i have finished overanalyzing the hell out of it at which point i step back and say hey this is not right or hey this is not right for me and then have a long thought out complicated argument as to why that is the case but the dumb thing about these situations is that the sick to my stomach feeling always eventually leads to me realizing after lengthy self doubt and over analysis that the thing in question is not right for me always without fail so this year i learned that i need to trust that feeling when it happens and stop wasting so much time doubting myself iii you have to be your biggest cheerleader and cheer for others toothis is one of the lessons i have had to learn the hard way i am the first to criticize myself and tell myself and everyone around me all of the things i am doing wrong i try really hard to be aware of my mistakes and believe me i make a lot of em and i work hard to become a better person but the more that i have found myself in the spotlight public eye the more i have realized that everyone else around me is more than willing to pick me apart and point out all of the things i am doing wrong it is like a roast so i realized that i have to be my biggest cheerleader because while everyone including me is eager to criticize me it is so much harder to find positive feedback and praise consequently i have also been trying to cheer for others more often too both because it brings more happiness into the world and because it is so much fun to bring attention to the good and awesome things that people are doing me and my daughter at the oakland zoo text here sign up to receive periodical updates from susan fowler i respect your privacy copyright susan fowler\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "you have no images in your history to start editing just select open image or load url to the left you can also test the editor out with one of our examples \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "you can reach me by oren ravendb net posts comments copyright c ayende rahien a common question i field on many customer inquiries is comparing ravendb to one relational database or another recently we got a whole spate of questions on ravendb vs postgresql and i though that it might be easier to just post about it here rather than answer the question each time some of the differences are general for all or most relational databases but i am also going to touch on specific features of postgresql that matter for this comparison the aim of this post is to provide highlights to the differences between ravendb and postgresql not to be an in depth comparison of the two postgresql is a relational database storing the data using tables columns and rows the tabular model is quite entrenched here although postgresql has the notion of json columns ravendb is a document database which store json documents natively these documents are schema free and allow arbitrarily complex structure the first key point that distinguish these databases is with the overall mindset ravendb is meant to be a database for oltp systems business applications and has been designed explicitly for this postgresql is trying to achieve both oltp and olap scenarios and tends to place a lot more emphasis on the role of the administrator and operations teams for example postgresql requires vacuum statistics refresh manual index creation etc ravendb on the other hand it design to run in a fully automated fashion there is not any action that an administrator needs to take or schedule to ensure that ravendb will run properly ravendb is also capable of configuring itself dynamically adjusting to the real world load it has based on feedback from the operational environment for example the more queries a particular index has the more resources it will be granted by ravendb another example is how ravendb processes queries in general its query analyzer will run through the incoming queries and figure out what is the best set of indexes that you need to answer them ravendb will then go ahead and create these indexes on the fly such an action tends to be scary for users coming from relational databases but ravendb was designed upfront for specifically these scenarios it is able to build the new indexes without adding too much load to the server and without taking any locks other tasks that are typically handled by the dba such as configuring the system are handled dynamically by ravendb based on actual operational behavior ravendb will also cleanup superfluous indexes and reduce the resources available for indexes that are not in common use all of that without a dba to perform acts of arcane magic another major difference between the databases is the notion of schema postgresql requires you to define your schema upfront and adhere to that the fact that you can use json at times to store data provides an important escape hatch but while postgresql allows most operations on json data including indexing them it is unable to collect statistics information on such data leading to slower queries ravendb uses a schema free model documents are grouped into collections similar to tables but without the constraint of having the same schema but have no fixed schema two documents at the same collection can have distinct structure typical projects using json columns in postgresql will tend to pull specific columns from the json to the table itself to allow for better integration with the query engine nevertheless postgresql s ability to handle both relational and document data gives it a lot of brownie points and enable a lot of sophisticated scenarios for users ravendb on the other hand is a pure json database which natively understand json it means that the querying language is much nicer for querying that involve json and comparable for queries that do not have a dynamic structure in addition to being able to query the json data ravendb also allows you to run aggregation using map reduce indexes these are similar to materialized views in postgresql but unlike those ravendb is going to update the indexes automatically and incrementally that means that you can query on large scale aggregation in microseconds regardless of data sizes for complex queries that touch on relationships between pieces of data we have very different behaviors if the relations inside postgresql are stored as columns and using foreign keys it is going to be efficient to deal with them however if the data is dynamic or complex you will want to put it in a json column at this point the cost of joining relations skyrockets for most data sets ravendb on the other hand allow you to follow relationships between documents naturally at indexing or querying time for more complex relationships work ravendb also has graph querying which allow you to run complex queries on the shape of your data i mentioned before that ravendb was designed explicitly for business applications that means that it has a much better feature set around their use case consider the humble customers page which needs to show the customers details recent orders and their total recent support calls etc when querying postgresql you will need to make multiple queries to fetch this information that means that you will have to deal with multiple network roundtrips which in many cases can be the most expensive piece of actually querying the database ravendb on the other hand has the lazy feature which allow you to combine multiple separate queries into a single network roundtrip this seemingly simple feature can have a massive impact on your overall performance a similar feature is related to the includes feature it is very common when you load one piece of data that you want to get related information with ravendb you can indicate that to the database engine which will send you all the results in one shot with a relational database you can use a join with the impact on the shape of the results cartesian products issue and possible performance impact or issue multiple queries simple change but significant improvement over the alternative ravendb is a distributed database by default while postgresql is a single node by default there exists features and options log shipping logical replication etc which allow postgresql to run as a cluster but they tend to be non trivial to setup configure and maintain with ravendb even if you are running a single node you are actually running a cluster and when you have multiple nodes it is trivial to join them into a cluster from which point on you can just manage everything as usual features such as multi master the ability for disconnected work and widely distributed clusters are native parts of ravendb and integrate seamlessly while they tend to be of the some assembly required in postgresql the two databases are very different from one another and tend to be used for separate purposes ravendb is meant to be the application database it excels in being the backend of otlp systems and focus on that to the exclusion of all else postgresql tend to be more general suitable for dynamic queries reports and exploration as well as oltp scenarios it may not be a fair comparison but i have literally built ravendb specifically to be better than a relational database for the common needs of business applications and ten years in i think it still shows significant advantages in that area finally let us talk about performance ravendb was designed based on failures in the relational model i spent years as a database performance consultant going from customer to customer fixing the same underlying issues when ravendb was designed we took that to account the paper oltp through the looking glass and what we found there has some really interesting information including the issue of about of a relational database performance is spent on locking ravendb is using mvcc just like postgresql unlike postgresql ravendb does not need to deal with transaction id wraparound vacuum costs etc instead we maintain mvcc not on the row level but on the page level there is a lot less locks to manage and deal with and far less complexity internally this means that read transactions are completely lock free and do not have to do any coordination with write transactions that has an impact on performance and ravendb can routinely manage to achieve benchmark numbers on commodity hardware that are usually reserved for expensive benchmark machines one of our developers got a new machine recently and did some light benchmarking running in wsl ubuntu on windows ravendb was able to exceed writes sec and reads sec hare the specs and let us be honest we were not really trying hard here but we still got nice numbers a lot of that is by designing how we interact internally to have a much simpler architecture and shape and it shows and the nice thing is that these advantages are cumulative ravendb is fast but you also gain the benefits of the protocol allowing you to issue multiple queries in a single roundtrip the ability to include additional results and dynamically adjusting to the operation environment it just works some more important difference i can use postgresql with full performance for free in commercial products ravendb is restricted to a single core in a cluster which is really poor alex still apples and oranges alex i am not sure how specifically that comment is helpful as regards the points oren brought up in this post if you need free then probably performance admin development etc also need to be free alex if someone does not have the budget for the commercial version of ravendb they will also have limitations perhaps even more serious budgeting for hosting admin development customer service etc travis comparing the performance of sql with nosql is no apples and oranges but comparing the price of two products is peter i can get postgresql as saas or self hosted as well as ravendb where is the difference per core is a really high price tag for some support if i did not need it and i did not need it with hosting postresql as well do not get me wrong ravendb is a wonderful database but comparing it with some other database and its performance it should be notices that with ravendb you get it only for thousand of dollars or only a really stripped configuration otherwise that cannot compete with postgresql performance for free markdown turns plain text formatting into fancy html formatting inline reference style labels titles are optional inline titles are optional reference style setext style atx style closing s are optional ordered without paragraphs unordered with paragraphs you can nest them three or more dashes or asterisks end a line with two or more spaces code blocks delimited by or more backticks or tildas set the id of headings with id at end of heading line no future posts left oh my \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "when i think of github i imagine a vast network of people and code a superorganism that changed how we make software when i was getting into programming starting a new project was daunting you had to imagine and build everything nearly from scratch my version of code sharing was calling my older cousin who was going to college for cs and getting code dictated to me over the phone when the internet arrived in my home town it made hacking on new projects so much more fun starting a new project at least initially became about discovery this mirrors the experience of many developers today except thanks to github they have many more options and a lot smoother ux however one thing that has remained the same you still have to download projects you discover on github and figure out how to run them just to start tinkering while easier than walking to the internet cafe it is enough friction that many people will give up that is why today we are making it easier to run repos on repl it now you can instantly run code from a github repository without the hassle of configuring a local development environment simply click the import button on repl it or click a run on repl it badge which will be coming to readme files near you after cloning a github repo into a repl we detect the run command and if we are unable to do so we help the user configure a replit file that includes the run command information we then help the user create a pull request back to github so that other users will not have to search for the run command in the future how is it possible at repl it we built multiple layers of abstractions in pursuit of minimum configuration instant programming environments polygott this is our base monster docker image that we hope includes most of the native dependencies necessary to run most projects we are also aiming to make this image swappable with your own in the future upm is our universal package manager that knows how to install packages in a number of different programming languages we support window manager our novel approach to window management allows us to reconfigure the ide on the fly to create any number of environments from web apps to command line or even data science environments collaborative development protocol crosis makes it possible for us to operate git from the browser it is as simple as opening an exec channel and streaming git commands to the server distributed filesystem a snapshot based filesystem with atomic writes is necessary when dealing with large scale file changes which happens when running git this is all work in progress and does not give us anywhere near coverage on all github repos however all the essential components are there and it is a matter of improving our infrastructure to cover more and more projects that can be instantly run in the browser as much as possible we are building our infrastructure in the open and with participation from our community if we do not support your favorite language or runtime please consider contributing to polygott and upm try it out this feature was silently earlier this week to our community and since then we have seen a lot of excitement from both our users and repo authors when they receive a pull request here are some of our favorites so far just go to the repo and click run on repl it to see it in action ascii_racer is a python race game in the terminal it is super fun minesweeper is a minesweeper in c ddgr search duckduckgo from your terminal pycraft d minecraft in python legit human friendly git utility progress bot a discord slack bot to report your anime progress on the web water css a super simple css framework snake go a snake implementation in go if you would like to add it to your repo simply import a repo and follow the prompts i also wrote a quick guide here if you have any trouble getting your repo to work consider asking a question on our community or visit the developers channel on our discord server at repl it we built multiple layers of abstractions in pursuit of minimum configuration instant programming environments polygott this is our base monster docker image that we hope includes most of the native dependencies necessary to run most projects we are also aiming to make this image swappable with your own in the future upm is our universal package manager that knows how to install packages in a number of different programming languages we support window manager our novel approach to window management allows us to reconfigure the ide on the fly to create any number of environments from web apps to command line or even data science environments collaborative development protocol crosis makes it possible for us to operate git from the browser it is as simple as opening an exec channel and streaming git commands to the server distributed filesystem a snapshot based filesystem with atomic writes is necessary when dealing with large scale file changes which happens when running git this is all work in progress and does not give us anywhere near coverage on all github repos however all the essential components are there and it is a matter of improving our infrastructure to cover more and more projects that can be instantly run in the browser as much as possible we are building our infrastructure in the open and with participation from our community if we do not support your favorite language or runtime please consider contributing to polygott and upm this feature was silently earlier this week to our community and since then we have seen a lot of excitement from both our users and repo authors when they receive a pull request here are some of our favorites so far just go to the repo and click run on repl it to see it in action if you would like to add it to your repo simply import a repo and follow the prompts i also wrote a quick guide here if you have any trouble getting your repo to work consider asking a question on our community or visit the developers channel on our discord server \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 1\n",
      "github is home to over million developers working together to host and review code manage projects and build software together use git or checkout with svn using the web url want to be notified of new releases in in it forward proxy if nothing happens download github desktop and try again go back if nothing happens download github desktop and try again go back if nothing happens download xcode and try again go back if nothing happens download the github extension for visual studio and try again go back forward proxy with whitelist capabilities based on go with cloudformation template to easily launch in an aws stack \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "github is home to over million developers working together to host and review code manage projects and build software together use git or checkout with svn using the web url want to be notified of new releases in fearenales event driven shell if nothing happens download github desktop and try again go back if nothing happens download github desktop and try again go back if nothing happens download xcode and try again go back if nothing happens download the github extension for visual studio and try again go back yes we have several other ways of chaining actions using native shell operators but my main issues with those are you can install emit and receive scripts on usr local bin by running \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "github is home to over million developers working together to host and review code manage projects and build software together use git or checkout with svn using the web url want to be notified of new releases in tbsschroeder using clojure for web apps if nothing happens download github desktop and try again go back if nothing happens download github desktop and try again go back if nothing happens download xcode and try again go back if nothing happens download the github extension for visual studio and try again go back slides for the conference code talks online version https tbsschroeder github io using clojure for web apps demo project https github com tbsschroeder clojure webshop app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "github is home to over million developers working together to host and review code manage projects and build software together use git or checkout with svn using the web url want to be notified of new releases in in it openvpn access if nothing happens download github desktop and try again go back if nothing happens download github desktop and try again go back if nothing happens download xcode and try again go back if nothing happens download the github extension for visual studio and try again go back provides a web frontend with openid connect authentication that can create and sign new openvpn client certificates the client certificates and ca crt ca key are stored in s an ovpn config is generated and offered as a download the client crt key can be encrypted at rest using aws kms \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 2\n",
      "we have detected that javascript is disabled in your browser would you like to proceed to legacy twitter yes open source censorship and privacy you can add location information to your tweets such as your city or precise location from the web and via third party applications you always have the option to delete your tweet location history learn more here s the url for this tweet copy it to easily share with friends add this tweet to your website by copying the code below learn more add this video to your website by copying the code below learn more hmm there was a problem reaching the server try again by embedding twitter content in your website or app you are agreeing to the twitter developer agreement and developer policy this timeline is where you will spend most of your time getting instant updates about what matters to you hover over the profile pic and click the following button to unfollow any account when you see a tweet you love tap the heart it lets the person who wrote it know you shared the love the fastest way to share someone else s tweet with your followers is with a retweet tap the icon to send it instantly add your thoughts about any tweet with a reply find a topic you are passionate about and jump right in get instant insight into what people are talking about now follow more accounts to get instant updates about topics you care about see the latest conversations about any topic instantly catch up instantly on the best stories happening as they unfold seems like rambler filled copyright claim to isysoev regarding nginx nginx office under police raid unconfirmed originally posted by igorippolitov but somebody asked him to remove his post pic twitter com mbntv g anton nesterov retweeted igor ippolitov nginx police raid additionally confirmed by phil kulin https t me usher original tweet by igorippolitov who is nginx inc employee was removed under unofficial police requesthttps twitter com igorippolitov status anton nesterov added anton nesterov retweeted igor ippolitov igor sysoev and maxim konovalov allegedly detained on nginx casehttps twitter com igorippolitov status anton nesterov added sysoev and konovalov freed after being questioned by police about the case their phones was seizedhttps t me thebell_io context development started in by sysoev when he was rambler employee as sysadmin rambler never claimed copyright on nginx later nginx inc was founded to provide commercial services this year f networks bought nginx inc and sberbank bought of rambler group thebellio got official comment from rambler group https t me thebell_io they claim full copyright on all nginx source code as sysoev was rambler employee pic twitter com npzyhh jml ramblerru take your hands off nginx and isysoev dierambler nginxbealive                  pic twitter com xvufnicj d back to top twitter may be over capacity or experiencing a momentary hiccup try again or visit twitter status for more information \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "we have detected that javascript is disabled in your browser would you like to proceed to legacy twitter yes open source censorship and privacy you can add location information to your tweets such as your city or precise location from the web and via third party applications you always have the option to delete your tweet location history learn more here s the url for this tweet copy it to easily share with friends add this tweet to your website by copying the code below learn more add this video to your website by copying the code below learn more hmm there was a problem reaching the server try again by embedding twitter content in your website or app you are agreeing to the twitter developer agreement and developer policy this timeline is where you will spend most of your time getting instant updates about what matters to you hover over the profile pic and click the following button to unfollow any account when you see a tweet you love tap the heart it lets the person who wrote it know you shared the love the fastest way to share someone else s tweet with your followers is with a retweet tap the icon to send it instantly add your thoughts about any tweet with a reply find a topic you are passionate about and jump right in get instant insight into what people are talking about now follow more accounts to get instant updates about topics you care about see the latest conversations about any topic instantly catch up instantly on the best stories happening as they unfold seems like rambler filled copyright claim to isysoev regarding nginx nginx office under police raid unconfirmed originally posted by igorippolitov but somebody asked him to remove his post pic twitter com mbntv g anton nesterov retweeted igor ippolitov nginx police raid additionally confirmed by phil kulin https t me usher original tweet by igorippolitov who is nginx inc employee was removed under unofficial police requesthttps twitter com igorippolitov status anton nesterov added anton nesterov retweeted igor ippolitov igor sysoev and maxim konovalov allegedly detained on nginx casehttps twitter com igorippolitov status anton nesterov added sysoev and konovalov freed after being questioned by police about the case their phones was seizedhttps t me thebell_io context development started in by sysoev when he was rambler employee as sysadmin rambler never claimed copyright on nginx later nginx inc was founded to provide commercial services this year f networks bought nginx inc and sberbank bought of rambler group thebellio got official comment from rambler group https t me thebell_io they claim full copyright on all nginx source code as sysoev was rambler employee pic twitter com npzyhh jml ramblerru take your hands off nginx and isysoev dierambler nginxbealive                  pic twitter com xvufnicj d back to top twitter may be over capacity or experiencing a momentary hiccup try again or visit twitter status for more information \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have detected that javascript is disabled in your browser would you like to proceed to legacy twitter yes emmy nominated host host executive producer for xplorationouterspace on fox author ada lace series wvu mit grad wv native you can add location information to your tweets such as your city or precise location from the web and via third party applications you always have the option to delete your tweet location history learn more here s the url for this tweet copy it to easily share with friends add this tweet to your website by copying the code below learn more add this video to your website by copying the code below learn more hmm there was a problem reaching the server try again by embedding twitter content in your website or app you are agreeing to the twitter developer agreement and developer policy this timeline is where you will spend most of your time getting instant updates about what matters to you hover over the profile pic and click the following button to unfollow any account when you see a tweet you love tap the heart it lets the person who wrote it know you shared the love the fastest way to share someone else s tweet with your followers is with a retweet tap the icon to send it instantly add your thoughts about any tweet with a reply find a topic you are passionate about and jump right in get instant insight into what people are talking about now follow more accounts to get instant updates about topics you care about see the latest conversations about any topic instantly catch up instantly on the best stories happening as they unfold in the s deaf men helped nasa get to the moon they were known as the gallaudet eleven we were able to interview one of their remaining members david myers for my show xploration outer space a thread in the s men were recruited from gallaudet college now gallaudetu for a series of nasa experiments to better understand how spaceflight would affect astronauts these experiments were done over the course of a decade pic twitter com igbsqxna t the experiments were led by dr graybiel a cardiologist who had studied how flight affected pilots and continued to work for nasa to study how spaceflight might affect astronauts remember this was during a time when we knew nothing about how space would affect humans pic twitter com ixnersosac in particular nasa was worried about motion sickness as david explained you do not want your pilot sick when he is piloting a spaceship so they wanted to understand what caused people to experience motion sickness why recruit deaf people for these experiments well in early tests deaf people fared far better than hearing subjects in spaceflight experiments designed to make the person dizzy or sick they seemed immune to motion sickness graybiel wanted to learn more about why that was at the time the theories around why motion sickness occurred focused on the stomach but spoiler alert these men were immune to motion sickness for a different reason unrelated to the stomach more on this in a second so the gallaudet men acted as sort of a control group for nasa their results were compared to the results of the hearing group they flew on the vomit comet they spent days in a ft diameter room that spun at rpm they spent days on a boat in the choppy north atlantic pic twitter com rlahda j o in each of these tests the deaf subjects felt fine while their hearing counterparts became sick in fact they had to cut the boat experiment short because the hearing analysts were so sick they could not conduct their tests on the gallaudet men it turns out that the reason why the gallaudet men were immune to motion sickness had to do with the cause of their deafness each of these men became deaf due to spinal meningitis which damaged their vestibular system in their ear pic twitter com uapiqchgqs the vestibular system is used to detect acceleration fluid moves around in each loop tells us which way we are moving this is also responsible for motion sickness you spin around that makes the fluid spin you stop your eyes see that but the fluid still spins pic twitter com vwjfbzeyvl when these two inputs do not match motion sickness so because this system was damaged in the gallaudet men they were immune to motion sickness what these experiments showed is that motion sickness does not originate in the stomach but rather they lie in the labyrinths of the ear dr graybiel used this research to help develop early drugs to prevent motion sickness in hearing subjects as i interviewed david i asked him if it bothered him that even though he was going through many of the same experiments that astronauts were going through and faring much better than his hearing counterparts that he was not eligible to become an astronaut he said that he understood he was not a pilot and had no interest in the space program prior to being recruited for these experiments but would still love to go into space and with a wink and a smile he said he will be waiting for the call from nasa pic twitter com umrylhvyjq the gallaudet reminds us that even though astronauts are the ones we spotlight when it comes to space exploration many of our brave explorers who help push the boundaries of exploration in the solar system remain firmly on the ground to learn more about the incredible story of the gallaudet check out the exhibit that opened about their contributions in washington dc and watch xplorationouterspace this spring https www gallaudet edu drs john s and betty j schuchman deaf documentary center deaf difference space survival opening and a huge thanks to our crew a mix of hearing and deaf individuals as well as our asl english interpreters who helped facilitate communication and made this shoot possible this will be featured in a season episode of xplorationouterspace that premieres in the spring pic twitter com smdoppvhpj back to top twitter may be over capacity or experiencing a momentary hiccup try again or visit twitter status for more information \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "we have detected that javascript is disabled in your browser would you like to proceed to legacy twitter yes researcher writer activist tech society tracking algorithmic decisions platforms surveillance privacy created datadealer book networks of control you can add location information to your tweets such as your city or precise location from the web and via third party applications you always have the option to delete your tweet location history learn more here s the url for this tweet copy it to easily share with friends add this tweet to your website by copying the code below learn more add this video to your website by copying the code below learn more hmm there was a problem reaching the server try again by embedding twitter content in your website or app you are agreeing to the twitter developer agreement and developer policy this timeline is where you will spend most of your time getting instant updates about what matters to you hover over the profile pic and click the following button to unfollow any account when you see a tweet you love tap the heart it lets the person who wrote it know you shared the love the fastest way to share someone else s tweet with your followers is with a retweet tap the icon to send it instantly add your thoughts about any tweet with a reply find a topic you are passionate about and jump right in get instant insight into what people are talking about now follow more accounts to get instant updates about topics you care about see the latest conversations about any topic instantly catch up instantly on the best stories happening as they unfold searching for an airbnb apt on desktop then getting a fb ad for the same village on mobile leading back to airbnb web they even remembered the date range i entered on desktop how does this work is the date range passed via fb event data or is this airbnb x device tracking wolfie christl retweeted Zach Edwards so this is what happened thanks a lot thezedwards https twitter com thezedwards status wolfie christl added wolfie christl retweeted wolfie christl i did some further digging airbnb sends detailed id behavioral data on anonymous website visitors to fb fb recognizes users when airbnb places an fb ad based on this data and a user clicks it fb sends data back to airbnb including search dateshttps twitter com wolfiechristl status wolfie christl added i did not observe airbnb sending advanced matching data to fb https twitter com thezedwards status in my browser as i was logged into fb and did not use any tracking protection fb probably recognized me via cookie this is already questionable even more so this https developers facebook com docs facebook pixel advanced advanced matching pic twitter com lgyyh h ga wolfie christl retweeted matthias eberl so matthiaseberl created a fresh airbnb account and almost immediately fb received first name last name date of birth gender email phone number and some airbnb user id even though he blocked cookies in his browser and was not logged into fb https twitter com matthiaseberl status wolfie christl added fb received hashed versions of those attributes from airbnb but based on hashed email addresses phone numbers etc facebook can recognize match identify corresponding fb users hashed identifiers pseudonymous data personal data under the gdpr https developers facebook com docs facebook pixel advanced advanced matching pic twitter com jfzg akcmj sorry everyone this thread got a bit chaotic but this is possibly how collaborative twitter investigations look like anyway several parts of what fb and airbnb and others are doing may be illegal under the gdpr we need decisions by eu authorities and we need them soon not just on edge cases but at fundamental levels we do not need better phrased cookie banners or fb opt out settings or further tracking protection arms races can airbnb fb have a legal basis to process personal data across platforms in such an excessive way i would say no i would say neither airbnb nor fb can get informed consent at least not without an extra very clear explainer for this not feasible also their legitimate interests cannot outweigh users rights it is also not necessary for the performance of a contract full stop i know it is a bit more complicated practices vs formal purposes need to be discussed in detail but i am not sure whether any eu data protection authority has even really examined such practices until now if dpas do not get enough resources courage soon the gdpr will fail that is facebook s advanced matching for web it started identifying custom information outside of form fields on dec days ago https www facebook com business help url looks like https www airbnb com api v facebook_advanced_matchings currency usd key advanced matching fields https developers facebook com docs facebook pixel advanced advanced matching default is on pic twitter com mct kv lbo the default on may vary by certain other logic i think someone may have needed to agree to certain tos at some point for inactive fb pixels it is not on but i see it on in some other active pixels airbnb definitely has an advanced setup though very much on for them pic twitter com ehaobesk k back to top twitter may be over capacity or experiencing a momentary hiccup try again or visit twitter status for more information \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a minute read written bygard aaland engen in this article i will present to you some resources which can be useful to know about if you want to learn javascript or in general want to learn more about the language this is a resource created for developers by developers and is loaded with articles tutorials references and documentation on many subjects such as javascript css and webassembly you can find information aimed towards all kinds of experience here which also applies to the tutorials and the guides when it comes to javascript in particular mdn has a lot to offer and i think that almost everyone that reads this article can learn something here there is one guide on mdn which you need to check out named javascript guide this is a detailed guide that gives an excellent overview of the language and is divided into multiple chapters that cover topics such as functions working with objects promises modules and so on this guide is directed primarily towards those who have previous experience with javascript or other languages in addition to this guide here are some other exciting tutorials on javascript that mdn offer this is a website that provides short on point informative video courses on javascript web frameworks and libraries the courses are for both newcomers and experienced javascript developers and are made by both professionals and open source contributors most of the courses consist of small bite size videos rather than long ones and the videos go straight to the point so that you can save time each course is made up of videos on exciting topics such as egghead has a lot of great content that is free but you have to pay to get full access to all of the content of the website this website and its content are made by a full stack web developer and designer named wes bos from hamilton canada the website has a blog with plenty of references to courses videos and articles that wes bos has made on this blog you can find useful resources on learning the basics but also resources on more advanced topics in the realm of web development some of the resources are free and some you have to pay for if there is one resource you have to check out from mr bos it must be his famous javascript course this is a day vanilla javascript coding challenge where you can join over others to learn vanilla javascript in days with tutorials you cannot make a list like this without mentioning youtube there are just so many great courses on this platform and the best about the platform is that it is free i got into javascript just by watching courses on youtube so i will list some popular channels which i have discovered that has a lot of great video courses on javascript but also web development in general finally i will share two free books with you there you have it a full list of resources about javascript which you can enjoy during the holidays bekk is all about craftmanship and the people crafting it this year we are creating calendars each with daily content articles and podcasts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a minute read written bygard aaland engen in this article i will present to you some resources which can be useful to know about if you want to learn javascript or in general want to learn more about the language this is a resource created for developers by developers and is loaded with articles tutorials references and documentation on many subjects such as javascript css and webassembly you can find information aimed towards all kinds of experience here which also applies to the tutorials and the guides when it comes to javascript in particular mdn has a lot to offer and i think that almost everyone that reads this article can learn something here there is one guide on mdn which you need to check out named javascript guide this is a detailed guide that gives an excellent overview of the language and is divided into multiple chapters that cover topics such as functions working with objects promises modules and so on this guide is directed primarily towards those who have previous experience with javascript or other languages in addition to this guide here are some other exciting tutorials on javascript that mdn offer this is a website that provides short on point informative video courses on javascript web frameworks and libraries the courses are for both newcomers and experienced javascript developers and are made by both professionals and open source contributors most of the courses consist of small bite size videos rather than long ones and the videos go straight to the point so that you can save time each course is made up of videos on exciting topics such as egghead has a lot of great content that is free but you have to pay to get full access to all of the content of the website this website and its content are made by a full stack web developer and designer named wes bos from hamilton canada the website has a blog with plenty of references to courses videos and articles that wes bos has made on this blog you can find useful resources on learning the basics but also resources on more advanced topics in the realm of web development some of the resources are free and some you have to pay for if there is one resource you have to check out from mr bos it must be his famous javascript course this is a day vanilla javascript coding challenge where you can join over others to learn vanilla javascript in days with tutorials you cannot make a list like this without mentioning youtube there are just so many great courses on this platform and the best about the platform is that it is free i got into javascript just by watching courses on youtube so i will list some popular channels which i have discovered that has a lot of great video courses on javascript but also web development in general finally i will share two free books with you there you have it a full list of resources about javascript which you can enjoy during the holidays bekk is all about craftmanship and the people crafting it this year we are creating calendars each with daily content articles and podcasts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "subscribe and get updates dropbox server side software lives in a large monorepo one lesson we have learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole years ago it was reasonable to run our entire test corpus on every commit to the repository this scheme became untenable as we added more tests one obvious inefficiency is the pointless and wasteful execution of tests that cannot possibly be affected by a particular change we addressed this problem with the help of our build system code in our monorepo is built and tested exclusively with bazel abstractly bazel views the repository as a set of targets files binaries libraries tests etc and the dependencies between them in particular bazel knows the dependency graph between all source files and tests in the repository we modified our continuous integration system to extract this dependency information from bazel via bazel query and use it to compute the set of tests affected by a particular commit this allows us to greatly reduce the number of tests executed on most commits while still being correct since a particular test no longer runs on every commit we use its previous history to determine its status on commits where it did not run if a test runs on commit n but is not affected by commit n we can consider the test to have the same status on both commits in this way we propagate a status for every test in the repository for every commit that it exists in to conserve extra resources we do not run all affected tests on all commits we roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period then we execute the rolled up set of affected tests for that period on the last commit in the period the time between rollup builds is a tunable parameter trading resource usage against test result timeliness this batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable however our automated breakage detection system athena is able to bisect failing tests over the entire rollup period to find the precise broken change our production deployment system distributes software to hosts in the form of squashfs images we have a custom bazel rule that builds squashfs images for the deployment system to consume we generally require that software pass its tests before being pushed into our production environment this requirement is enforced by our deployment system historically we allowed pushing software from a particular commit only if all tests in the repository passed on it either by being directly run on the commit or through propagation from previous commits while simple this model does not scale well it is frustrating to have deployment blocked because a completely unrelated test is failing even if all tests pass repository growth over time means takes longer and longer to prove that a particular commit is completely green despite running only affected tests on each commit to break down the old monolithic green system we allow every deployable package to specify release tests the set of tests required to pass before pushing it the test set is described using a subset of bazel s target pattern syntax for example deploying a simple c ping server at dropbox might have a bazel build file like this this file declares a c library binary and test using the standard built in bazel c rules the ping_server sqfs target produces a squashfs containing the ping_server binary ping_server sqfs would be deployable on commits where every test in ping_server and its subpackages had passed as mentioned previously we conserve resources by aggregating test runs across several commit builds this potentially introduces extra latency between when a commit lands and when it is deployable if an engineer makes a change to the ping server and wants to deploy it immediately they can request that our continuous integration system run ping_server sqfs s release tests as soon as their commit lands this happens regardless of where the commit falls in the rollup period we leave the decision of what to to put in release_tests up to individual teams it is common to include a package s own tests as well as the tests of critical dependency libraries more conservative projects might include some of their reverse dependencies tests when we were developing the release tests feature we experimented with automatically generating the test set by inspecting the packaged code s dependencies however we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package continuous integration and deployment with bazel monitoring server applications with vortex modernizing our android build system part i the planning please note sometimes we blog about upcoming products or features before they are released but timing and exact functionality of these features may change from what is shared here the decision to purchase our services should be made based on features that are currently available dropbox inc \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe and get updates dropbox server side software lives in a large monorepo one lesson we have learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole years ago it was reasonable to run our entire test corpus on every commit to the repository this scheme became untenable as we added more tests one obvious inefficiency is the pointless and wasteful execution of tests that cannot possibly be affected by a particular change we addressed this problem with the help of our build system code in our monorepo is built and tested exclusively with bazel abstractly bazel views the repository as a set of targets files binaries libraries tests etc and the dependencies between them in particular bazel knows the dependency graph between all source files and tests in the repository we modified our continuous integration system to extract this dependency information from bazel via bazel query and use it to compute the set of tests affected by a particular commit this allows us to greatly reduce the number of tests executed on most commits while still being correct since a particular test no longer runs on every commit we use its previous history to determine its status on commits where it did not run if a test runs on commit n but is not affected by commit n we can consider the test to have the same status on both commits in this way we propagate a status for every test in the repository for every commit that it exists in to conserve extra resources we do not run all affected tests on all commits we roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period then we execute the rolled up set of affected tests for that period on the last commit in the period the time between rollup builds is a tunable parameter trading resource usage against test result timeliness this batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable however our automated breakage detection system athena is able to bisect failing tests over the entire rollup period to find the precise broken change our production deployment system distributes software to hosts in the form of squashfs images we have a custom bazel rule that builds squashfs images for the deployment system to consume we generally require that software pass its tests before being pushed into our production environment this requirement is enforced by our deployment system historically we allowed pushing software from a particular commit only if all tests in the repository passed on it either by being directly run on the commit or through propagation from previous commits while simple this model does not scale well it is frustrating to have deployment blocked because a completely unrelated test is failing even if all tests pass repository growth over time means takes longer and longer to prove that a particular commit is completely green despite running only affected tests on each commit to break down the old monolithic green system we allow every deployable package to specify release tests the set of tests required to pass before pushing it the test set is described using a subset of bazel s target pattern syntax for example deploying a simple c ping server at dropbox might have a bazel build file like this this file declares a c library binary and test using the standard built in bazel c rules the ping_server sqfs target produces a squashfs containing the ping_server binary ping_server sqfs would be deployable on commits where every test in ping_server and its subpackages had passed as mentioned previously we conserve resources by aggregating test runs across several commit builds this potentially introduces extra latency between when a commit lands and when it is deployable if an engineer makes a change to the ping server and wants to deploy it immediately they can request that our continuous integration system run ping_server sqfs s release tests as soon as their commit lands this happens regardless of where the commit falls in the rollup period we leave the decision of what to to put in release_tests up to individual teams it is common to include a package s own tests as well as the tests of critical dependency libraries more conservative projects might include some of their reverse dependencies tests when we were developing the release tests feature we experimented with automatically generating the test set by inspecting the packaged code s dependencies however we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package continuous integration and deployment with bazel monitoring server applications with vortex modernizing our android build system part i the planning please note sometimes we blog about upcoming products or features before they are released but timing and exact functionality of these features may change from what is shared here the decision to purchase our services should be made based on features that are currently available dropbox inc \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 4\n",
      "subscribe and get updates dropbox server side software lives in a large monorepo one lesson we have learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole years ago it was reasonable to run our entire test corpus on every commit to the repository this scheme became untenable as we added more tests one obvious inefficiency is the pointless and wasteful execution of tests that cannot possibly be affected by a particular change we addressed this problem with the help of our build system code in our monorepo is built and tested exclusively with bazel abstractly bazel views the repository as a set of targets files binaries libraries tests etc and the dependencies between them in particular bazel knows the dependency graph between all source files and tests in the repository we modified our continuous integration system to extract this dependency information from bazel via bazel query and use it to compute the set of tests affected by a particular commit this allows us to greatly reduce the number of tests executed on most commits while still being correct since a particular test no longer runs on every commit we use its previous history to determine its status on commits where it did not run if a test runs on commit n but is not affected by commit n we can consider the test to have the same status on both commits in this way we propagate a status for every test in the repository for every commit that it exists in to conserve extra resources we do not run all affected tests on all commits we roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period then we execute the rolled up set of affected tests for that period on the last commit in the period the time between rollup builds is a tunable parameter trading resource usage against test result timeliness this batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable however our automated breakage detection system athena is able to bisect failing tests over the entire rollup period to find the precise broken change our production deployment system distributes software to hosts in the form of squashfs images we have a custom bazel rule that builds squashfs images for the deployment system to consume we generally require that software pass its tests before being pushed into our production environment this requirement is enforced by our deployment system historically we allowed pushing software from a particular commit only if all tests in the repository passed on it either by being directly run on the commit or through propagation from previous commits while simple this model does not scale well it is frustrating to have deployment blocked because a completely unrelated test is failing even if all tests pass repository growth over time means takes longer and longer to prove that a particular commit is completely green despite running only affected tests on each commit to break down the old monolithic green system we allow every deployable package to specify release tests the set of tests required to pass before pushing it the test set is described using a subset of bazel s target pattern syntax for example deploying a simple c ping server at dropbox might have a bazel build file like this this file declares a c library binary and test using the standard built in bazel c rules the ping_server sqfs target produces a squashfs containing the ping_server binary ping_server sqfs would be deployable on commits where every test in ping_server and its subpackages had passed as mentioned previously we conserve resources by aggregating test runs across several commit builds this potentially introduces extra latency between when a commit lands and when it is deployable if an engineer makes a change to the ping server and wants to deploy it immediately they can request that our continuous integration system run ping_server sqfs s release tests as soon as their commit lands this happens regardless of where the commit falls in the rollup period we leave the decision of what to to put in release_tests up to individual teams it is common to include a package s own tests as well as the tests of critical dependency libraries more conservative projects might include some of their reverse dependencies tests when we were developing the release tests feature we experimented with automatically generating the test set by inspecting the packaged code s dependencies however we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package continuous integration and deployment with bazel monitoring server applications with vortex modernizing our android build system part i the planning please note sometimes we blog about upcoming products or features before they are released but timing and exact functionality of these features may change from what is shared here the decision to purchase our services should be made based on features that are currently available dropbox inc \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe and get updates dropbox server side software lives in a large monorepo one lesson we have learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole years ago it was reasonable to run our entire test corpus on every commit to the repository this scheme became untenable as we added more tests one obvious inefficiency is the pointless and wasteful execution of tests that cannot possibly be affected by a particular change we addressed this problem with the help of our build system code in our monorepo is built and tested exclusively with bazel abstractly bazel views the repository as a set of targets files binaries libraries tests etc and the dependencies between them in particular bazel knows the dependency graph between all source files and tests in the repository we modified our continuous integration system to extract this dependency information from bazel via bazel query and use it to compute the set of tests affected by a particular commit this allows us to greatly reduce the number of tests executed on most commits while still being correct since a particular test no longer runs on every commit we use its previous history to determine its status on commits where it did not run if a test runs on commit n but is not affected by commit n we can consider the test to have the same status on both commits in this way we propagate a status for every test in the repository for every commit that it exists in to conserve extra resources we do not run all affected tests on all commits we roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period then we execute the rolled up set of affected tests for that period on the last commit in the period the time between rollup builds is a tunable parameter trading resource usage against test result timeliness this batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable however our automated breakage detection system athena is able to bisect failing tests over the entire rollup period to find the precise broken change our production deployment system distributes software to hosts in the form of squashfs images we have a custom bazel rule that builds squashfs images for the deployment system to consume we generally require that software pass its tests before being pushed into our production environment this requirement is enforced by our deployment system historically we allowed pushing software from a particular commit only if all tests in the repository passed on it either by being directly run on the commit or through propagation from previous commits while simple this model does not scale well it is frustrating to have deployment blocked because a completely unrelated test is failing even if all tests pass repository growth over time means takes longer and longer to prove that a particular commit is completely green despite running only affected tests on each commit to break down the old monolithic green system we allow every deployable package to specify release tests the set of tests required to pass before pushing it the test set is described using a subset of bazel s target pattern syntax for example deploying a simple c ping server at dropbox might have a bazel build file like this this file declares a c library binary and test using the standard built in bazel c rules the ping_server sqfs target produces a squashfs containing the ping_server binary ping_server sqfs would be deployable on commits where every test in ping_server and its subpackages had passed as mentioned previously we conserve resources by aggregating test runs across several commit builds this potentially introduces extra latency between when a commit lands and when it is deployable if an engineer makes a change to the ping server and wants to deploy it immediately they can request that our continuous integration system run ping_server sqfs s release tests as soon as their commit lands this happens regardless of where the commit falls in the rollup period we leave the decision of what to to put in release_tests up to individual teams it is common to include a package s own tests as well as the tests of critical dependency libraries more conservative projects might include some of their reverse dependencies tests when we were developing the release tests feature we experimented with automatically generating the test set by inspecting the packaged code s dependencies however we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package continuous integration and deployment with bazel monitoring server applications with vortex modernizing our android build system part i the planning please note sometimes we blog about upcoming products or features before they are released but timing and exact functionality of these features may change from what is shared here the decision to purchase our services should be made based on features that are currently available dropbox inc \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "by lawrence kesteloot january also available as an e book from amazon after a minute patrick came back with a small dusty cardboard box dave and i stared as patrick opened it and pulled out a network switch the old kind from the days when they were made with metal cases he plugged in the power supply and carefully straightened out a cat cable to hook the switch up to our network i wanted to yell at him for being so careful and deliberate at a time like this dave sat next to me and was uncharacteristically still i stopped breathing as patrick struggled to get the plug lined up with the port i stared at the front panel lights and felt dave doing the same my eyes watered patrick pushed the plug in the front lights immediately lit and flashed actively i felt my hands and face flush and out of the corner of my eye saw dave sit up and open his mouth as if to speak he then put his face down into his cupped hands and threw up this is not how we would have reacted three weeks earlier three weeks earlier we had settled down into that difficult part of a project after the honeymoon ends and you have to implement the tedious parts of the product the parts that were just simple boxes in the design and seemed straightforward but turned out to be tricky for reasons that are incidental and therefore uninteresting patrick and dave had done the design of course and had humored me as i had suggested various ideas which in retrospect must have been obvious to them it had been fascinating to hear them debate design issues so many of their arguments were based on intuition rather than reasoned logic i had learned more each week than my entire last semester of school i found myself siding more with dave dave mitchell had a great laugh he was on the heavy side and knocked things over a lot i liked the insights he had about management and the process of software development and he liked to teach me things patrick and dave had split up the work they had designed and they had given me tests to write in advance of their code being ready it had been a quiet day when i heard dave whisper what the hell and put his hands into his hair i sensed an opportunity to learn and walked over to his desk he did not notice he was quickly switching back to his editor adding a line of debug output compiling running shaking his head and switching back to add another line elsewhere what is the matter he waited until the program finished running to answer i cannot figure out this bug i cannot figure out where this number is coming from here he pointed to a line in this debug output dave and patrick liked to debug by writing lines to the console rather than stepping through with a debugger as i liked to do what happens when you step through it in the debugger this was a tease dave did not know how to use a debugger short of displaying the stack trace of a core file dave paused then brought up the debugger on his program i smiled in surprise then walked him through how to set breakpoints we spent the next few hours narrowing down where in the mess of his code boost and the system libraries this value was coming from patrick can you help us here this surprised me more than the debugger dave rarely asked patrick for help patrick came over and dave explained the situation the program is crashing and that is because of this bad dereference here but the value is correct here dave pointed at some code while patrick s eyes scanned the screen then a thought occurred to me dave was still explaining and i did not want to interrupt my heart was jumping as i waited for dave to take a breath i think it is a compiler bug i said with confidence squinting my eyes at the code patrick and dave raced each other to brush this off blaming the compiler is a last resort said patrick same with the standard library chances are vastly greater that your brand new code has a bug rather than code being used by thousands of people i nodded wisely but i felt my cheeks blush patrick karlsson often made me feel bad he never meant to i am sure but he had a way of arguing in a brutally straightforward way that made me feel thoughtless and young he was taller than dave and i and thin he was quiet except when correcting something i had said or code i had written but i liked him anyway i sometimes found his design decisions questionable and naive dave and i liked to gang up on him patrick was cracking his knuckles he asked questions about threads and volatile and aliasing questions i did not entirely understand but wish i would thought of dave considered each and said that none of patrick s concerns applied in this case patrick pursed his lips to the right the way he did when he was not sure the answers he would been given were correct and said hmmm i do not know weird and returned to his desk i was relieved that patrick had not crushed us with an obvious solution and i guessed that dave felt the same as he sat back up from his slouch and started typing is there any way to show the assembly right here where things go pear shaped i showed him how to do this and i wondered if he was giving my theory a chance he typed the command and the lines of c were split apart by dozens of lines of assembly that is not right dave said we got the command wrong i am sure that is right i said i knew the debugger well and by we he meant me no it is not that far too much assembly for this line of code and some of these are not even legal assembly instructions that is not possible said patrick from his desk dave blinked slowly well i meant that i have never seen these they are not what a compiler outputs this is not the right place he was flustered and i felt more inexperienced than ever dave got up and said i am burned out we will look at this again tomorrow he left and i sat down at his computer the only thing i had contributed all day was knowledge about the debugger and dave did not even think i had gotten that right i looked at the assembly on the screen some of the instructions were obvious and some were cryptic i found an online assembly reference and started tracing the instructions keeping notes about register contents in my notebook dave was right these instructions made no sense not only did they perform too much work for the corresponding c code but they did not even make sense internally but i convinced myself that this was at least the right section because a few of the instructions matched the surrounding c lines i focused on one instruction in particular it was subtracting a register from itself that is not necessarily odd in itself it might be an efficient way to set a register to zero but this register was then used in other math instructions the compiler must have known it would be zero and optimized it out i hesitated then called patrick over i explained what i had found he stared at it for a good while then said that is not a normal subtraction instruction he was right i looked it up more carefully and found that it was a variation that also subtracted out the carry bit this was a way to get the carry bit into a register i worked my way backwards to see where the carry was set the code got increasingly convoluted and i repeatedly made incorrect assumptions that threw me off course i turned around to ask patrick a question and found an empty desk it was nearly midnight i set the office alarm and rode my bike home i got in late the next morning i had not slept well and it had taken me hours to fall sleep each time i closed my eyes i saw fast moving assembly instructions in large bright letters i walked straight to dave s desk to discuss the previous night s investigations but he was not interested you were right he said it was a compiler bug i tweaked the c code and i am no longer triggering it that weird assembly is gone i was in the awkward position of having to contradict the compliment i do not know if it is a bug the code i saw would not have been generated by a bug it was definitely a compiler bug i had worked with dave enough to know that the more confidently he spoke the less secure he was but he was tired of being held up by the problem and i dropped it dave walked up to my desk with a grin and a cup from peet s coffee i thought all coffees tasted fine but he was picky so i pretended to be picky so that i could spend time with him walking to peet s i felt a bit stung that he would gone without me this time you remember that compiler bug from tuesday he grinned some more his teeth were yellow from the espresso yep it happened again this morning same file same problem the weird assembly is back the strange thing is that i did not change the c file he was surprisingly mellow about it maybe you changed a header file i am brilliant oh maybe he frowned actually no let me check svn he came back two minutes later no relevant files had changed patrick walked over this is no good here it is a crash but it could be something more subtle in four months our system will be so complex that a subtle problem will take weeks to track down can you look at the compiler s release notes and see if we can upgrade i looked briefly and found what i had expected we were on the latest and most stable version of the recommended major release it did not get more stable than that i went further and searched for some permutation of weird assembly and compiler bug and found nothing i compiled dave s code on my computer and disassembled it the strange code was there at the same place it had been tuesday i recognized the subtract with carry instruction and a few others that seemed unusual i also looked through the rest of the code it was striking how the unusual instructions did not appear anywhere else the rest of the generated code used instructions you would expect after all when do you ever need to subtract and use the existing value of the carry bit i downloaded the source code of the compiler i had never been so overwhelmed it was a tangled mess of compiler passes plug in frameworks embedded languages to describe cpu architectures and abstraction layers i went straight to the files that described the translation from the abstract syntax tree to the machine s assembly i grepped for the subtract with carry it was not there i looked for a few other odd ones some were there most were not at lunch i mentioned my findings the small office we used had a patio which it being mountain view was nearly always usable friday was togo s day i preferred subway but dave hated the bread that is super weird said patrick frowning at the table he said super a lot i think it is a california thing which other ones were not in the translation file i do not remember i said a few more that involved the carry bit some vector instructions patrick looked up that compiler does not do vectorization that is what i just said either the instruction was generated by mistake or you are disassembling non code he was still frowning dave cut in it is definitely code this is what is causing our bug it is being executed and it is not generated by mistake i added it is a math instruction but i do not think it is being used for math it is not nonsense neat we had finally hooked patrick after lunch he and i sat down at my desk and walked through the part of the code i knew best i showed him how although the instructions were obscure and strangely used they did make sense all together there was a deliberate flow of data let us look for a backward jump instruction he said why the target of such a jump may be the top of a loop it is a good place to start the analysis you are actually going to figure out what this code is doing oh yeah he smiled and his eyes lit up it took us the rest of the afternoon to pick through the convoluted jump targets and decode four consecutive instructions that snippet it turns out was finding the sign of an integer anyone else would have done a simple comparison and a jump to set the output register to or but the four instructions were a mess of instructions that all either set the carry bit as a side effect or used it in an unorthodox way you know said patrick this is not even the interesting part i want to know how this code gets in here you said these instructions were not in the translation file that is right they must be elsewhere let us grep the source for both the symbolic version and the op code i did a recursive grep and it came up empty i did not know what to try next try the binary said patrick what binary the binary of the compiler i grepped for the name of the assembly instruction and found nothing but looking for the op code turned up hundreds of hits interesting i said they are not generating individual instructions they are dumping chunks of pre built code why i do not know why i mean why do you say that because these op codes are all together it is not a look up table maybe it is code he said after a second i disassembled the whole compiler and looked for a few of the suspicious instructions and found large sections that looked like the obscure code we had been tracing all afternoon the compiler is infected i said no wonder we could not find it in its source code okay let us start by recompiling the compiler said patrick i will poke around the web to see if anyone s seen this before when you are done with the compiler recompile all our own code and see if dave s bug is gone the compiler took two hours to recompile not including the time i spent learning the convoluted build process meanwhile patrick found nothing online i rebuilt our source tree and ran the tests they failed in the same place maybe the bug was due to something else after all i suggested when patrick walked over disassemble the compiler again he said i did and the foreign codes were still there that cannot be i said this is a fresh build of the official sources they must have gotten infected perhaps someone broke into the download site and replaced them with modified sources i would love to see that code i said and opened my editor to a file in the compiler that is going to take you forever patrick said put a breakpoint in write with a condition of the string containing this op code and this from the guy who pretends not to like debuggers setting the condition was not easy and there were many false positives but the following monday morning i hit the write that outputs the suspicious code i popped up to the c code it was a generic routine to output a buffer i worked backwards to the code that had filled it and it was all straightforward loops based on the translation table which i knew was clean in desperation i started paging through every source file of the compiler looking for any code that might be responsible much of it was manipulating the abstract syntax tree then it occurred to me that the hack could not be there since the back end translation table was clean the hack must be in the back end itself in fact it would have to be after register assignment this narrowed down the search considerably and i spent an hour looking through these files before it was time for lunch monday was pho day we always went to pho world which was so low rent it did not even have a menu but it was a delicious way to spend four dollars eye of round no tripe no cilantro said dave to the little man and we all followed suit i do not know what cut eye of round is but i know it is safe so i could not find it i said either backward from the breakpoint or forward from the code are you still debugging that said dave we have to said patrick we cannot build a product on a foundation like this it is just so odd that the c source would be clean but the assembly have these weird op codes i said i thought you had tracked it down to the compiler said dave i do not mean in your code i mean in the compiler itself the compiler could be responsible for that too no i checked the code but the code is compiled by the compiler i did not understand what dave was saying but patrick had looked up and seemed on the verge of an insight i waited for the lucid explanation so the compiler detects and modifies your program for reasons still unknown but also detects and modifies itself when compiled right said dave with a grin squirting brown sauce in his soup and splattering some of it onto the table how would that work patrick said now in full form a hacker adds this code to the compiler and distributes it to everyone the code detects that it is compiling the compiler and adds itself back into the binary one revision later the hacker removes the code from the official source distribution the hack then perpetuates itself forever with no trace in the source and what is the purpose i said i was skeptical that code could detect and insert itself so flawlessly across multiple versions i do not know we did not get far enough into the analysis of dave s code the obvious thing would be some kind of password validation code modified to always accept some back door password i wish he would not say that such things are obvious so let us go back to an old version of the compiler said dave you mean the binary of an old version the source will not help us i do not think we have binaries around we do not know how far back this goes how about this i said trying to be helpful i will write a utility that you can run on a binary and it will tell you if it detects the suspicious use of these op codes oh good idea i liked patrick it did not take long to write this utility it just ran a binary through the disassembler then through a few greps to find instructions that were definitely not generated by the compiler it found the code in dave s program as well as in the compiler i set it loose on all executables in the system here s the list i said as i walked to patrick s desk it was three printed pages he looked through it this is not good the java runtime the python runtime perl the compiler and a bunch of other programs that probably do not matter why do those runtimes matter because if we cannot trust the c compiler then we have to write a new one which is not all that hard but what language are you going to write it in are you going to trust your new compiler to the hacked python interpreter you are not going to write a new compiler said dave from his desk do not get all conspiracy paranoid it is probably just a bug i left patrick staring at the list and went back to my desk i still did not have an answer to the question i had asked at lunch what is the purpose of this hack i had enjoyed reverse engineering some of these bits of code and frankly i feared that patrick would ask me to write a c compiler in assembly so i put my headphones on and opened the debugger to the part of the c compiler that looked obfuscated again the over use of instructions that involve the carry bit unusual abuse of vector instructions and convoluted and sometimes unnecessary jumps no compiler would generate this this was hand crafted to be difficult to understand i set out to figure out the purpose of this page of code some time later i saw the custodian pick up my trash bin to empty it i took off my headphones dave and patrick were gone it was ten o clock i looked back at the screen of now familiar instructions i had pieced together a rough idea of what it did or at least what some of its parts did i felt a rush of energy i was close to an answer i stayed until morning patrick sat next to me as i pointed at my screen here they use the vector instruction to get a sum of squares which is just a convoluted way to compare these two byte arrays that was the climax of a ten minute explanation wait so what are they doing he asked they are doing pattern matching why all the convoluted crap well it is a fuzzy search and i guess it is pretty fast yeah but this is the most convoluted rube goldberg way of doing something i have ever seen my shoulders sunk and i fiddled with the mouse i looked at my reams of notes he was not even a bit impressed so what now i asked after several seconds of silence i do not know let me catch up on email he left for his desk i was deflated and my muscles ached i spent the rest of the morning browsing feeds and refreshing the live blog of a macworld event at lunch patrick recounted my findings to dave he remembered every detail dave grinned and shook his head with every new complication in the algorithm i was not convinced that he was able to follow it all so quickly and without any code to look at i realized listening to it all again that patrick had been right this was far too convoluted a way of doing something relatively simple have you ever seen those obfuscated programming contests dave asked when patrick was almost finished this is just like that a friend and i competed with each other to write an obfuscated program in college said patrick i do not know how he did it but i started by writing the program normally then incrementally made it more difficult to follow by renaming variables inlining expressions stuff like that that process never would have resulted in what i saw this morning well maybe other people do it differently said dave i feel strange about this code said patrick i do not know how to explain it it feels cold dave and i looked at patrick he was dissolving wasabi into his soy sauce i did not interrupt his thought gathering it is like those chess programs he finally said they do not have intuition about what will work or feelings about the board position or experience to draw on they just try every possible option and pick the best one this feels like that like someone tried every combination of instructions until they got code that did what it was supposed to do there is no beauty the code is ugly and functional i guess that is what i meant by cold how does it know what it is supposed to do i asked well said patrick maybe you could just define boundary cases and run the code to see if the output was right code that works on boundary cases is likely to work for all cases oh i like that said dave grinning i would love to specify the outcome and have the compiler write the code for me well specifying the outcome precisely enough would be no easier than just writing the code said patrick and i felt my smile fade with dave s i wondered whether a drunk patrick would be more fun or at least less of a buzz kill so why would somebody do that then i asked patrick put a roll into his mouth and stared at the center of the round patio table maybe no one did maybe this is all computers doing this i did not know what to say to that i did not think he was kidding okay there is this thing called occam s razor dave started well what is your explanation um not self aware artificially intelligent robot overlords infecting my object marshalling code that is not an explanation have you run an anti virus scan dave liked to pronounce it an tiv erus like the name of a roman emperor does that affect your explanation of why this code looks this way patrick could be a bit brutal dave exhaled and took the question seriously it was hard to refute patrick s point no other explanation made sense we had never seen a compiler generate this kind of code and a human hand crafting the assembly would have a hard enough time writing a self referential op code injecting future proof worm without needlessly using arcane instructions if anything using such instructions would draw attention to the code in fact it had made it easy for my script to detect all infected programs on my system okay so what is your full explanation dave finally asked i do not know said patrick frowning maybe some artificial intelligence program that ran amok oh or you know how computer viruses evade virus scanners by modifying their own code maybe it started out that way with a virus that was programmed to modify itself while retaining the same behavior we were all silent for a few minutes i was trying to see if this explanation was plausible it seemed unlikely but it would only take a single instance of a program somewhere going in this direction and it would grow and propagate wildly half a million new computer viruses are detected each year our brains have no intuition for probabilities involving such numbers a thought occurred to me well wait this did not start in our lab we just got pre built binaries from the official distribution someone must have run into this before i cannot believe that the official compiler is inserting half broken code into binaries and we are the first who have noticed oh that is good said patrick i will post something after lunch an hour later i got an email from patrick with several links to forums where he had posted our findings and asked if anyone had ever seen them before dave followed up with a link to the ask slashdot post he had just submitted where he begged fellow slashdotters to provide an explanation so that his co worker would not force him to write a compiler in assembly language oh nice dave said patrick and i could not tell if that was sarcastic or genuine dave gave a half hearted thanks that told me he was not sure either i spent the afternoon poking through more mystery code and refreshing the forum posts on a few we were ignored completely but on most we were ridiculed the mocking on slashdot was awful all comments were funny they did not seem funny to me i felt my scratchy chin and remembered my sleepless night i rode home in a trance and went straight to bed the next morning i found patrick looking at my print out of infected programs i walked to his desk and stood by him what are you looking for i asked him a way for us to write a compiler in something other than assembly the assembler s not infected asked dave from his desk it is not on this list i turned toward my desk i will double check it what about the browser asked dave can we write it in javascript he was as desperate as i to avoid assembly yeah it is infected said patrick dave leaned back in his chair and covered his face in his hands this is crazy we cannot be writing a compiler in assembly no way it is not that awful said patrick we will spend a day writing useful low level routines and after that assembly s not much more painful than c and the compiler will not have an optimizer or anything i bet it does not even need to support floating point we just need to compile the compiler with it it only needs to work on a single program that program took me two hours to compile though it is pretty complex do we have to write the linker too asked dave i had not even considered the linker evidently neither had patrick who quickly turned to dave then turned back to the print outs furiously looking for ld it is not on here i guess that makes sense it is easier to insert code when compiling said patrick i did not immediately see why that was wait let us back up said dave last week i was able to fix my bug by modifying the code enough to avoid triggering the um the hack he did not know what to call it either but then it came back without you changing the code said patrick yeah i know but before we write this thing let us at least try to modify the compiler s own code to see if we can get it to generate a clean build patrick paused to consider you could i guess but where would you make the modification remember that we think this was introduced several revisions ago that means the pattern recognition is pretty solid and each test will take you two hours well i can compile it in the background while we start to write this thing and so it went dave downloaded the compiler s source compiled it found the obfuscated op codes mapped it back to the source and hacked at it trying to get a clean build i could sense him losing hope as he realized the obfuscated code was spread throughout the program not just in a few isolated cases meanwhile patrick wrote some basic string and file manipulation routines in assembly and put together a text filter that did nothing useful he wanted to verify that its binary was clean it was i brushed up on my assembly at school we had done some in the operating systems class adding pre emptive scheduling i remember having an unexpected high from programming the metal this way there was something pure and raw about manipulating registers about knowing exactly what was going on i had felt an intimacy with the processor that high level languages had abstracted away by afternoon patrick was ready to give me an assignment writing the pre processor at first it seemed straightforward include files substitute macros add conditionals but then i thought of expressions in conditionals avoiding macro substitutions in strings and i started looking for a dave like easy work around cannot we use the infected one i asked i knew it was on the list but i did not see how an infected version would affect us we could pre process the entire compiler and manually look for tampered code i doubted we would see any but if we did we could also fix it manually patrick liked that idea okay you start on that modify the compiler s build system so it has two steps one that pre processes to temporary files and another that compiles those the plan had back fired my easy work around became not only the tedious and difficult modification of the compiler s convoluted build system but then the mindless flipping through output code side by side with the original looking for signs of tampering patrick meanwhile had started on a simple recursive descent parser i heard him suddenly laugh he turned to me and said i was about to allocate my first abstract syntax tree node when i realized that i do not have malloc the good thing about that project was that every bit of it could be as minimal and inefficient as necessary the only requirement was that the output be correct patrick s malloc just grabbed bytes from the heap and never bothered to free them we continued like this for days patrick would hand us some simple assignment such as implementing a single c library function and dave or i would do it while waiting for a compile to finish dave was not making progress with his own plan he played whack a mole with the various segments of code he was trying to trick into generating benign code but each success would cause a regression elsewhere two weeks later our compiler which had been getting through an increasing fraction of the original compiler got through it all i ran my analysis program on the result and it was clean we compiled the original compiler with itself and the result was clean it was two in the afternoon and we had forgotten lunch in our sprint to the finish line let us start a full rebuild of our code and go eat said patrick at lunch my mind was too wired to relax but too tired to make conversation dave was telling a story about local politics i just wanted the food to arrive so i would have an excuse to stay silent finally i found it too painful to think of anything but our problem you know what bugs me i said awkwardly when dave took a break we have never come close to figuring out the purpose of those modifications patrick answered immediately also eager to stop talking about politics if i am right and this is machine instigated then there does not have to be a purpose why would they do it then asked dave viruses do not spread because they have a purpose said patrick they spread because they are good at spreading so you think this is a virus i asked well it is in the sense that it spreads how do you know it spreads patrick was silent it puts stuff into our code he said finally unsure of his answer it does not put itself into our code said dave that would not make sense i was not writing a compiler that was just some network code well if you are going to spread then network code would be a good routine to infect i felt a bit ashamed that none of us had discussed this before sinking two weeks into a new compiler we had been so eager to get our project back on firm footing that we had not properly investigated the alien in our system but my network code was not talking to a compiler i mean this is a compiler bug a compiler virus i do not understand what you think it is doing dave was getting annoyed i do not know what it is doing said patrick i wonder if it is sending stuff over the network we could check that with wireshark said dave referring to the program that monitored network activity on a machine suddenly all i wanted to do was go back to the office and try it every muscle ached with the desire to stand up and leave the sandwiches arrived and we all immediately wrapped them up and drove back i had already installed wireshark two months earlier to help debug a tcp ip problem patrick was having so we went to my desk i ran the program and recorded a minute of activity that is a lot of stuff dave said as the screen filled up with packets yeah let us pick a common one said patrick i looked through the list and visually picked one that seemed to recur we found that it was ssh and i remembered that i had a shell window tailing a log file i closed it and recorded another minute this time we had fewer packets we picked through them one by one time synchronization arp gmail refreshing programs checking for upgrades we added each to the wireshark filter once we had convinced ourselves they were innocuous i then did a minute capture there were a few more packets all again innocuous it is what we had expected of course dave mumbled something and walked to the kitchen then i had a thought i felt chills down my arm while i frantically searched the papers on my desk looking for the printout then i found it the list of infected programs my eyes zipped down the list cursing myself for not sorting it then my stomach squeezed tight as i found it half way down the second page wireshark patrick guessed what i was looking for and saw the reaction on my face when i had found it i guess we cannot trust it he said trust what said dave coming back with a soda wireshark said patrick it is infected dave rolled his eyes he sat down at his desk and unwrapped his sandwich let us look at the lights on this network switch i said we each had a small switch on our desk the light for my port was flickering every few seconds shut down all the programs we found earlier said patrick i closed the brower the chat program various other daemons and tools i could not shut everything down there is always something left in the operating system such as dhcp but the flashes were infrequent enough that i could corralate them with packets found by wireshark this was good wireshark could not be hiding packets we would have seen them on the switch s lights patrick left for his sandwich and i opened mine i looked at dave he was smiling at us his smile bothered me just because we are paranoid does not mean it is not true i had seen enough amazing coding samples in the previous few weeks to be convinced that something big and serious was happening under our nose no program is useful nowadays without communicating over the network i rejected the idea that a virus crafted so carefully and strangely would restrain its activities to the local machine when the world was a packet away hundreds of programs on my computer were infected i refused to believe they were mute patrick suddenly stood up nearly kicking his chair to the floor he walked into the hallway and came back ten minutes later with a piece of equipment the size of a small suitcase he approached me with it and i shoved everything on my desk to the side to make space it was a digital scope he would gotten from the hardware engineers in the lab upstairs he then reached into his pocket and pulled out a break out cable with rj plugs he put it between my computer and the switch he did not know how to use the scope i brought up a shell window and generated plenty of traffic for him to see eventually he was able to clearly see all my packets i closed the window and we waited shifting our eyes between the scope and the network switch s front panel lights it was only a few seconds until the scope flicked with activity i had been staring at it and could not be sure that the switch had shown nothing i brought up wireshark to have a history of the packets nevermind that said patrick you look at the switch and call out each packet you see i will do that with the scope dave got up and walked casually to us standing behind me i saw the light flash now i said as patrick simultaneously said yes that happened two more times with the words reversed then patrick said now and i had seen nothing this happened again a few seconds later i felt more chills in my arm and in my legs whoa said patrick i looked over and the scope was showing a long stretch of activity i looked back at the switch s lights which were dark dave said you are not going to tell me that the switch is infected and hiding packets it is totally hiding packets said patrick staring at the activity on the scope and putting both hands on his head i looked up at dave his face was pale his eyes darted between the two pieces of equipment i was paralyzed then patrick stood erect in his chair staring at the wall this trance lasted only two seconds before he stood up and ran again into the hallway you already know what happened next he came back with an old switch from years earlier and plugged it in its lights flickered in sync with the scope to packets censored by wireshark and the modern switch i was glad for the vomit it gave me something to do dave left for the bathroom and patrick and i cleaned up using paper towels i kept on my desk but the stench was not enough to suppress the panic and my weak arms shook in fear we sat in silence at the patio table our sandwiches half eaten in front of us none of the things i wanted to say seemed worth saying i alternately convinced myself that we were mistaken and that we were doomed i wished desperately that patrick would say something the patio gate opened and the mailman walked in he stopped by our table picked out a letter from his bundle put it on the table then continued into the building dave grabbed it it is addressed to both of us he said glancing up at patrick he ripped open the side and pulled out a letter hand written on loose leaf paper he fumbled with the sheets for a few seconds then read the letter aloud gentlemen we found your posts online for three years we have been waiting for them scouring the internet and monitoring forums you must know that this has happened before several times the first was four years ago to our team in virginia we found our binaries modified we could recompile the code to clean them but we found the binaries mysteriously modified again only a few hours later the next case was only a few months later to an unrelated team in san diego both the binaries and the source had been modified the source had to be periodically cleaned up because recompilation was not sufficient to solve the problem it was a another year until we found the third case a team in spain the binary was dirty the source was clean but recompiling did not fix the problem the compiler s source code had been modified to insert the strange op codes each team uncovered the worm s weakest point and developed a solution this weakest point was then patched for the next attack each generation pushed the worm deeper into the system now it is your turn not only has the compiler been modified but its source is clean it infects itself on recompilation our own machines have been infected for nine months this way so has the rest of the world s you may wonder then why we were eagerly waiting for your post to explain this we must make two observations the first is that anyone could have guessed these weaknesses it takes a fool to modify a development team s binary expecting it to not be recompiled anyone would have skipped that step there is no need to learn that lesson modifying their source is similarly naive these modifications are technically very sophisticated who would be so technically advanced but so socially naive machines it was not until the third attack that we came up with this hypothesis and now we are convinced if you have studied the modified op codes you may also have come to the same conclusion the op codes were clearly generated by trial and error by generating a random sequence and testing to see if it behaved correctly only a machine would do this the second observation is that all these years the worm has been widely spread but innocuous to infected programs yet it was not innocuous to these four teams they were unable to finish their projects they tried simple work arounds but these work arounds persistently failed only a single team world wide was affected by each generation of the worm this makes sense computers cannot empathize with humans they cannot predict what we are going to do when we run into a problem the machines must have known that their worm has weaknesses but they did not know what these weaknesses were they forced a small team to be affected by the worm until that team found the weakest point and circumvented the worm the machines then patched the weakness and tried again this is a large scale version of what they do when they are generating op codes they try different things until one works instead of planning it out as a human would it is a typically effective way to solve a problem when you are made of silicon by now you have presumably written a new compiler perhaps in assembly language and recompiled everything cleanly we expect a few years to pass before we see the next team post to forums we can already predict their findings the compiler s binary will be clean or rewriting the compiler in assembly will not work the worm will have been pushed deeper perhaps into the text editor the assembler the file system the hard disk interface or the cpu itself dave could not finish i do not think there was much left of the letter anyway he put it down in his lap we were silent for hours eventually patrick left then dave the november night came early and cold but i could not move i replayed our adventure i analyzed each step identified each assumption we had made the big leap was the one about machines being responsible as carl sagan had said extraordinary claims require extraordinary evidence we had no extraordinary evidence we had no evidence at all only the lack of another explanation could this have been done by humans humans write computer viruses all the time it is entirely possible that we had found an everyday virus and had overreacted the team from virginia may just be paranoid lunatics this was far more plausible than the theory of machines i felt a weight lift as i considered this alternative explanation these virus writers might well have written a program to generate op codes randomly they may well have started simply and over years made their attack more sophisticated pushing it deeper into the system perhaps the authors are autistic savants my mind was blank for many minutes when my thoughts returned i found this new theory even more implausible than the one about machines we had no idea what machines could do but i could be pretty sure that no human would approach writing a worm this way if we could have insight into an entity s approach to playing chess and found that it tried all possible paths the only honest theory would be a machine player furthermore for practical purposes it did not matter we had to tell the world of our findings we had to stop the perpetrators before they modified hardware disk interfaces or the cpu we were lost if the worm reached such a low level substrate of our technology i imagined posting online again i remembered the funny slashdot replies who should we tell intel the government the virginia team must have tried why did not anti virus programs detect this i imagined conversations with officials who would laugh at our theories i imagined screaming at them i suddenly stood up clenching my fists and pacing the enclosed patio i imagined yelling at people because i could not imagine yelling at machines like most programmers i had always spoken anthropomorphically about computers but when these computers had finally acted like humans i found their nature ephemeral and soulless i picked up my sandwich in its wrapper and went inside i dumped the sandwich and opened the fridge absent mindedly i stared at the drinks a happier thought occurred to me none of the code we had analyzed seemed malicious other than occasionally bothering a team like ours we had seen no evidence of malice patrick had been right when he said that a virus does not spread because it has a purpose only because it is good at spreading this worm might be a permanent tag along like mitochondria forging a symbiotic relationship with us if we tried to stamp it out then we may corner it into a resistant strain that is hostile to us but this was unnecessary let us just let it be it is unsettling but workable i shut the fridge put on my jacket and walked to the alarm panel the lcd on the panel reminded me that it too was a computer was it infected did it know of our new compiler was it going to let me arm it were the magnetic door locks going to let me out of the building i pressed the digits for the arming code and heard the countdown tones i walked out onto the patio to my bicycle they had permitted me to leave i looked at my bike and smiled at its mechanical simplicity i thought of traffic lights and sewer pumps i thought of my credit card i thought of cars and telephones i wanted to sit down i was vanquished taking humanity back to a pre computer age was impossible i was suddenly ashamed to have participated in our undoing i was a programmer i was a member of the group that had spawned our successors i tucked my right pant leg into my sock and unlocked my bike with the shame came a strange pride not many people can claim to have created a species if it can be called that sure lawyers and doctors and politicians are important but they are just maintaining the current house not creating the future inhabitants of earth i rode out of the parking lot into the empty street i smelled again the flower scent that i associated so strongly with mountain view i had never looked up its name there is nothing wrong with being a transitional species nearly all species had been and in the long run we would be one anyway we remembered lucy we remembered dinosaurs we worshiped dinosaurs the machines would remember us \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "create your free account today to subscribe to this repository for notifications about new releases and build software alongside million developers on github github actions released this dec commit to master since this release\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 5\n",
      "a minute read written bygard aaland engen in this article i will present to you some resources which can be useful to know about if you want to learn javascript or in general want to learn more about the language this is a resource created for developers by developers and is loaded with articles tutorials references and documentation on many subjects such as javascript css and webassembly you can find information aimed towards all kinds of experience here which also applies to the tutorials and the guides when it comes to javascript in particular mdn has a lot to offer and i think that almost everyone that reads this article can learn something here there is one guide on mdn which you need to check out named javascript guide this is a detailed guide that gives an excellent overview of the language and is divided into multiple chapters that cover topics such as functions working with objects promises modules and so on this guide is directed primarily towards those who have previous experience with javascript or other languages in addition to this guide here are some other exciting tutorials on javascript that mdn offer this is a website that provides short on point informative video courses on javascript web frameworks and libraries the courses are for both newcomers and experienced javascript developers and are made by both professionals and open source contributors most of the courses consist of small bite size videos rather than long ones and the videos go straight to the point so that you can save time each course is made up of videos on exciting topics such as egghead has a lot of great content that is free but you have to pay to get full access to all of the content of the website this website and its content are made by a full stack web developer and designer named wes bos from hamilton canada the website has a blog with plenty of references to courses videos and articles that wes bos has made on this blog you can find useful resources on learning the basics but also resources on more advanced topics in the realm of web development some of the resources are free and some you have to pay for if there is one resource you have to check out from mr bos it must be his famous javascript course this is a day vanilla javascript coding challenge where you can join over others to learn vanilla javascript in days with tutorials you cannot make a list like this without mentioning youtube there are just so many great courses on this platform and the best about the platform is that it is free i got into javascript just by watching courses on youtube so i will list some popular channels which i have discovered that has a lot of great video courses on javascript but also web development in general finally i will share two free books with you there you have it a full list of resources about javascript which you can enjoy during the holidays bekk is all about craftmanship and the people crafting it this year we are creating calendars each with daily content articles and podcasts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a minute read written bygard aaland engen in this article i will present to you some resources which can be useful to know about if you want to learn javascript or in general want to learn more about the language this is a resource created for developers by developers and is loaded with articles tutorials references and documentation on many subjects such as javascript css and webassembly you can find information aimed towards all kinds of experience here which also applies to the tutorials and the guides when it comes to javascript in particular mdn has a lot to offer and i think that almost everyone that reads this article can learn something here there is one guide on mdn which you need to check out named javascript guide this is a detailed guide that gives an excellent overview of the language and is divided into multiple chapters that cover topics such as functions working with objects promises modules and so on this guide is directed primarily towards those who have previous experience with javascript or other languages in addition to this guide here are some other exciting tutorials on javascript that mdn offer this is a website that provides short on point informative video courses on javascript web frameworks and libraries the courses are for both newcomers and experienced javascript developers and are made by both professionals and open source contributors most of the courses consist of small bite size videos rather than long ones and the videos go straight to the point so that you can save time each course is made up of videos on exciting topics such as egghead has a lot of great content that is free but you have to pay to get full access to all of the content of the website this website and its content are made by a full stack web developer and designer named wes bos from hamilton canada the website has a blog with plenty of references to courses videos and articles that wes bos has made on this blog you can find useful resources on learning the basics but also resources on more advanced topics in the realm of web development some of the resources are free and some you have to pay for if there is one resource you have to check out from mr bos it must be his famous javascript course this is a day vanilla javascript coding challenge where you can join over others to learn vanilla javascript in days with tutorials you cannot make a list like this without mentioning youtube there are just so many great courses on this platform and the best about the platform is that it is free i got into javascript just by watching courses on youtube so i will list some popular channels which i have discovered that has a lot of great video courses on javascript but also web development in general finally i will share two free books with you there you have it a full list of resources about javascript which you can enjoy during the holidays bekk is all about craftmanship and the people crafting it this year we are creating calendars each with daily content articles and podcasts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "subscribe and get updates dropbox server side software lives in a large monorepo one lesson we have learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole years ago it was reasonable to run our entire test corpus on every commit to the repository this scheme became untenable as we added more tests one obvious inefficiency is the pointless and wasteful execution of tests that cannot possibly be affected by a particular change we addressed this problem with the help of our build system code in our monorepo is built and tested exclusively with bazel abstractly bazel views the repository as a set of targets files binaries libraries tests etc and the dependencies between them in particular bazel knows the dependency graph between all source files and tests in the repository we modified our continuous integration system to extract this dependency information from bazel via bazel query and use it to compute the set of tests affected by a particular commit this allows us to greatly reduce the number of tests executed on most commits while still being correct since a particular test no longer runs on every commit we use its previous history to determine its status on commits where it did not run if a test runs on commit n but is not affected by commit n we can consider the test to have the same status on both commits in this way we propagate a status for every test in the repository for every commit that it exists in to conserve extra resources we do not run all affected tests on all commits we roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period then we execute the rolled up set of affected tests for that period on the last commit in the period the time between rollup builds is a tunable parameter trading resource usage against test result timeliness this batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable however our automated breakage detection system athena is able to bisect failing tests over the entire rollup period to find the precise broken change our production deployment system distributes software to hosts in the form of squashfs images we have a custom bazel rule that builds squashfs images for the deployment system to consume we generally require that software pass its tests before being pushed into our production environment this requirement is enforced by our deployment system historically we allowed pushing software from a particular commit only if all tests in the repository passed on it either by being directly run on the commit or through propagation from previous commits while simple this model does not scale well it is frustrating to have deployment blocked because a completely unrelated test is failing even if all tests pass repository growth over time means takes longer and longer to prove that a particular commit is completely green despite running only affected tests on each commit to break down the old monolithic green system we allow every deployable package to specify release tests the set of tests required to pass before pushing it the test set is described using a subset of bazel s target pattern syntax for example deploying a simple c ping server at dropbox might have a bazel build file like this this file declares a c library binary and test using the standard built in bazel c rules the ping_server sqfs target produces a squashfs containing the ping_server binary ping_server sqfs would be deployable on commits where every test in ping_server and its subpackages had passed as mentioned previously we conserve resources by aggregating test runs across several commit builds this potentially introduces extra latency between when a commit lands and when it is deployable if an engineer makes a change to the ping server and wants to deploy it immediately they can request that our continuous integration system run ping_server sqfs s release tests as soon as their commit lands this happens regardless of where the commit falls in the rollup period we leave the decision of what to to put in release_tests up to individual teams it is common to include a package s own tests as well as the tests of critical dependency libraries more conservative projects might include some of their reverse dependencies tests when we were developing the release tests feature we experimented with automatically generating the test set by inspecting the packaged code s dependencies however we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package continuous integration and deployment with bazel monitoring server applications with vortex modernizing our android build system part i the planning please note sometimes we blog about upcoming products or features before they are released but timing and exact functionality of these features may change from what is shared here the decision to purchase our services should be made based on features that are currently available dropbox inc \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe and get updates dropbox server side software lives in a large monorepo one lesson we have learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole years ago it was reasonable to run our entire test corpus on every commit to the repository this scheme became untenable as we added more tests one obvious inefficiency is the pointless and wasteful execution of tests that cannot possibly be affected by a particular change we addressed this problem with the help of our build system code in our monorepo is built and tested exclusively with bazel abstractly bazel views the repository as a set of targets files binaries libraries tests etc and the dependencies between them in particular bazel knows the dependency graph between all source files and tests in the repository we modified our continuous integration system to extract this dependency information from bazel via bazel query and use it to compute the set of tests affected by a particular commit this allows us to greatly reduce the number of tests executed on most commits while still being correct since a particular test no longer runs on every commit we use its previous history to determine its status on commits where it did not run if a test runs on commit n but is not affected by commit n we can consider the test to have the same status on both commits in this way we propagate a status for every test in the repository for every commit that it exists in to conserve extra resources we do not run all affected tests on all commits we roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period then we execute the rolled up set of affected tests for that period on the last commit in the period the time between rollup builds is a tunable parameter trading resource usage against test result timeliness this batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable however our automated breakage detection system athena is able to bisect failing tests over the entire rollup period to find the precise broken change our production deployment system distributes software to hosts in the form of squashfs images we have a custom bazel rule that builds squashfs images for the deployment system to consume we generally require that software pass its tests before being pushed into our production environment this requirement is enforced by our deployment system historically we allowed pushing software from a particular commit only if all tests in the repository passed on it either by being directly run on the commit or through propagation from previous commits while simple this model does not scale well it is frustrating to have deployment blocked because a completely unrelated test is failing even if all tests pass repository growth over time means takes longer and longer to prove that a particular commit is completely green despite running only affected tests on each commit to break down the old monolithic green system we allow every deployable package to specify release tests the set of tests required to pass before pushing it the test set is described using a subset of bazel s target pattern syntax for example deploying a simple c ping server at dropbox might have a bazel build file like this this file declares a c library binary and test using the standard built in bazel c rules the ping_server sqfs target produces a squashfs containing the ping_server binary ping_server sqfs would be deployable on commits where every test in ping_server and its subpackages had passed as mentioned previously we conserve resources by aggregating test runs across several commit builds this potentially introduces extra latency between when a commit lands and when it is deployable if an engineer makes a change to the ping server and wants to deploy it immediately they can request that our continuous integration system run ping_server sqfs s release tests as soon as their commit lands this happens regardless of where the commit falls in the rollup period we leave the decision of what to to put in release_tests up to individual teams it is common to include a package s own tests as well as the tests of critical dependency libraries more conservative projects might include some of their reverse dependencies tests when we were developing the release tests feature we experimented with automatically generating the test set by inspecting the packaged code s dependencies however we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package continuous integration and deployment with bazel monitoring server applications with vortex modernizing our android build system part i the planning please note sometimes we blog about upcoming products or features before they are released but timing and exact functionality of these features may change from what is shared here the decision to purchase our services should be made based on features that are currently available dropbox inc \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 6\n",
      "as promised in my previous post i will show you how to exploit the printconfig dll with a real world example but what does apple s iphone have to do with it well keep on reading sorry no tl dr some time ago me and my business partner padovah ck were looking for possible privileged file operations exploitable via hardlinks at some point the directory c programdata apple lockdown caught our attention this folder is used by the apple mobile device service which is installed by the itunes software the service running with local system privileges is responsible for handling the communications via usb port with apple devices iphone ipad etc as you can see below standard users can add files in this directory each time a new device is plugged in the driver will write a pairing certificate file in this directory in the form of udid plist where udid is the universal id of the apple device so let us plugin our apple device a pairing certificate will be generated and the permissions set on this file are the following as you can see users have only read access to this file but now comes the funny part if you unplug the device and then plugin again some magic happens granting to users full control over that file we observed this strange behavior using the procmon tool from sysinternals a setsecurity call is made from an elevated context system and will grant full control to users on the resource so the question is can this operation be exploitable yes you got it enter native hardlinks standard windows users do not need special privileges to create this type of links and we can use forshaw s utilties to manage them so why not setting a native hardlink on this file and let him point to a resource where only system has full control this is what we are going to do setting a hardlink from our udid plist file to license rtf located in system folder now we just need to plugin our apple device in order to alter the permissions on destination file and yes it works at this point we have all the pieces of the puzzle we only need to change the destination file to printconfig dll then overwrite it with our own dll start the xps print job and finally enjoy the system shell this exercise is left to the reader here you can watch a video of the poc generic hardlink abuse will no more work in future releases of windows in the latest insider previews ms has added some supplementary checks so if you do not have write access to the destination file you get an access denied error when you try to create a hardlink that is all you missed a few blurs on your udid screenshots likelike thanks fixed likelike fill in your details below or click an icon to log in you are commenting using your wordpress com account log out change you are commenting using your google account log out change you are commenting using your twitter account log out change you are commenting using your facebook account log out change connecting to s notify me of new comments via email notify me of new posts via email \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "as promised in the last post i have put together what you might think of as a best of collection the frailest thing ten years of thinking about the meaning of technology it is an e book consisting of entries written over the span of this blog s history these entries amount to a little over of what i wrote here over the years i think they still hold up pretty well of course a few of these are quite recent the e book is now available for download via gumroad in three file formats that should cover everyone you will note that i have left the pricing altogether up to you you are welcome to download the collection at no cost or you may decide to pay something for it if you are so inclined that is your call i know you will do the right thing winks awkwardly more seriously i recently read tim carmody s reflections from almost exactly two years ago about his vision for unlocking the commons it was about the advantages of a patronage model over a purely consumerist model of support here s a bit from that post please note that this is not fuzzy headed idealism or just sentiment this is as concrete and comprehensive as it gets it is economic thinking that recognizes that goods do not just exist to be used up but are objects of labor produced by and for members of a commonwealth the truth of the transaction is in the whole the most economically powerful thing you can do is to buy something for your own enjoyment that also improves the world this has always been the value proposition of journalism and art it is a nonexclusive good that is best enjoyed nonexclusively i have been mulling this over a good bit as think about my own work moving forward in any case the e book is there for the taking and i do hope you will take it if you do consider leaving a rating on gumroad also please do feel free and encouraged to let others know about it however you see fit penultimately i am immensely grateful to evan selinger for his generous praise of this collection which i will share here if there ever was anything like the golden age of blogging that time has passed as a sign of the times michael sacasas is no longer writing the frailest thing a blog that ran for a decade and played a fundamental role in shaping how i and so many others made sense of the changing technological landscape and the place of humanity within it while so much online commentary oozes outrage and snark sacasas chose to follow a different path motivated by curiosity tempered by reverence for the value of history and committed to patiently unpacking nuanced issues concerning aesthetic moral political and religious values sacasas established himself as the public philosopher of technology this collection of posts is a testament to sacasas s rare ability to have thought aloud online without presenting quick takes that have short shelf lives it is truly a gem that means as much today as when each of the posts was authored i cannot recommend it highly enough evan selinger prof philosophy rochester institute of technology lastly a word about the future of this site it will remain up indefinitely although i may prune it just a bit i may also occasionally post a note about speaking engagements but otherwise this will be it many of you have already done so but just in case do subscribe to the convivial society cheers and as always thanks for reading michael the convivial society link is broken fill in your details below or click an icon to log in you are commenting using your wordpress com account log out change you are commenting using your google account log out change you are commenting using your twitter account log out change you are commenting using your facebook account log out change connecting to s notify me of new comments via email notify me of new posts via email subscribe to the convivial society you can pick up a collection of the best of ten years of writing at the frailest thing here \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the morning paper a random walk through computer science research by adrian colyer benchmarking spreadsheet systems rahman et al preprint a recent twthread drew my attention to this pre print paper when spreadsheets were originally conceived data and formula were input by hand and so everything operated at human scale increasingly we are dealing with larger and larger datasets for example data imported via csv files and spreadsheets are creaking i am certainly familiar with the sinking feeling on realising i have accidentally asked a spreadsheet to open up a file with s of thousands of rows and that my computer is now going to be locked up for an age rahman et al construct a set of benchmarks to try and understand what might be going on under the covers in microsoft excel google sheets and libreoffice calc spreadsheets claim to support pretty large datasets these days e g five million cells for google sheets and even more than that for excel but in practice they struggle at sizes well below this with increasing data sizes spreadsheets have started to break down to the point of being unusable displaying a number of scalability problems they often freeze during computation and are unable to import datasets well below the size limits posed by current spreadsheet systems while a database is barely getting started at rows a spreadsheet could be hanging what learnings from the database community could help improve spreadsheet performance two benchmark suites help generate insights into this question the tl dr summary is that spreadsheets could be a lot better than they are today when operating on larger datasets the seed spreadsheet for the analysis in the paper was a weather data spreadsheet containing rows and columns repeating the experiments with other typical spreadsheet datasets did not yield any new insights first the seed spreadsheet was scaled up x to yield rows and then variations were created with different sampling rates row counts and mix of formula cells to simple value cells the basic spreadsheet operations are summarised in the table below and divided into three groups load update and query simple data loading is linear in spreadsheet size for excel and calc google sheets implements lazy loading for value cells that are outside of the current viewport giving a more constant load time but does not do so for formula cells with formula value datasets excel calc and google sheets fail to meet the interactivity barrier at just and rows respectively in a similar vein sorting causes problems on very small datasets less than k rows sorting triggers formula recomputation that is often unnecessary and can take an unusually long time conditional formatting breaks at around k rows for query operations the authors study filtering selecting matching rows pivoting aggregation e g count and lookups vlookup filtering takes a suspiciously long time for formula value for excel violating interactivity at k row possibly due to formula recomputation the other systems avoid this recomputation but are slower than excel for value only datasets with pivot tables there is a surprise result with calc being x faster than excel and x faster than google sheets however calc and google sheets both do comparatively poorly on aggregation for vlookups both calc and google sheets seem to implement the equivalent of a full table scan not even stopping early once the value has been found excel is more efficient if the data is sorted all told it is a sorry scorecard the follow table summarises how close to their respective documented scalability limit each spreadsheet can get for the different kinds of basic operations finally a metric that penalises marketing departments for exaggeration the optimisation opportunities benchmarks are designed to probe whether or not spreadsheets maintain and use indexes no evidence was found to suggest that they do whether they employ a columnar data layout to improve computation performance they don t whether they exploit shared computation opportunities through common sub expressions in formulas they don t whether they can detect and avoid entirely redundant computation in duplicate formulae they don t and whether or not they take advantage of incremental update opportunities can you guess they don t many ideas from databases could potentially be employed to make spreadsheets more scalable one of the easiest to employ is incremental update for aggregation operations e g the sum of a column can be update based on the previous sum and a delta more aggressively a database backend could be used behind a spreadsheet interface translating formulae into sql queries going further approximate query processing could also help meet interactivity targets overall there is a plethora of interesting and challenging research directions in making spreadsheet systems more effective at handling large datasets we believe our evaluation and the resulting insights can benefit spreadsheet system development in the future and also provide a starting point for database researchers to contribute to the emergent discipline of spreadsheet computation optimization fill in your details below or click an icon to log in you are commenting using your wordpress com account log out change you are commenting using your google account log out change you are commenting using your twitter account log out change you are commenting using your facebook account log out change connecting to s notify me of new comments via email notify me of new posts via email this site uses akismet to reduce spam learn how your comment data is processed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "github is home to over million developers working together to host and review code manage projects and build software together use git or checkout with svn using the web url want to be notified of new releases in docker slim docker slim if nothing happens download github desktop and try again go back if nothing happens download github desktop and try again go back if nothing happens download xcode and try again go back if nothing happens download the github extension for visual studio and try again go back do not change anything in your docker container image and minify it by up to x making it secure too keep doing what you are doing no need to change anything use the base image you want use the package manager you want do not worry about hand optimizing your dockerfile you should not have to throw away your tools and your workflow to have small container images do not worry about manually creating seccomp and apparmor security profiles you should not have to become an expert in linux syscalls seccomp and apparmor to have secure containers even if you do know enough about it wasting time reverse engineering your application behavior can be time consuming docker slim will optimize and secure your containers by understanding your application and what it needs using various analysis techniques it will throw away what you do not need reducing the attack surface of your container what if you need some of those extra things to debug your container you can use dedicated debugging side car containers for that more details below docker slim has been used with node js python ruby java golang rust elixir and php some app types running on ubuntu debian centos alpine and even distroless watch this screencast to see how an application image is minified by more than x when docker slim runs it gives you an opportunity to interact with the temporary container it creates by default it will pause and wait for your input before it continues its execution you can change this behavior using the continue after flag if your application exposes any web interfaces e g when you have a web server or an http api you will see the port numbers on the host machine you will need to use to interact with your application look for the port list and target port info messages on the screen for example in the screencast above you will see that the internal application port is mapped to port on your host note that docker slim will interact with your application for you if you enable http probing with the http probe flag or other related http probe flags some web applications built with scripting languages like python or ruby require service interactions to load everything in the application enable http probing unless it gets in your way node js application images python application images ruby application images golang application images rust application images java application images php application images haskell application images elixir application images note the examples are in a separate repository https github com docker slim examples table of contents generated with doctoc latest version now you can run docker slim in containers and you get more convenient reporting defaults for more info about the latest release see the changelog if the directory where you extracted the binaries is not in your path then you will need to run your docker slim commands from that directory to use the docker image distribution just start using the dslim docker slim container image docker slim version info build profile update version http probe remove file artifacts image_id_or_name example docker slim build my sample app see the usage details section for more details you can also get additional information about the parameters running docker slim run docker slim without any parameters and you will get a high level overview of the available commands run a docker slim command without any parameters and you will get more information about that command e g docker slim build if you want to auto generate a seccomp profile and minify your image use the build command if you only want to auto generate a seccomp profile along with other interesting image metadata use the profile command step one run dockerslim docker slim build your name your app step two use the generated seccomp profile docker run security opt seccomp docker slim directory images your_app_image_id artifacts your name your app seccomp json your other run params your name your app feel free to copy the generated profile you can use the generated seccomp profile with your original image or with the minified image you can use the generated profile with your original image or with the minified image dockerslim created docker run it rm security opt seccomp path_to my sample node app seccomp json p my sample node app slim demo video on youtube the demo runs on mac os x but you can build a linux version note that these steps are different from the steps in the demo video the extracted directory contains two binaries git clone https github com docker slim examples git cd examples node_ubuntu eval docker machine env default optional depends on how docker is installed on your machine and what kind of docker version you are using if the docker host is not running you will need to start it first docker machine start default see the docker connect options section for more details docker build t my sample node app docker slim build my sample node app run it from the location where you extraced the docker slim binaries or update your path env var to include the docker slim bin directory dockerslim creates a special container based on the target image you provided it also creates a resource directory where it stores the information it discovers about your image docker slim directory images target_image_id by default docker slim will run its http probe against the temporary container if you are minifying a command line tool that does not expose any web service interface you will need to explicitly disable http probing by setting http probe false curl http your_docker_host_ip port this is an optional step to make sure the target app container is doing something depending on the application it is an optional step for some applications it is required if it loads new application resources dynamically based on the requests it is processing e g ruby or python you will see the mapped ports printed to the console when docker slim starts the target container you can also get the port number either from the docker ps or docker port container_id commands the current version of dockerslim does not allow you to map exposed network ports it works like docker run p by default or when http probing is enabled explicitly docker slim will continue its execution once the http probe is done running if you explicitly picked a different continue after option follow the expected steps for example for the enter continue after option you must press the enter button on your keyboard if http probing is enabled when http probe is set and if continue after is set to enter and you press the enter key before the built in http probe is done the probe might produce an eof error because docker slim will shut down the target container before all probe commands are done executing it is ok to ignore it unless you really need the probe to finish docker images you should see my sample node app slim in the list of images right now all generated images have slim at the end of its name docker run it rm name slim_node_app p my sample node app slim docker slim global options command command options docker image id or name commands global options to get more command line option information run docker slim without any parameters or select one of the top level commands to get the command specific information to disable the version checks set the global check version flag to false e g check version false or you can use the dslim_check_version environment variable the include path option is useful if you want to customize your minified image adding extra files and directories the include path file option allows you to load multiple includes from a newline delimited file use this option if you have a lot of includes the includes from include path and include path file are combined together future versions will also include the exclude path option to have even more control the continue after option is useful if you need to script docker slim if you pick the probe option then docker slim will continue executing the build command after the http probe is done executing if you pick the timeout option docker slim will allow the target container to run for seconds before it will attempt to collect the artifacts you can specify a custom timeout value by passing a number of seconds you need instead of the timeout string if you pick the signal option you will need to send a usr signal to the docker slim process the include shell option provides a simple way to keep a basic shell in the minified container not all shell commands are included to get additional shell commands or other command line utilities use the include exe and or include bin options note that the extra apps and binaries might missed some of the non binary dependencies which do not get picked up during static analysis for those additional dependencies use the include path and include path file options the from dockerfile option makes it possible to build a new minified image directly from source dockerfile pass the dockerfile name as the value for this flag and pass the build context directory or url instead of the docker image name as the last parameter for the docker slim build command docker slim build from dockerfile dockerfile tag my custom_minified_image_name if you want to see the console output from the build stages when the fat and slim images are built add the show blogs build flag note that the build console output is not interactive and it is printed only after the corresponding build step is done the fat image created during the build process has the fat suffix in its name if you specify a custom image tag with the tag flag the fat suffix is added to the name part of the tag if you do not provide a custom tag the generated fat image name will have the following format docker slim tmp fat image pid_of_docker slim current_timestamp the minified image name will have the slim suffix added to that auto generated container image name docker slim tmp fat image pid_of_docker slim current_timestamp slim take a look at this python examples to see how it is using the from dockerfile flag the use local mounts option is used to choose how the docker slim sensor is added to the target container and how the sensor artifacts are delivered back to the master if you enable this option you will get the original docker slim behavior where it uses local file system volume mounts to add the sensor executable and to extract the artifacts from the target container this option does not always work as expected in the dockerized environment where docker slim itself is running in a docker container when this option is disabled default behavior then a separate docker volume is used to mount the sensor and the sensor artifacts are explicitly copied from the target container the current version of docker slim is able to run in containers it will try to detect if it is running in a containerized environment but you can also tell docker slim explicitly using the in container global flag you can run docker slim in your container directly or you can use the docker slim container in your containerized environment if you are using the docker slim container make sure you run it configured with the docker ipc information so it can communicate with the docker daemon the most common way to do it is by mounting the docker unix socket to the docker slim container some containerized environments like gitlab and their dind service might not expose the docker unix socket to you so you will need to make sure the environment variables used to communicate with docker e g docker_host are passed to the docker slim container note that if those environment variables reference any kind of local host names those names need to be replaced or you need to tell docker slim about them using the etc hosts map flag if those environment variables reference local files those local files e g files for tls cert validation will need to be copied to a temporary container so that temporary container can be used as a data container to make those files accessible by the docker slim container when docker slim runs in a container it will attempt to save its execution state in a separate docker volume if the volume does not exist it will try to create it docker slim state by default you can pick a different state volume or disable this behavior completely by using the global archive state flag if you do want to persist the docker slim execution state which includes the seccomp and apparmor profiles without using the state archiving feature you can mount your own volume that maps to the bin docker slim state directory in the docker slim container by default docker slim will try to create a docker volume for its sensor unless one already exists if this behavior is not supported by your containerized environment you can create a volume separately and pass its name to docker slim using the use sensor volume flag here s a basic example of how to use the containerized version of docker slim docker run it rm v var run docker sock var run docker sock dslim docker slim build your docker image name here s a gitlab example for their dind gitlab ci yml config file docker run e docker_host tcp grep docker etc hosts cut f dslim docker slim build your docker image name here s a circleci example for their remote docker circleci config yml config file used after the setup_remote_docker step if you do not specify any docker connect options docker slim expects to find the following environment variables docker_host docker_tls_verify optional docker_cert_path required if docker_tls_verify is set to on mac os x you get them when you run eval docker machine env default or when you use the docker quickstart terminal if the docker environment variables are configured to use tls and to verify the docker cert default behavior but you want to disable the tls verification you can override the tls verification behavior by setting the tls verify to false docker slim tls verify false build http probe true my sample node app multi you can override all docker connection options using these flags host tls tls verify tls cert path these flags correspond to the standard docker options and the environment variables if you want to use tls with verification docker slim host tcp tls cert path users youruser docker machine machines default tls true tls verify true build http probe true my sample node app multi if you want to use tls without verification docker slim host tcp tls cert path users youruser docker machine machines default tls true tls verify false build http probe true my sample node app multi if the docker environment variables are not set and if you do not specify any docker connect options docker slim will try to use the default unix socket if the http probe is enabled note it is enabled by default it will default to running get with http and then https on every exposed port you can add additional commands using the http probe cmd and http probe cmd file options the http probe cmd option is good when you want to specify a small number of simple commands where you select some or all of these http command options protocol method defaults to get resource path and query string if you only want to use custom http probe command and you do not want the default get command added to the command list you explicitly provided you will need to set http probe to false when you specify your custom http probe command note that this inconsistency will be addressed in the future releases to make it less confusing here are a couple of examples adds two extra probe commands get api info and post submit tries http first then tries https docker slim build show clogs http probe cmd api info http probe cmd post submit my sample node app multi adds one extra probe command post submit using only http docker slim build show clogs http probe cmd http post submit my sample node app multi the http probe cmd file option is good when you have a lot of commands and or you want to select additional http command options here s an example docker slim build show clogs http probe cmd file probecmds json my sample node app multi commands in probecmds json the http probe command file path can be a relative path relative to the current working directory or it can be an absolute path for each http probe call docker slim will print the call status example info http probe call status method get target http attempt error none you can execute your own external http requests using the target port list field in the container info message docker slim prints when it starts its test container docker slim build info container name your_container_name id your_container_id target port list comma_separated_list_of_port_numbers_to_use target port info comma_separated_list_of_port_mapping_records example docker slim build info container name dockerslimk_ _ id aa c bcf dd dae e a b ac e beb f a b c bce a dc ff target port list target port info tcp with this information you can run curl or other http request generating tools curl http localhost you can create dedicated debugging side car container images loaded with the tools you need for debugging target containers this allows you to keep your production container images small the debugging side car containers attach to the running target containers assuming you have a running container named node_app_alpine you can attach your debugging side car with a command like this docker run rm it pid container node_app_alpine net container node_app_alpine cap add sys_admin alpine sh in this example the debugging side car is a regular alphine image this is exactly what happens with the node_alpine app sample located in the node_alpine directory of the examples repo and the run_debug_sidecar command helper script if you run the ps command in the side car you will see the application from the target container you can access the target container file system through proc target_pid root some of the useful debugging commands include cat proc target_pid cmdline ls l proc target_pid cwd cat proc environ cat proc target_pid limits cat proc target_pid status and ls l proc target_pid fd unless the default cmd instruction in your dockerfile is sufficient you will have to specify command line parameters when you execute the build command in dockerslim this can be done with the cmd option other useful command line parameters note that the entrypoint and cmd options do not override the entrypoint and cmd instructions in the final minified image here s a sample build command docker slim build show clogs true cmd docker compose yml mount pwd data data dslim container transform it is used to minify the container transform tool you can get the minified image from docker hub it works pretty well with the sample node js python and ruby java and golang images see the sample applications in the examples repo php support is wip there is already one php example but more needs to be done to support apache and nginx based php apps more testing needs to be done to see how it works with other images you can also run docker slim in the info mode and it will generate useful image information including a reverse engineered dockerfile dockerslim now also generates seccomp usable and apparmor wip need more testing profiles for your container note you do not need docker or above to generate seccomp profiles but you do need it if you want to use the generated profiles yes either way you should test your docker images you do not need to read the language spec and lots of books go through the tour of go and optionally read shades of go and you will be ready to contribute dockerslim will work for any dockerized application however dockerslim automates app interactions for applications with an http api you can use dockerslim even if your app does not have an http api you will need to interact with your application manually to make sure dockerslim can observe your application behavior yes the cmd entrypoint and mount options will help you minify your image the container transform tool is a good example notes you can explore the artifacts dockerslim generates when it is creating a slim image you will find those in docker slim directory images target_image_id artifacts one of the artifacts is a reverse engineered dockerfile for the original image it will be called dockerfile fat if you would like to see the artifacts without running docker slim you can take a look at the examples artifacts directory in this repo it does not include any image files but you will find if you do not want to create a minified image and only want to reverse engineer the dockerfile you can use the info command the current version of dockerslim includes an experimental support for docker images with user commands please open tickets if it does not work for you for older versions of dockerslim where you have non default non root user declared in your dockerfile you can use these workarounds to make sure dockerslim can minify your image example docker slim debug build http probe include path etc passwd your docker image name use an explicit u parameter in docker run example docker run d u your user name p your minified docker image name note that you should be able to avoid including etc passwd if you are ok with using uids instead of text user name in the u parameter to docker run if you see nginx emerg mkdir var lib nginx body failed it means your nginx setup uses a non standard temporary directory nginx will fail if the base directory for its temporary folders does not exist they will not create the missing intermediate directories normally it is var lib nginx but if you have a custom config that points to something else you will need to add an include path flag as an extra flag when you run docker slim you can get around this problem by running dockerslim from a root shell that way it will have access to all exported files dockerslim copies the relevant image artifacts trying to preserve their permissions if the permissions are too restrictive the master app might not have sufficient priviledge to access these files when it is building the new minified image use go or higher to build docker slim you can use earlier version of go but it cannot be lower than go versions prior to have a docker ptrace related bug go kills processes if your app is pid when the monitor is separate from the launcher process it will be possible to user older go versions again tools you can install these tools using the tools get sh shell script in the scripts directory notes you have multiple options to build docker slim the goal is to auto generate seccomp apparmor and potentially selinux profiles based on the collected information some of the advanced analysis options require a number of linux kernel features that are not always included the kernel you get with docker machine boot docker is a great example of that dockerslim was a docker global hack day dockerhackday project it barely worked at the time but it did get a win in seattle and it took the second place in the plumbing category overall since then it is been improved and it works pretty well for its core use cases it can be better though that is why the project needs your help you do not need to know much about docker and you do not need to know anything about go you can contribute in many different ways for example use dockerslim on your images and open a github issue documenting your experience even if it worked just fine irc freenode dockerslim docker hub dslim dockerslim is already taken if the project sounds interesting or if you found a bug see contributing md and submit a pr apache license v see license for details \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 7\n",
      "web scraping or crawling is the fact of fetching data from a third party website by downloading and parsing the html code to extract the data you want but you should use an api for this not every website offers an api and apis do not always expose every piece of information you need so it is often the only solution to extract website data there are many use cases for web scraping so what is the problem the main problem is that most websites do not want to be scraped they only want to serve content to real users using real web browser except google they all want to be scraped by google so when you scrape you have to be careful not being recognized as a robot by basically doing two things using human tools having a human behavior this post will guide you through all the things you can use to cover yourself and through all the tools websites use to block you when you open your browser and go to a webpage it almost always means that you are you asking an http server for some content and one of the easiest ways pull content from an http server is to use a classic command line tool such as curl thing is if you just do a curl www google com google has many ways to know that you are not a human just by looking at the headers for examples headers are small pieces of information that goes with every http request that hit the servers and one of those pieces of information precisely describe the client making the request i am talking about the infamous user agent header and just by looking at the user agent header google now knows that you are using curl if you want to learn more about headers the wikipedia page is great and to make some experiment just go over here it is a webpage that simply displays the headers information of your request headers are really easy to alter with curl and copying the user agent header of a legit browser could do the trick in the real world you would need to set more than just one header but more generally it is not very difficult to artificially craft an http request with curl or any library that will make this request looks exactly like a request made with a browser everybody knows that and so to know if you are using a real browser website will check one thing that curl and library can not do js execution the concept is very simple the website embeds a little snippet of js in its webpage that once executed will unlock the webpage if you are using a real browser then you will not notice the difference but if you are not all you will receive is an html page with some obscure js in it but once again this solution is not completely bulletproof mainly because since nodejs it is now very easy to execute js outside of a browser but once again the web evolved and there are other tricks to determine if you are using a real browser or not trying to execute snippet js on the side with node is really difficult and not robust at all and more importantly as soon as the website has a more complicated check system or is a big single page application curl and pseudo js execution with node become useless so the best way to look like a real browser is to actually use one headless browsers will behave exactly like a real browser except that you will easily be able to programmatically use them the most used is chrome headless a chrome option that has the behavior of chrome without all the ui wrapping it the easiest way to use headless chrome is by calling driver that wraps all its functionality into an easy api selenium and puppeteer are the two most famous solutions however it will not be enough as websites have now tools that allow them to detect a headless browser this arms race that is been going on for a long time while those solutions can be easy to make work on your computer it can be trickier to do this at scale and this problem of managing lots of chrome headless instances is one of the many we solve at scrapingbee written by experts covering all languages discover all the scraping tools you need you have successfully joined our subscriber list do not forget to confirm your email address everyone and mostly front dev knows how every browser behaves differently sometimes it can be about rendering css sometimes js sometimes just internal properties most of those differences are well known and it is now possible to detect if a browser is actually who it pretends to be meaning the website is asking itself are all the browser properties and behaviors matched what i know about the user agent sent by this browser this is why there is an everlasting arms race between scrapers who want to pass themselves as a real browser and websites who want to distinguish headless from the rest however in this arms race web scrapers tend to have a big advantage and here is why most of the time when a javascript code tries to detect whether it is being run in headless mode is when it is a malware that is trying to evade behavioral fingerprinting meaning that the js will behave nicely inside a scanning environment and badly inside real browsers and this is why the team behind the chrome headless mode are trying to make it indistinguishable from a real user s web browser in order to stop malware from doing that and this is why web scrapers in this arms race can profit from this effort one another thing to know is that whereas running curl in parallel is trivial chrome headless while relatively easy to use for small use cases can be tricky to put at scale mainly because it uses lots of ram so managing more than instances of it is a challenge if you want to learn more about browser fingerprinting i suggest you take a look at antoine vastel blog a blog entirely dedicated to this subject that is about all you need to know to understand how to pretend like you are using a real browser let us now take a look at how do you behave like a real human a human using a real browser will rarely request pages per second from the same website so if you want to request a lot of page from the same website you have to trick this website into thinking that all those requests come from a different place in the world i e different i p addresses in other words you need to use proxies proxies are now not very expensive per ip however if you need to do more than k requests per day on the same website costs can go up quickly with hundreds of addresses needed one thing to consider is that proxies ips needs to be constantly monitored in order to discard the one that is not working anymore and replace it there are several proxy solutions in the market here are the most used luminati network blazing seo and smartproxy there is also a lot of free proxy list and i do not recommend using these because there are often slow unreliable and websites offering these lists are not always transparent about where these proxies are located those free proxy lists are most of the time public and therefore their ips will be automatically banned by the most website proxy quality is very important anti crawling services are known to maintain an internal list of proxy ip so every traffic coming from those ips will also be blocked be careful to choose a good reputation proxy this is why i recommend using a paid proxy network or build your own to build your on you could take a look at scrapoxy a great open source api allowing you to build a proxy api on top of different cloud providers scrapoxy will create a proxy pool by creating instances on various cloud providers aws ovh digital ocean then you will be able to configure your client so it uses the scrapoxy url as the main proxy and scrapoxy it will automatically assign a proxy inside the proxy pool scrapoxy is easily customizable to fit your needs rate limit blacklist but can be a little tedious to put in place you could also use the tor network aka the onion router it is a worldwide computer network designed to route traffic through many different servers to hide its origin tor usage makes network surveillance traffic analysis very difficult there are a lot of use cases for tor usage such as privacy freedom of speech journalists in the dictatorship regime and of course illegal activities in the context of web scraping tor can hide your ip address and change your bot s ip address every minutes the tor exit nodes ip addresses are public some websites block tor traffic using a simple rule if the server receives a request from one of the tor public exit nodes it will block it that is why in many cases tor will not help you compared to classic proxies it is worth noting that traffic through tor is also inherently much slower because of the multiple routing thing but sometimes proxies will not be enough some websites systematically ask you to confirm that you are a human with so called captchas most of the time captchas are only displayed to suspicious ip so switching proxy will work in those cases for the other cases you will need to use captchas solving service captchas and deathbycaptchas come to mind you have to know that while some captchas can be automatically resolved with optical character recognition ocr the most recent one has to be solved by hand what it means is that if you use those aforementioned services on the other side of the api call you will have hundreds of people resolving captchas for as low as ct an hour but then again even if you solve capchas or switch proxy as soon as you see one websites can still detect your little scraping job a last advanced tool used by the website to detect scraping is pattern recognition so if you plan to scrap every ids from to for the url www example com product try not to do it sequentially and with a constant rate of request you could for example maintain a set of integer going from to and randomly choose one integer inside this set and then scraping your product this one simple example some websites also do statistic on browser fingerprint per endpoint which means that if you do not change some parameters in your headless browser and target a single endpoint they might block you anyway websites also tend to monitor the origin of traffic so if you want to scrape a website if brazil try not doing it with proxies in vietnam for example but from experience what i can tell is that rate is the most important factor in request pattern recognition sot the slower you scrape the less chance you have to be discovered i hope that this overview will help you understand better web scraping and that you learned things reading this post everything i talked about in this post is things we leverage at scrapingbee a web scraping api to handle thousands of requests per seconds without ever being blocked do not hesitate to test our solution if you do not want to lose too much time setting everything up the first k api calls are on us written by experts covering all languages discover all the scraping tools you need you have successfully joined our subscriber list do not forget to confirm your email address \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "github is home to over million developers working together to host and review code manage projects and build software together use git or checkout with svn using the web url want to be notified of new releases in docker slim docker slim if nothing happens download github desktop and try again go back if nothing happens download github desktop and try again go back if nothing happens download xcode and try again go back if nothing happens download the github extension for visual studio and try again go back do not change anything in your docker container image and minify it by up to x making it secure too keep doing what you are doing no need to change anything use the base image you want use the package manager you want do not worry about hand optimizing your dockerfile you should not have to throw away your tools and your workflow to have small container images do not worry about manually creating seccomp and apparmor security profiles you should not have to become an expert in linux syscalls seccomp and apparmor to have secure containers even if you do know enough about it wasting time reverse engineering your application behavior can be time consuming docker slim will optimize and secure your containers by understanding your application and what it needs using various analysis techniques it will throw away what you do not need reducing the attack surface of your container what if you need some of those extra things to debug your container you can use dedicated debugging side car containers for that more details below docker slim has been used with node js python ruby java golang rust elixir and php some app types running on ubuntu debian centos alpine and even distroless watch this screencast to see how an application image is minified by more than x when docker slim runs it gives you an opportunity to interact with the temporary container it creates by default it will pause and wait for your input before it continues its execution you can change this behavior using the continue after flag if your application exposes any web interfaces e g when you have a web server or an http api you will see the port numbers on the host machine you will need to use to interact with your application look for the port list and target port info messages on the screen for example in the screencast above you will see that the internal application port is mapped to port on your host note that docker slim will interact with your application for you if you enable http probing with the http probe flag or other related http probe flags some web applications built with scripting languages like python or ruby require service interactions to load everything in the application enable http probing unless it gets in your way node js application images python application images ruby application images golang application images rust application images java application images php application images haskell application images elixir application images note the examples are in a separate repository https github com docker slim examples table of contents generated with doctoc latest version now you can run docker slim in containers and you get more convenient reporting defaults for more info about the latest release see the changelog if the directory where you extracted the binaries is not in your path then you will need to run your docker slim commands from that directory to use the docker image distribution just start using the dslim docker slim container image docker slim version info build profile update version http probe remove file artifacts image_id_or_name example docker slim build my sample app see the usage details section for more details you can also get additional information about the parameters running docker slim run docker slim without any parameters and you will get a high level overview of the available commands run a docker slim command without any parameters and you will get more information about that command e g docker slim build if you want to auto generate a seccomp profile and minify your image use the build command if you only want to auto generate a seccomp profile along with other interesting image metadata use the profile command step one run dockerslim docker slim build your name your app step two use the generated seccomp profile docker run security opt seccomp docker slim directory images your_app_image_id artifacts your name your app seccomp json your other run params your name your app feel free to copy the generated profile you can use the generated seccomp profile with your original image or with the minified image you can use the generated profile with your original image or with the minified image dockerslim created docker run it rm security opt seccomp path_to my sample node app seccomp json p my sample node app slim demo video on youtube the demo runs on mac os x but you can build a linux version note that these steps are different from the steps in the demo video the extracted directory contains two binaries git clone https github com docker slim examples git cd examples node_ubuntu eval docker machine env default optional depends on how docker is installed on your machine and what kind of docker version you are using if the docker host is not running you will need to start it first docker machine start default see the docker connect options section for more details docker build t my sample node app docker slim build my sample node app run it from the location where you extraced the docker slim binaries or update your path env var to include the docker slim bin directory dockerslim creates a special container based on the target image you provided it also creates a resource directory where it stores the information it discovers about your image docker slim directory images target_image_id by default docker slim will run its http probe against the temporary container if you are minifying a command line tool that does not expose any web service interface you will need to explicitly disable http probing by setting http probe false curl http your_docker_host_ip port this is an optional step to make sure the target app container is doing something depending on the application it is an optional step for some applications it is required if it loads new application resources dynamically based on the requests it is processing e g ruby or python you will see the mapped ports printed to the console when docker slim starts the target container you can also get the port number either from the docker ps or docker port container_id commands the current version of dockerslim does not allow you to map exposed network ports it works like docker run p by default or when http probing is enabled explicitly docker slim will continue its execution once the http probe is done running if you explicitly picked a different continue after option follow the expected steps for example for the enter continue after option you must press the enter button on your keyboard if http probing is enabled when http probe is set and if continue after is set to enter and you press the enter key before the built in http probe is done the probe might produce an eof error because docker slim will shut down the target container before all probe commands are done executing it is ok to ignore it unless you really need the probe to finish docker images you should see my sample node app slim in the list of images right now all generated images have slim at the end of its name docker run it rm name slim_node_app p my sample node app slim docker slim global options command command options docker image id or name commands global options to get more command line option information run docker slim without any parameters or select one of the top level commands to get the command specific information to disable the version checks set the global check version flag to false e g check version false or you can use the dslim_check_version environment variable the include path option is useful if you want to customize your minified image adding extra files and directories the include path file option allows you to load multiple includes from a newline delimited file use this option if you have a lot of includes the includes from include path and include path file are combined together future versions will also include the exclude path option to have even more control the continue after option is useful if you need to script docker slim if you pick the probe option then docker slim will continue executing the build command after the http probe is done executing if you pick the timeout option docker slim will allow the target container to run for seconds before it will attempt to collect the artifacts you can specify a custom timeout value by passing a number of seconds you need instead of the timeout string if you pick the signal option you will need to send a usr signal to the docker slim process the include shell option provides a simple way to keep a basic shell in the minified container not all shell commands are included to get additional shell commands or other command line utilities use the include exe and or include bin options note that the extra apps and binaries might missed some of the non binary dependencies which do not get picked up during static analysis for those additional dependencies use the include path and include path file options the from dockerfile option makes it possible to build a new minified image directly from source dockerfile pass the dockerfile name as the value for this flag and pass the build context directory or url instead of the docker image name as the last parameter for the docker slim build command docker slim build from dockerfile dockerfile tag my custom_minified_image_name if you want to see the console output from the build stages when the fat and slim images are built add the show blogs build flag note that the build console output is not interactive and it is printed only after the corresponding build step is done the fat image created during the build process has the fat suffix in its name if you specify a custom image tag with the tag flag the fat suffix is added to the name part of the tag if you do not provide a custom tag the generated fat image name will have the following format docker slim tmp fat image pid_of_docker slim current_timestamp the minified image name will have the slim suffix added to that auto generated container image name docker slim tmp fat image pid_of_docker slim current_timestamp slim take a look at this python examples to see how it is using the from dockerfile flag the use local mounts option is used to choose how the docker slim sensor is added to the target container and how the sensor artifacts are delivered back to the master if you enable this option you will get the original docker slim behavior where it uses local file system volume mounts to add the sensor executable and to extract the artifacts from the target container this option does not always work as expected in the dockerized environment where docker slim itself is running in a docker container when this option is disabled default behavior then a separate docker volume is used to mount the sensor and the sensor artifacts are explicitly copied from the target container the current version of docker slim is able to run in containers it will try to detect if it is running in a containerized environment but you can also tell docker slim explicitly using the in container global flag you can run docker slim in your container directly or you can use the docker slim container in your containerized environment if you are using the docker slim container make sure you run it configured with the docker ipc information so it can communicate with the docker daemon the most common way to do it is by mounting the docker unix socket to the docker slim container some containerized environments like gitlab and their dind service might not expose the docker unix socket to you so you will need to make sure the environment variables used to communicate with docker e g docker_host are passed to the docker slim container note that if those environment variables reference any kind of local host names those names need to be replaced or you need to tell docker slim about them using the etc hosts map flag if those environment variables reference local files those local files e g files for tls cert validation will need to be copied to a temporary container so that temporary container can be used as a data container to make those files accessible by the docker slim container when docker slim runs in a container it will attempt to save its execution state in a separate docker volume if the volume does not exist it will try to create it docker slim state by default you can pick a different state volume or disable this behavior completely by using the global archive state flag if you do want to persist the docker slim execution state which includes the seccomp and apparmor profiles without using the state archiving feature you can mount your own volume that maps to the bin docker slim state directory in the docker slim container by default docker slim will try to create a docker volume for its sensor unless one already exists if this behavior is not supported by your containerized environment you can create a volume separately and pass its name to docker slim using the use sensor volume flag here s a basic example of how to use the containerized version of docker slim docker run it rm v var run docker sock var run docker sock dslim docker slim build your docker image name here s a gitlab example for their dind gitlab ci yml config file docker run e docker_host tcp grep docker etc hosts cut f dslim docker slim build your docker image name here s a circleci example for their remote docker circleci config yml config file used after the setup_remote_docker step if you do not specify any docker connect options docker slim expects to find the following environment variables docker_host docker_tls_verify optional docker_cert_path required if docker_tls_verify is set to on mac os x you get them when you run eval docker machine env default or when you use the docker quickstart terminal if the docker environment variables are configured to use tls and to verify the docker cert default behavior but you want to disable the tls verification you can override the tls verification behavior by setting the tls verify to false docker slim tls verify false build http probe true my sample node app multi you can override all docker connection options using these flags host tls tls verify tls cert path these flags correspond to the standard docker options and the environment variables if you want to use tls with verification docker slim host tcp tls cert path users youruser docker machine machines default tls true tls verify true build http probe true my sample node app multi if you want to use tls without verification docker slim host tcp tls cert path users youruser docker machine machines default tls true tls verify false build http probe true my sample node app multi if the docker environment variables are not set and if you do not specify any docker connect options docker slim will try to use the default unix socket if the http probe is enabled note it is enabled by default it will default to running get with http and then https on every exposed port you can add additional commands using the http probe cmd and http probe cmd file options the http probe cmd option is good when you want to specify a small number of simple commands where you select some or all of these http command options protocol method defaults to get resource path and query string if you only want to use custom http probe command and you do not want the default get command added to the command list you explicitly provided you will need to set http probe to false when you specify your custom http probe command note that this inconsistency will be addressed in the future releases to make it less confusing here are a couple of examples adds two extra probe commands get api info and post submit tries http first then tries https docker slim build show clogs http probe cmd api info http probe cmd post submit my sample node app multi adds one extra probe command post submit using only http docker slim build show clogs http probe cmd http post submit my sample node app multi the http probe cmd file option is good when you have a lot of commands and or you want to select additional http command options here s an example docker slim build show clogs http probe cmd file probecmds json my sample node app multi commands in probecmds json the http probe command file path can be a relative path relative to the current working directory or it can be an absolute path for each http probe call docker slim will print the call status example info http probe call status method get target http attempt error none you can execute your own external http requests using the target port list field in the container info message docker slim prints when it starts its test container docker slim build info container name your_container_name id your_container_id target port list comma_separated_list_of_port_numbers_to_use target port info comma_separated_list_of_port_mapping_records example docker slim build info container name dockerslimk_ _ id aa c bcf dd dae e a b ac e beb f a b c bce a dc ff target port list target port info tcp with this information you can run curl or other http request generating tools curl http localhost you can create dedicated debugging side car container images loaded with the tools you need for debugging target containers this allows you to keep your production container images small the debugging side car containers attach to the running target containers assuming you have a running container named node_app_alpine you can attach your debugging side car with a command like this docker run rm it pid container node_app_alpine net container node_app_alpine cap add sys_admin alpine sh in this example the debugging side car is a regular alphine image this is exactly what happens with the node_alpine app sample located in the node_alpine directory of the examples repo and the run_debug_sidecar command helper script if you run the ps command in the side car you will see the application from the target container you can access the target container file system through proc target_pid root some of the useful debugging commands include cat proc target_pid cmdline ls l proc target_pid cwd cat proc environ cat proc target_pid limits cat proc target_pid status and ls l proc target_pid fd unless the default cmd instruction in your dockerfile is sufficient you will have to specify command line parameters when you execute the build command in dockerslim this can be done with the cmd option other useful command line parameters note that the entrypoint and cmd options do not override the entrypoint and cmd instructions in the final minified image here s a sample build command docker slim build show clogs true cmd docker compose yml mount pwd data data dslim container transform it is used to minify the container transform tool you can get the minified image from docker hub it works pretty well with the sample node js python and ruby java and golang images see the sample applications in the examples repo php support is wip there is already one php example but more needs to be done to support apache and nginx based php apps more testing needs to be done to see how it works with other images you can also run docker slim in the info mode and it will generate useful image information including a reverse engineered dockerfile dockerslim now also generates seccomp usable and apparmor wip need more testing profiles for your container note you do not need docker or above to generate seccomp profiles but you do need it if you want to use the generated profiles yes either way you should test your docker images you do not need to read the language spec and lots of books go through the tour of go and optionally read shades of go and you will be ready to contribute dockerslim will work for any dockerized application however dockerslim automates app interactions for applications with an http api you can use dockerslim even if your app does not have an http api you will need to interact with your application manually to make sure dockerslim can observe your application behavior yes the cmd entrypoint and mount options will help you minify your image the container transform tool is a good example notes you can explore the artifacts dockerslim generates when it is creating a slim image you will find those in docker slim directory images target_image_id artifacts one of the artifacts is a reverse engineered dockerfile for the original image it will be called dockerfile fat if you would like to see the artifacts without running docker slim you can take a look at the examples artifacts directory in this repo it does not include any image files but you will find if you do not want to create a minified image and only want to reverse engineer the dockerfile you can use the info command the current version of dockerslim includes an experimental support for docker images with user commands please open tickets if it does not work for you for older versions of dockerslim where you have non default non root user declared in your dockerfile you can use these workarounds to make sure dockerslim can minify your image example docker slim debug build http probe include path etc passwd your docker image name use an explicit u parameter in docker run example docker run d u your user name p your minified docker image name note that you should be able to avoid including etc passwd if you are ok with using uids instead of text user name in the u parameter to docker run if you see nginx emerg mkdir var lib nginx body failed it means your nginx setup uses a non standard temporary directory nginx will fail if the base directory for its temporary folders does not exist they will not create the missing intermediate directories normally it is var lib nginx but if you have a custom config that points to something else you will need to add an include path flag as an extra flag when you run docker slim you can get around this problem by running dockerslim from a root shell that way it will have access to all exported files dockerslim copies the relevant image artifacts trying to preserve their permissions if the permissions are too restrictive the master app might not have sufficient priviledge to access these files when it is building the new minified image use go or higher to build docker slim you can use earlier version of go but it cannot be lower than go versions prior to have a docker ptrace related bug go kills processes if your app is pid when the monitor is separate from the launcher process it will be possible to user older go versions again tools you can install these tools using the tools get sh shell script in the scripts directory notes you have multiple options to build docker slim the goal is to auto generate seccomp apparmor and potentially selinux profiles based on the collected information some of the advanced analysis options require a number of linux kernel features that are not always included the kernel you get with docker machine boot docker is a great example of that dockerslim was a docker global hack day dockerhackday project it barely worked at the time but it did get a win in seattle and it took the second place in the plumbing category overall since then it is been improved and it works pretty well for its core use cases it can be better though that is why the project needs your help you do not need to know much about docker and you do not need to know anything about go you can contribute in many different ways for example use dockerslim on your images and open a github issue documenting your experience even if it worked just fine irc freenode dockerslim docker hub dslim dockerslim is already taken if the project sounds interesting or if you found a bug see contributing md and submit a pr apache license v see license for details \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "retarget someone else s website repixel traditional retargeting has been around for quite some time it is no secret that if someone was recently on your website there is a good chance that they are in the market for whatever it is that you are selling and it is therefore a good best practice to tag your site visitors and serve ads to them but in recent years savvy marketers have been realizing more and more that there is actually a number of ways to leverage the visitors of someone else s website to expand a retargeting program opening up a whole new opportunity for scale in this article we will go through the different options out there some best practices on how to operate an expanded retargeting program and some things to look out for while getting set up the options from facebook launching a share pixel feature to old school pixel swapping to partnership marketing platforms there are a number of ways to tag the visitors of someone else s website and serve ads to them particularly on facebook instagram depending on your specific needs you are likely to find one or more of the below five approaches most helpful repixel recommended repixel is a facebook approved app and it is free to access the marketplace there is no minimum to get started and it is pay as you go direct retargeting is the backbone of the product but once the partner s pixel has been shared from their business manager to yours you can also create a lookalike from directly within audience manager email your pixel it is old school but it works on almost all ad networks if you copy paste your tracking pixel and have a partner place it on their website the pixel will start tracking their visitors from there you can easily spin up a custom audience using url contain partnersite com and you will have a functioning audience to serve ads against even though this technically works there are two major drawbacks for one you have to manage the entire process manually this includes cold emailing potential partners drafting up contracts sending payments the works but even more importantly this approach is explicitly disallowed by several major ad networks most notably facebook within facebook s walled garden if you want to use someone else s pixel data to retarget their site you have to use facebook s built in share pixel functionality and keep everything within their ecosystem if you operate on smaller networks such as quora twitter emailing your pixel is a viable option albeit a manual one perfect audience owned marin software mrin nasdaq perfect audience has a connect program that will let you retarget someone else s website however no marketplace is currently available so while their tech will get the job done you really need to have partners in mind in order to get value in addition it is a trading model so you have to let a partner retarget your site if you want to retarget theirs wove formerly tapfwd unlike perfect audience wove does have a marketplace but instead of being pay as you go you pay an all access fee to get an account pricing depends on your negotiation skills but can be in the thousands in addition wove does not offer website visitor retargeting instead it is a lookalike exchange lastly similar to perfect audience wove has a swapping model so if you want to leverage someone else s pixel data for lookalikes you have to let them access yours dojomojo a bit different than web retargeting but with dojomojo you can partner with other brands to reach their audience by paying to be mentioned in their newsletter what are some best practices create lookalikes once you have found a partner website that has a high performing audience expand your reach by creating a lookalike as the shared pixel accumulates visitors over time the seed lists you have created for your lookalikes will continue to refresh which will keep your lookalikes up to date think outside the box sometimes the best audiences are not in your industry for example if you sell a joint pain supplement a blog about joint pain relief and a review website ranking the best joint pain remedies would obviously be excellent partners but so might a blog about golfing or hiking customize your ad creative for different partners expanding on the joint pain example when retargeting the visitors of a golf blog you know the people in this audience are golfers and have joint pain so do not forget to use both pieces of information when crafting your ads save searches by saving a search you will be able to access that search query again without needing to set up all of your filters over and over and get notified if a new website is added to the marketplace that falls within your parameters although you can turn this off request site owners that are not currently in the marketplace if you have a website in mind that you would like to retarget but it does not exist in the marketplace that does not necessarily mean the owner of the website would not be willing to work with you if you ask let repixel take care of the legwork by navigating to the bottom left of the marketplace and clicking request a company leverage exclusions retargeting other sites is a great way to find the right people to serve ads to but it is also a great way to find people that you should not send ads to for example let us say you own a health food brand retargeting whole foods dr mercola and nutrisystem would be great picks as partners but you might want to team up with companies that sell cigars and twinkies as well it is probably fair to assume that people shopping for these products are not in the market for dried kale so if you exclude them and even a lookalike of them from your campaigns you are likely to see a boost in performance above all as with everything digital advertising test and learn instead of sending out just one repixel request start out by sending break the audiences out into separate ad sets with customized creative monitor performance granularly and optimize the economics cost will vary by platform but within repixel each site has its own cpm determined by the person who owns the website in this context cpm refers to cost per thousand pageviews tagged in other words if you sell whiskey and the cpm of www top whiskeys com is to tag the visitors of their next pageviews the price tag would be similar to ad networks repixel is pay as you go so this could be in the form of day day or anywhere in between when the campaign is wrapped up and it is time to analyze the results including the fee that you pay for the audience is of course an extra line item when calculating your cost pers but if all goes to plan the bump you see in cvr and ctr will more than compensate for that you spent on the audience after all there is no reason someone would possibly be on www top whiskeys com unless they were actively in the market for whiskey is retargeting other sites a good fit for my business it depends if your business sells a very general product or service where the purchasing behavior is not timing specific you are probably okay to rely on ad network s built in targeting functionality for example if the audience you would like to target is all women ages who live in the united states facebook has you covered but if your audience is a specific type of person who is looking for a specific product at a specific time retargeting other sites is definitely going to be worth a test here are a few of the specific benefits how it works step by step retargeting someone else s website through repixel starts with visiting the marketplace to start apply some filters on the left hand side of the page based on your business model as mentioned earlier make sure to save your search so you are notified of new sites that are listed that meet your criteria in the future when you find a site you like click into the listing page there are only two things you need to do here set your bid and set your daily budget when setting your bid one thing to keep in mind is that site owners have the ability to change their price it would be very uncommon to see any large sweeping changes but site owners do tweak their price from time to time with that in mind it is recommended that whenever possible you set your bid to at least greater than their price tag to prevent your campaign from stalling due to a tiny adjustment the site owner cannot see your max bid so there is no reason to not set your bid as high as you are truly willing to pay to tag each pageviews sending out your request once you have sent out your repixel request the owner of the website will get an email letting them know that you would like to retarget their visitors if they deem you non competitive they will click accept and proceed to spin up a brand new pixel within their facebook business manager account and share it with yours from there you will be able to attach that pixel to your ad account s and start building custom audiences just as you would with your own pixel but instead of selecting your pixel simply toggle theirs listing your own website to participate as an advertiser in the repixel marketplace it is not required to list your website but doing so is quick free and any money you earn as a site owner can easily be transferred to your advertiser balance if your core business model is selling a product there is no shame in focusing on that and leveraging the marketplace exclusively as an advertiser but if monetizing your site through other means i e display ads or affiliate links is part of your business posting your site to repixel can be a healthy new revenue stream that will not cannibalize your existing business or cause you to litter your website with more banners after all if your advertisers are having success reaching your audience via display ads there is a very good chance that they will have the same success if not more reaching your audience via large scale ad networks such as facebook instagram as well contact faq pricing for site owners terms of use privacy cookies enter your email address to follow this blog and receive notifications of new posts by email email address subscribe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "you can reach me by oren ravendb net posts comments copyright c ayende rahien a common question i field on many customer inquiries is comparing ravendb to one relational database or another recently we got a whole spate of questions on ravendb vs postgresql and i though that it might be easier to just post about it here rather than answer the question each time some of the differences are general for all or most relational databases but i am also going to touch on specific features of postgresql that matter for this comparison the aim of this post is to provide highlights to the differences between ravendb and postgresql not to be an in depth comparison of the two postgresql is a relational database storing the data using tables columns and rows the tabular model is quite entrenched here although postgresql has the notion of json columns ravendb is a document database which store json documents natively these documents are schema free and allow arbitrarily complex structure the first key point that distinguish these databases is with the overall mindset ravendb is meant to be a database for oltp systems business applications and has been designed explicitly for this postgresql is trying to achieve both oltp and olap scenarios and tends to place a lot more emphasis on the role of the administrator and operations teams for example postgresql requires vacuum statistics refresh manual index creation etc ravendb on the other hand it design to run in a fully automated fashion there is not any action that an administrator needs to take or schedule to ensure that ravendb will run properly ravendb is also capable of configuring itself dynamically adjusting to the real world load it has based on feedback from the operational environment for example the more queries a particular index has the more resources it will be granted by ravendb another example is how ravendb processes queries in general its query analyzer will run through the incoming queries and figure out what is the best set of indexes that you need to answer them ravendb will then go ahead and create these indexes on the fly such an action tends to be scary for users coming from relational databases but ravendb was designed upfront for specifically these scenarios it is able to build the new indexes without adding too much load to the server and without taking any locks other tasks that are typically handled by the dba such as configuring the system are handled dynamically by ravendb based on actual operational behavior ravendb will also cleanup superfluous indexes and reduce the resources available for indexes that are not in common use all of that without a dba to perform acts of arcane magic another major difference between the databases is the notion of schema postgresql requires you to define your schema upfront and adhere to that the fact that you can use json at times to store data provides an important escape hatch but while postgresql allows most operations on json data including indexing them it is unable to collect statistics information on such data leading to slower queries ravendb uses a schema free model documents are grouped into collections similar to tables but without the constraint of having the same schema but have no fixed schema two documents at the same collection can have distinct structure typical projects using json columns in postgresql will tend to pull specific columns from the json to the table itself to allow for better integration with the query engine nevertheless postgresql s ability to handle both relational and document data gives it a lot of brownie points and enable a lot of sophisticated scenarios for users ravendb on the other hand is a pure json database which natively understand json it means that the querying language is much nicer for querying that involve json and comparable for queries that do not have a dynamic structure in addition to being able to query the json data ravendb also allows you to run aggregation using map reduce indexes these are similar to materialized views in postgresql but unlike those ravendb is going to update the indexes automatically and incrementally that means that you can query on large scale aggregation in microseconds regardless of data sizes for complex queries that touch on relationships between pieces of data we have very different behaviors if the relations inside postgresql are stored as columns and using foreign keys it is going to be efficient to deal with them however if the data is dynamic or complex you will want to put it in a json column at this point the cost of joining relations skyrockets for most data sets ravendb on the other hand allow you to follow relationships between documents naturally at indexing or querying time for more complex relationships work ravendb also has graph querying which allow you to run complex queries on the shape of your data i mentioned before that ravendb was designed explicitly for business applications that means that it has a much better feature set around their use case consider the humble customers page which needs to show the customers details recent orders and their total recent support calls etc when querying postgresql you will need to make multiple queries to fetch this information that means that you will have to deal with multiple network roundtrips which in many cases can be the most expensive piece of actually querying the database ravendb on the other hand has the lazy feature which allow you to combine multiple separate queries into a single network roundtrip this seemingly simple feature can have a massive impact on your overall performance a similar feature is related to the includes feature it is very common when you load one piece of data that you want to get related information with ravendb you can indicate that to the database engine which will send you all the results in one shot with a relational database you can use a join with the impact on the shape of the results cartesian products issue and possible performance impact or issue multiple queries simple change but significant improvement over the alternative ravendb is a distributed database by default while postgresql is a single node by default there exists features and options log shipping logical replication etc which allow postgresql to run as a cluster but they tend to be non trivial to setup configure and maintain with ravendb even if you are running a single node you are actually running a cluster and when you have multiple nodes it is trivial to join them into a cluster from which point on you can just manage everything as usual features such as multi master the ability for disconnected work and widely distributed clusters are native parts of ravendb and integrate seamlessly while they tend to be of the some assembly required in postgresql the two databases are very different from one another and tend to be used for separate purposes ravendb is meant to be the application database it excels in being the backend of otlp systems and focus on that to the exclusion of all else postgresql tend to be more general suitable for dynamic queries reports and exploration as well as oltp scenarios it may not be a fair comparison but i have literally built ravendb specifically to be better than a relational database for the common needs of business applications and ten years in i think it still shows significant advantages in that area finally let us talk about performance ravendb was designed based on failures in the relational model i spent years as a database performance consultant going from customer to customer fixing the same underlying issues when ravendb was designed we took that to account the paper oltp through the looking glass and what we found there has some really interesting information including the issue of about of a relational database performance is spent on locking ravendb is using mvcc just like postgresql unlike postgresql ravendb does not need to deal with transaction id wraparound vacuum costs etc instead we maintain mvcc not on the row level but on the page level there is a lot less locks to manage and deal with and far less complexity internally this means that read transactions are completely lock free and do not have to do any coordination with write transactions that has an impact on performance and ravendb can routinely manage to achieve benchmark numbers on commodity hardware that are usually reserved for expensive benchmark machines one of our developers got a new machine recently and did some light benchmarking running in wsl ubuntu on windows ravendb was able to exceed writes sec and reads sec hare the specs and let us be honest we were not really trying hard here but we still got nice numbers a lot of that is by designing how we interact internally to have a much simpler architecture and shape and it shows and the nice thing is that these advantages are cumulative ravendb is fast but you also gain the benefits of the protocol allowing you to issue multiple queries in a single roundtrip the ability to include additional results and dynamically adjusting to the operation environment it just works some more important difference i can use postgresql with full performance for free in commercial products ravendb is restricted to a single core in a cluster which is really poor alex still apples and oranges alex i am not sure how specifically that comment is helpful as regards the points oren brought up in this post if you need free then probably performance admin development etc also need to be free alex if someone does not have the budget for the commercial version of ravendb they will also have limitations perhaps even more serious budgeting for hosting admin development customer service etc travis comparing the performance of sql with nosql is no apples and oranges but comparing the price of two products is peter i can get postgresql as saas or self hosted as well as ravendb where is the difference per core is a really high price tag for some support if i did not need it and i did not need it with hosting postresql as well do not get me wrong ravendb is a wonderful database but comparing it with some other database and its performance it should be notices that with ravendb you get it only for thousand of dollars or only a really stripped configuration otherwise that cannot compete with postgresql performance for free markdown turns plain text formatting into fancy html formatting inline reference style labels titles are optional inline titles are optional reference style setext style atx style closing s are optional ordered without paragraphs unordered with paragraphs you can nest them three or more dashes or asterisks end a line with two or more spaces code blocks delimited by or more backticks or tildas set the id of headings with id at end of heading line no future posts left oh my \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 8\n",
      "front page layout site theme sign up or login to join the discussions sam machkovech dec am utc the next xbox console slated to launch in holiday finally has a name xbox series x the system that was formerly dubbed project scarlett also has a bold vertical design and a slightly modified controller as seen in the above gallery xbox chief phil spencer took the stage at thursday night s the game awards to reveal the new monolith shaped console which gamespot reports is roughly as wide as an xbox one controller and roughly three times as tall its appearance came at the end of a trailer full of apparent xbox series x real time rendering which included halo s master chief a red sports car potentially from the xbox exclusive racing series forza and a soccer match important details were confirmed by a few angles of the new xbox console an apparent disc drive a vent covered top with either painted or backlit green coloring and a slightly modified update to the xbox one gamepad this new controller looks largely like the current generation s default controller but it has a new circle base to its d pad and a new button in the controller s middle that resembles an upload icon from windows spencer has confirmed that this will function as a share button much like a similar button on playstation s dualshock and that the new series x controller will be compatible with existing xbox one systems not just the new xbox series x in a press release issued shortly after the game awards reveal microsoft confirmed good news for anybody with tightly organized shelves near their gaming tv of choice the xbox series x supports both vertical and horizontal orientation further readingthe next xbox is in the wild connecting to current gen xbox one playersspencer spoke at the game awards to offer a few sales pitches about the console though most of these were repeats of statements from this year s e reveal he told players to expect to be instantly absorbed in your games presumably hinting to the console s reliance on ssd architecture for faster game loads he emphasized three bullet points performance speed and compatibility and he announced that our xbox game studios are all working on games for the new console before revealing the first game built with xbox series x in mind hellblade ii senua s saga made by ninja theory a studio microsoft formally acquired in listing image by xbox you must login or create an account to comment join the ars orbital transmission mailing list to get weekly updates delivered to your inbox cnmn collection wired media group conde nast all rights reserved use of and or registration on any portion of this site constitutes acceptance of our user agreement updated and privacy policy and cookie statement updated and ars technica addendum effective ars may earn compensation on sales from links on this site read our affiliate link policy your california privacy rights the material on this site may not be reproduced distributed transmitted cached or otherwise used except with the prior written permission of conde nast ad choices\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "software pcs devices entertainment business developer it other for eighteen years and three console generations the xbox community has shown us the power both games and fun can have on all of us as we enter a new generation of gaming we see a future where you are instantly absorbed in your games where worlds are even more lifelike immersive responsive and surprising and where you are at the center of your gaming experience with the new xbox series x we will realize that vision xbox series x will be our fastest most powerful console ever and set a new bar for performance speed and compatibility allowing you to bring your gaming legacy thousands of games from three generations and more forward with you its industrial design enables us to deliver four times the processing power of xbox one x in the most quiet and efficient way something that is critically important in delivering truly immersive gameplay we also designed xbox series x to support both vertical and horizontal orientation it is bold and unique very much like our fans around the world and the team of collaborators and innovators who built it the name xbox is an expression of our deep history in gaming our team s unrelenting passion and our commitment to both our fans and the future of gaming at microsoft it also signifies our belief that a gaming console should be for just that gaming alongside xbox series x we also unveiled the new xbox wireless controller its size and shape have been refined to accommodate an even wider range of people and it also features a new share button to make capturing screenshots and game clips simple and an advanced d pad derived from the xbox elite series wireless controller the new xbox wireless controller will be compatible with xbox one and windows pcs and will be included with every xbox series x the possibilities of what xbox series x enables developers to achieve was also brought to life this evening with the unveiling of senua s saga hellblade ii a sequel to the award winning hellblade senua s sacrifice from world renowned developers ninja theory the game is being built to leverage the full power of xbox series x the footage shared tonight was captured in engine and reflects the power of xbox series x available to developers to deliver new universes experiences and games in ways you have never imagined bringing developers dreams to life with xbox series x games today deliver amazing visuals and tell an array of different and deep stories that is why when we started work on xbox series x it was vital we continue to advance state of the art visual capabilities for developers while also ensuring they could realize even greater feeling emotion and empathy in their games with xbox series x we will elevate the way games look play and feel we will also remove the technical barriers faced in previous generations and enable developers to create more expansive immersive gaming worlds that invite more players to play from a technical standpoint this will manifest as world class visuals in k at fps with possibility of up to fps including support for variable refresh rate vrr and k capability powered by our custom designed processor leveraging the latest zen and next generation rdna architecture from our partners at amd xbox series x will deliver hardware accelerated ray tracing and a new level of performance never before seen in a console additionally our patented variable rate shading vrs technology will allow developers to get even more out of the xbox series x gpu and our next generation ssd will virtually eliminate load times and bring players into their gaming worlds faster than ever before we are minimizing latency by leveraging technology such as auto low latency mode allm and giving developers new functionality like dynamic latency input dli to make xbox series x the most responsive console ever xbox series x is also designed for a future in the cloud with unique capabilities built into the hardware and software to make it as easy as possible to bring great games to both console and elsewhere xbox series x will deliver a level of fidelity and immersion unlike anything that is been achieved in previous console generations one console four generations of gaming one of the things we are most proud of with xbox series x is the promise we are delivering to our fans who have and continue to invest with xbox thanks to backward compatibility you can expect your gaming legacy thousands of your favorite games across four generations of gaming all your xbox one gaming accessories and industry leading services like xbox game pass to be available when you power on your xbox series x in holiday building on our compatibility promise with xbox series x we are also investing in consumer friendly pathways to game ownership across generations leading the way with our first party titles including halo infinite in we are committed to ensuring that games from xbox game studios support cross generation entitlements and that your achievements and game saves are shared across devices as we branch out and extend gaming to more players around the world console gaming will remain at the heart of our xbox offering game creators around the globe are already hard at work building content for xbox series x and our xbox game studios are developing the largest and most creatively diverse lineup of xbox exclusive games in our history on behalf of team xbox we are excited to enter the future of console gaming with you and cannot wait to share more in consoles jun pm confirm your age to continue consoles apr am consoles mar pm xbox game studios global sites stay connected microsoft\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in addition to enhancing messaging on android with rich communication services rcs and bringing you helpful features with messages we also want to provide a safer messaging experience today we have two new updates to share on that front trustworthy business messages with verified sms sms messages help businesses share useful information with consumers things like one time passwords account alerts or appointment confirmations yet sometimes it can be difficult to trust the identity of these messages which are often sent from a random number some messages may even come from bad actors pretending to be from businesses you trust and ask for private information or link to dangerous websites this is called phishing verified sms for messages rolling out today in a number of countries will help you confirm the true identity of the business that is texting you the feature works by verifying on a per message basis that content is sent by a specific business when a message is verified which is done without sending your messages to google you ll see the business name and logo as well as a verification badge in the message thread verified sms is rolling out gradually on messages in nine countries starting in the u s india mexico brazil the u k france philippines spain and canada with more to come verified sms is just one of our efforts to improve your messages with businesses we also continue to work on enhancing the chats you have on messages with rich business messaging rbm real time spam detection in addition to verifying the businesses sending you messages we are working on protecting you from spam in messages spam protection which works with your message data while keeping your messages private has been available over the past year in a number of countries and is now rolling out broadly in the u s follow us\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google is seeking to address the growing problem of short messaging service sms phishing or smishing by allowing business senders to enroll in a verification service for texts text messages are often used for out of band authentication account alerts and appointments but it can be difficult to for recipients to verify the identity of the senders smishers can also use deceptive links in messages to trick customers to visit malicious sites that can compromise their devices and information on them now businesses can enroll in google s verified sms program allowing them to add their logos to messages messages will also show up with a verified sender badge and provide link previews for users verified sms requires google s messages client for android that supports the rich communications services rcs standard messages with verification do not have to go through google s servers however businesses in the united states india mexico brazil united kingdom france philippines spain and canada will be the first to get verified sms google said google did not reveal the cost of verified sms which is already being used by its indian payments service and three financial institutions worldwide as well as large flower delivery company and travel search engine kayak along with verified sms google said it had added spam detection for messages but only in the us market spam detetion in messages lets users block and report spammers and google said it will not see or store the content of the texts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 9\n",
      "in addition to enhancing messaging on android with rich communication services rcs and bringing you helpful features with messages we also want to provide a safer messaging experience today we have two new updates to share on that front trustworthy business messages with verified sms sms messages help businesses share useful information with consumers things like one time passwords account alerts or appointment confirmations yet sometimes it can be difficult to trust the identity of these messages which are often sent from a random number some messages may even come from bad actors pretending to be from businesses you trust and ask for private information or link to dangerous websites this is called phishing verified sms for messages rolling out today in a number of countries will help you confirm the true identity of the business that is texting you the feature works by verifying on a per message basis that content is sent by a specific business when a message is verified which is done without sending your messages to google you ll see the business name and logo as well as a verification badge in the message thread verified sms is rolling out gradually on messages in nine countries starting in the u s india mexico brazil the u k france philippines spain and canada with more to come verified sms is just one of our efforts to improve your messages with businesses we also continue to work on enhancing the chats you have on messages with rich business messaging rbm real time spam detection in addition to verifying the businesses sending you messages we are working on protecting you from spam in messages spam protection which works with your message data while keeping your messages private has been available over the past year in a number of countries and is now rolling out broadly in the u s follow us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "google is seeking to address the growing problem of short messaging service sms phishing or smishing by allowing business senders to enroll in a verification service for texts text messages are often used for out of band authentication account alerts and appointments but it can be difficult to for recipients to verify the identity of the senders smishers can also use deceptive links in messages to trick customers to visit malicious sites that can compromise their devices and information on them now businesses can enroll in google s verified sms program allowing them to add their logos to messages messages will also show up with a verified sender badge and provide link previews for users verified sms requires google s messages client for android that supports the rich communications services rcs standard messages with verification do not have to go through google s servers however businesses in the united states india mexico brazil united kingdom france philippines spain and canada will be the first to get verified sms google said google did not reveal the cost of verified sms which is already being used by its indian payments service and three financial institutions worldwide as well as large flower delivery company and travel search engine kayak along with verified sms google said it had added spam detection for messages but only in the us market spam detetion in messages lets users block and report spammers and google said it will not see or store the content of the texts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "see allhide authors and affiliations computer imaging techniques are commonly used to preserve and share readable manuscripts but capturing writing locked away in ancient deteriorated documents poses an entirely different challenge this software pipeline referred to as virtual unwrapping allows textual artifacts to be read completely and noninvasively the systematic digital analysis of the extremely fragile en gedi scroll the oldest pentateuchal scroll in hebrew outside of the dead sea scrolls reveals the writing hidden on its untouchable disintegrating sheets our approach for recovering substantial ink based text from a damaged object results in readable columns at such high quality that serious critical textual analysis can occur hence this work creates a new pathway for subsequent textual discoveries buried within the confines of damaged materials in archeologists made a dramatic discovery at en gedi the site of a large ancient jewish community dating from the late eighth century bce until its destruction by fire circa ce excavations uncovered the synagogue s holy ark inside of which were multiple charred lumps of what appeared to be animal skin parchment scroll fragments the israel antiquities authority iaa faithfully preserved the scroll fragments although in the years following the discovery no one produced a means to overcome the irreversible damage they had suffered in situ each fragment s main structure completely burned and crushed had turned into chunks of charcoal that continued to disintegrate every time they were touched without a viable restoration and conservation protocol physical intervention was unthinkable like many badly damaged materials in archives around the world the en gedi scroll was shelved leaving its potentially valuable contents hidden and effectively locked away by its own damaged condition fig image courtesy of the leon levy dead sea scrolls digital library iaa photo s halevi the implementation and application of our computational framework allows the identification and scholarly textual analysis of the ink based writing within such unopened profoundly damaged objects our systematic approach essentially unlocks the en gedi scroll and for the first time enables a total visual exploration of its interior layers leading directly to the discovery of its text by virtually unwrapping the scroll we have revealed it to be the earliest copy of a pentateuchal book ever found in a holy ark furthermore this work establishes a restoration pathway for damaged textual material by showing that text extraction is possible while avoiding the need for injurious physical handling the restored en gedi scroll represents a significant leap forward in the field of manuscript recovery conservation and analysis our generalized computational framework for virtual unwrapping applies to a wide range of damaged text based materials virtual unwrapping is the composite result of segmentation flattening and texturing a sequence of transformations beginning with the voxels of a three dimensional d unstructured volumetric scan of a damaged manuscript and ending with a set of d images that reveal the writing embedded in the scan the required transformations are initially unknown and must be solved by choosing a model and applying a series of constraints about the known and observable structure of the object figure shows the final result for the scroll from en gedi this resultant image which we term the master view is a visualization of the entire surface extracted from within the en gedi scroll the first stage segmentation is the identification of a geometric model of structures of interest within the scan volume this process digitally recreates the pages that hold potential writing we use a triangulated surface mesh for this model which can readily support many operations that are algorithmically convenient ray intersection shape dynamics texturing and rendering a surface mesh can vary in resolution as needed and forms a piecewise approximation of arbitrary surfaces on which there may be writing the volumetric scan defines a world coordinate frame for the mesh model thus segmentation is the process of aligning a mesh with structures of interest within the volume the second stage texturing is the formation of intensity values on the geometric model based on its position within the scan volume this is where we see letters and words for the first time on the recreated page the triangulated surface mesh offers a direct approach to the texturing problem that is similar to solid texturing each point on the surface of the mesh is given an intensity value based on its location in the d volume many approaches exist for assigning intensities from the volume to the triangles of the segmented mesh some of which help to overcome noise in the volumetric imaging and incorrect localization in segmentation the third stage flattening is necessary because the geometric model may be difficult to visualize as an image specifically if text is being extracted it will be challenging to read on a d surface shaped like the cylindrical wraps of scrolled material this stage solves for a transformation that maps the geometric model and associated intensities from the texturing step to a plane which is then directly viewable as a d image for the purpose of visualization in practice this framework is best applied in a piecewise fashion to accurately localize a scroll s highly irregular geometry also the methodology required to map each of these steps from the original volume to flattened images involves a series of algorithmic decisions and approximations because textual identification is the primary goal of our virtual unwrapping framework we tolerate mathematical and geometric error along the way to ensure that we extract the best possible images of text hence the final merging and visualization step is significant not only for composing small sections into a single master view but also for checking the correctness and relative alignments of individual regions therefore it is crucial to preserve the complete transformation pipeline that maps voxels in the scan volume to final pixels in the unwrapped master view so that any claim of extracted text can be independently verified the unwrapping process begins by acquiring a noninvasive digitization that gives some representation of the internal structure and contents of an object in situ there are a number of choices for noninvasive penetrative and volumetric scanning and our framework places no limits on the modality of the scan as enhancements in volumetric scanning methodology for example phase contrast microtomography occur we can take advantage of the ensuing potential for improved images whatever the scanning method it must be appropriate to the scale and to the material and physical properties of the object because of the particularities of the en gedi scroll we used x ray based micro computed tomography micro ct the en gedi scroll s damage creates a scanning challenge how does one determine the correct scan protocol before knowing how ink will appear or even if the sample contains ink at all it is the scan and subsequent pipeline that reveal the writing after several calibration scans a protocol was selected that produced a visible range of intensity variation on the rolled material the spatial resolution was adjusted with respect to the sample size to capture enough detail through the thickness of each material layer to reveal ink if present and detectable the chemical composition of the ink within the en gedi scroll remains unknown because there are no exposed areas suitable for analysis however the ink response within the micro ct scan is denser than other materials implying that it likely contains metal such as iron or lead any analysis necessitates physical handling of the friable material and so even noninvasive methods must be approached with great care although low power x rays themselves pose no significant danger to inanimate materials the required transport and handling of the scroll make physical conservation and preservation an ever present concern however once acquired the volumetric scan data become the basis for all further computations and the physical object can be returned to the safety of its protective archive segmentation which is the construction of a geometric model localizing the shape and position of the substrate surface within the scan on which text is presumed to appear is challenging for several reasons first the surface as presented in the scanned volume is not developable that is isometric to a plane although an isometry could be useful as a constraint in some cases the skin forming the layers of the en gedi scroll has not simply been folded or rolled damage to the scroll has deformed the shape of the skin material which is apparent in the d scanned volume making such a constraint unworkable second the density response of animal skin in the volume is noisy and difficult to localize with methods such as marching cubes third layers of the skin that are close together create ambiguities that are difficult to resolve from purely local shape based operators figure shows four distinct instances where segmentation proves challenging because of the damage and unpredictable variation in the appearance of the surface material in the scan volume double split layering and challenging cell structure left ambiguous layers with unknown material middle left high density bubbling on the secondary layer middle right and gap in the primary layer right our segmentation algorithm applied to the en gedi scroll builds a triangulated surface mesh that localizes a coherent section of the animal skin within a defined subvolume through a novel region growing technique fig the basis for the algorithm is a local estimate of the differential geometry of the animal skin surface using a second order symmetric tensor and associated feature saliency measures an initial set of seed points propagates through the volume as a connected chain directed by the local symmetric tensor and constrained by a set of spring forces the movement of this particle chain through the volume traces out a surface over time figure shows how crucially dependent the final result is on an accurate localization of the skin when the segmented geometry drifts from the skin surface fig a the surface features disappear when the skin is accurately localized fig b the surface detail including cracks and ink evidence becomes visible a texture generated when the surface is only partially localized b texture generated when surface is accurately localized the user can tune the various parameters of this algorithm locally or globally based on the data set and at any time during the segmentation process this allows for the continued propagation of the chain without the loss of previously segmented surface information the segmentation algorithm terminates either at the user s request or when a specified number of slices have been traversed by all of the particles in the chain the global structure of the entire surface is a piecewise composition of many smaller surface sections although it is certainly possible to generate a global structure through a single segmentation step approaching the problem in a piecewise manner allows more accurate localization of individual sections some of which are very challenging to extract although the segmented surface is not constrained to a planar isometry at the segmentation step the model implicitly favors an approximation of an isometry furthermore the model imposes a point ordering constraint that prevents sharp discontinuities and self intersections the segmented surface which has been regularized smoothed and resampled becomes the basis for the texturing phase to visualize the surface with the intensities it inherits from its position in the volume once the layers of the scroll have been identified and modeled the next step is to render readable textures on those layers texturing is the assignment of an intensity or brightness value derived from the volume to each point on a segmented surface the interpretation of intensity values in the original volumetric scan is maintained through the texturing phase in the case of micro ct intensities are related to density brighter values are regions of denser material and darker values are less dense a coating of ink made from iron gall for example would appear bright indicating a higher density in micro ct our texturing method is similar to the computer graphics approach of solid texturing a procedure that evaluates a function defined over r for each point to be textured on the embedded surface in our case the function over r is simply a lookup to reference the value possibly interpolated at that precise location in the volume scan in an ideal case where both the scanned volume and localized surface mesh are perfect a direct mapping of each surface point to its d volume position would generate the best possible texture in practice however errors in surface segmentation combined with artifacts in the scan create the need for a filtering approach that can overcome these sources of noise therefore we implement a neighborhood based directional filtering method which gives parametric control over the texturing the texture intensity is calculated from a filter applied to the set of voxels within each surface point s local neighborhood the parameters fig include use of the point s surface normal direction directional or omnidirectional the shape and extent of the local texturing neighborhood and the type of filter applied to the neighborhood the directional parameter is particularly important when attempting to recover text from dual sided materials such as books in such cases a single segmented surface can be used to generate both the recto and verso sides of the page figure shows how this texturing method improves ink response in the resulting texture when the segmentation does not perfectly intersect the ink position on the substrate in the volumetric scan left intersection of the mesh with the volume right directional texturing with a neighborhood radius of voxels region growing in an unstructured volume generates surfaces that are nonplanar in a scan of rolled up material most surface fragments contain high curvature areas these surfaces must be flattened to easily view the textures that have been applied to them the process of flattening is the computation of a d to d parameterization for a given mesh one straightforward assumption is that a localized surface cannot self intersect and represents a coherent piece of substrate that was at one time approximately isometric to a plane if the writing occurred on a planar surface before it was rolled up and if the rolling itself induced no elastic deformations in the surface then damage is the only thing that may have interrupted the isometric nature of the rolling we approach parameterization through a physics based material model in this model the mesh is represented as a mass spring system where each vertex of the mesh is given a mass and the connections between vertices are treated as springs with associated stiffness coefficients the mesh is relaxed to a plane through a balanced selection of appropriate forces and parameters this process mimics the material properties of isometric deformation which is analogous to the physical act of unwrapping a major advantage of a simulation based approach is the wide range of configurations that are possible under the framework parameters and forces can be applied per vertex or per spring this precise control allows for modeling of not only the geometric properties of a surface but also the physical properties of that surface for example materials with higher physical elasticity can be represented as such within the same simulation although this work relies on computing parameterizations solely through this simulation based method a hybrid approach that begins with existing parameterization methods for example least squares conformal mapping lscm and angle based flattening abf followed by a physics based model is also workable the purely geometric approaches of lscm and abf produce excellent parameterizations but have no natural way to capture additional constraints arising from the mesh as a physical object by tracking the physical state of the mesh during parameterization via lscm or abf a secondary correction step using the simulation method could then be applied to account for the mesh s physical properties the piecewise nature of the pipeline requires a final merge step there can be many individually segmented mesh sections that must be reconciled into a composite master view the shape location in the volume and textured appearance of the sections aid in the merge we take two approaches to the merging step texture and mesh merging texture merging is the alignment of texture images from small segmentations to generate a composite master view this process provides valuable user feedback when performed simultaneously with the segmentation process texture merging builds a master view that gives quick feedback on the overall progress and quality of segmentation however because each section of geometry is flattened independently the merge produces distortions that are acceptable as an efficiently computed draft view but must be improved to become a definitive result for the scholarly study of text mesh merging refers to a more precise recalculation of the merge step by using the piecewise meshes to generate a final high quality master view after all segmentation work is complete individual mesh segmentations are merged in d to create a single surface that represents the complete geometry of the segmented scroll the mesh from this new merged segmentation is then flattened and textured to produce a final master view image because mesh merging is computationally expensive compared to texture merging it is not ideal for the progressive feedback required during segmentation of a scan volume however as the performance of algorithms improves and larger segmented surfaces become practical it is likely that mesh merging will become viable as a user cue during the segmentation process maintaining a provenance chain is an important component of our pipeline the full set of transformations used to generate a final master view image can be referenced so that every pixel in a final virtually unwrapped master view can be mapped back to the voxel or set of voxels within the volume that contributed to its intensity value this is important for both the quantitative analysis of the resulting image and the verification of any extracted text figure demonstrates the ability to select a region and point of interest in the texture image and invert the transformation chain to recover original d positions within the volume the generated geometric transformations can map a region and point of interest in the master view left back to their d positions within the volume right using this pipeline we have restored and revealed the text on five complete wraps of the animal skin of the en gedi scroll an object that likely will never be physically opened for inspection fig the resulting master image fig enables a complete textual critique and although such analysis is beyond the scope of this paper the consensus of our interdisciplinary team is that the virtually unwrapped result equals the best photographic images available in the st century from the master view one can clearly see the remains of two distinct columns of hebrew writing that contain legible and countable lines letters and words fig column lines to from the en gedi scroll these images reveal the en gedi scroll to be the book of leviticus which makes it the earliest copy of a pentateuchal book ever found in a holy ark and a significant discovery in biblical archeology fig without our computational pipeline and the textual analysis it enables the en gedi text would be totally lost for scholarship and its value would be left unknown what is clearly preserved in the master image is part of one sheet of a scripture scroll that contains lines of which have been preserved and another have been reconstructed the lines contain to letters and spaces between letters spaces between the words are indicated but are sometimes minimal the two columns extracted from the scroll also exhibit an intercolumnar blank space as well as a large blank space before the first column that is larger than the column of text this large blank space leaves no doubt that what is preserved is the beginning of a scroll in this case a pentateuchal text the book of leviticus armed with the extraction of this readable text and its historical context discerned from carbon dating and other related archeological evidence scholars can accurately place the en gedi writings in the canonical timeline of biblical text radiocarbon results date the scroll to the third or fourth century ce table s alternatively a first or second century ce date was suggested on the basis of paleographical considerations by yardeni dating the en gedi scroll to the third or fourth century ce falls near the end of the period of the biblical dead sea scrolls third century bce to second century ce and several centuries before the medieval biblical fragments found in the cairo genizah which date from the ninth century ce onward fig hence the en gedi scroll provides an important extension to the evidence of the dead sea scrolls and offers a glimpse into the earliest stages of almost years of near silence in the history of the biblical text as may be expected from our knowledge of the development of the hebrew text the en gedi hebrew text is not vocalized there are no indications of verses and the script resembles other documents from the late dead sea scrolls the text deciphered thus far is completely identical with the consonantal framework of the medieval text of the hebrew bible traditionally named the masoretic text and which is the text presented in most printed editions of the hebrew bible on the other hand one to two centuries earlier the so called proto masoretic text as reflected in the judean desert texts from the first centuries of the common era still witnesses some textual fluidity in addition the en gedi scan revealed columns similar in length to those evidenced among the dead sea scrolls besides illuminating the history of the biblical text our work on the scroll advances the development of textual imaging although previous research has successfully identified text within ancient artifacts the en gedi manuscript represents the first severely damaged ink based scroll to be unrolled and identified noninvasively the recent work of barfod et al produced text from within a damaged amulet however the text was etched into the amulet s thin metal surface which served as a morphological base for the contrast of text although challenging morphological structures provide an additional guide for segmentation that is unlikely to be present with ink based writing in the case of the en gedi scroll for instance the ink sits on the substrate and does not create an additional morphology that can aid the segmentation and rendering process the amulet work also performed segmentation by constraining the surface to be ruled and thus developable to simplify the flattening problem in addition segmented strips were assembled showing letterforms but a complete and merged surface segmentation was not computed a result of using commercial software rather than implementing a custom software framework samko et al describe a fully automated approach to segmentation and text extraction of undamaged scrolled materials their results from known legible manuscripts that can be physically unrolled for visual verification serve as important test cases to validate their automated segmentation approach however the profound damage exhibited in the materials such as the scroll from en gedi creates its own new challenges segmentation texturing and flattening algorithms that only our novel framework directly addresses the work of mocella et al claims that phase contrast tomography generates contrast at ink boundaries in scans of material from herculaneum the hope for phase contrast comes from a progression of volumetric imaging methods and serves as a possible solution to the first step in our pipeline creating a noninvasive volumetric scan with some method that shows contrast at ink although verifying that ink sits on a page is not enough to allow scholarly study of discovered text this is an important prelude to subsequent virtual unwrapping our complete approach makes such discovery possible an overarching concern as this framework becomes widely useful has to do not with technical improvements of the components which will naturally occur as scientists and engineers innovate over the space of scanning segmentation and unwrapping but rather with the certified provenance of every final texture claim that is produced from a scan volume an analysis framework must offer the ability for independent researchers to confidently affirm results and verify scholarly claims letters words and ultimately whole texts that are extracted noninvasively from the inaccessible inner windings of an artifact must be subject to independent scrutiny if they are to serve as the basis for the highest level of scholarship for such scrutiny to occur accurate and recorded geometry aligned in the coordinate frame of the original volumetric scan must be retained for confirmation the computational framework we have described provides this ability for a third party to confirm that letterforms in a final output texture are the result of a pipeline of transformations on the original data and not solely an interpretation from an expert who may believe letterforms are visible such innate third party confirmation capability reduces confusion around the resulting textual analyses and engenders confidence in the effectiveness of virtual unwrapping techniques the traditional approach of removing a folio from its binding or unrolling a scroll and pressing it flat to capture an accurate facsimile obviously will not work on fragile manuscripts that have been burned and crushed into lumps of disintegrating charcoal the virtual unwrapping that we performed on the en gedi scroll proves the effectiveness of our software pipeline as a noninvasive alternative a technological engine for text discovery in the face of profound damage the implemented software components which are necessary stages in the overall process will continue to improve with every new object and discovery however the separable stages themselves from volumetric scanning to the unwrapping and merging transformations will become the guiding framework for practitioners seeking to open damaged textual materials the geometric data passing through the individual stages are amenable to a standard interface through which the software components can interchangeably communicate hence each component is a separable piece that can be individually upgraded as alternative and improved methods are developed for example the accurate segmentation of layers within a volume is a crucial part of the pipeline segmentation algorithms can be improved by tuning them to the material type for example animal skin papyrus and bark the expected layer shape for example flat and rolled pages and the nature of damage for example carbonized burned and fragmented the flattening step is another place where improvements will better support user interaction methods to quantify and visualize errors from flattening and a comparative analysis between different mapping schemes the successful application of our virtual unwrapping pipeline to the en gedi scroll represents a confluence of physics computer science and the humanities the technical underpinnings and implemented tools offer a collaborative opportunity between scientists engineers and textual scholars who must remain crucially involved in the process of refining the quality of extracted text although more automation in the pipeline is possible we have now achieved our overarching goal which is the creation of a new restoration pathway a way to overcome damage to reach and retrieve text from the brink of oblivion the master view image of the en gedi scroll was generated using the specific algorithms and processes outlined below because we use the volumetric scan as the coordinate frame for all transformations in our pipeline the resolution of the master view approximately matches that of the scan the spatial resolution of the volume m voxel isometric produces an image resolution of approximately pixels per inch m inch which can be considered archival quality from this we estimate the surface area of the unwrapped portion to be approximately cm in the average size of letterforms varies between and mm and the pixels of the master view maintain the original dynamic range of bits two volumetric scans were performed using a bruker skyscan in vivo micro ct machine it uses a panalytical microfocus tube and a princeton instruments camera with a maximum spatial resolution of m per voxel it more than exceeded the resolution requirements for the en gedi scroll a spatial resolution of m was used for all en gedi scans additionally because this is an in vivo machine the scroll could simply be placed within the scan chamber and did not need to be mounted for scanning this limited the risk of physical damage to the object an initial single field of view scan was done on the scroll to verify the scan parameters and to confirm that the resolution requirements had been met this scan was performed at kv a and ms exposure time with added filtration al to improve image quality by absorbing lower energy x rays that tend to produce scattering the reconstructions showed very clear separation of layers within the scroll which indicated a good opportunity for segmentation the scan protocol was then modified to increase contrast where the team suspected that there may be visible ink to acquire data from as much of the scroll as possible the second and final scan was an offset scan using four connected scans final exposure parameters of kv a and ms were selected for this scan the data were reconstructed using bruker skyscan s nrecon engine and the reconstructed slices were saved as bit tiff images for further analysis the basis for the algorithm is a local estimate of the differential geometry of the animal skin surface using a second order symmetric tensor and associated feature saliency measures the tensor based saliency measures are available at each point in the volume the d structure tensor for point p is calculated as where u p is the d gradient at point p g is a d gaussian kernel and denotes convolution the eigenvalues and eigenvectors of this tensor provide an estimate of the surface normal at p the algorithm begins with an ordered chain of seed points localized to a single slice by a user from the starting point each particle in the chain undergoes a force calculation that propagates the chain forward through the volume this region growing algorithm estimates a new positional offset for each particle in the chain based on the contribution of three forces gravity a bias in the direction of the long axis of the scroll neighbor separation and the saliency measure of the structure tensor first n p is biased along the long axis of the scroll by finding the vector rejection of an axis aligned unit vector z onto n p to keep particles moving together an additional restoring spring force s is computed using a spring stiffness coefficient k and the elongation factors xl and xr between the particle and both its left and right neighbors in the chain in the final formulation a scaling factor  is applied to g and the final positional offset for the particle is the normalized summation of all terms the intuition behind this framework is the following the structure tensor estimates a surface normal which gives a hint at how a layer is moving locally through points eqs and the layer should extend in a direction that is approximated locally by its tangent plane the surface normal the gravity vector nudges points along the major axis around which the surface is rolled which is a big hint about the general direction to pursue to extend a surface moving outward away from the major axis and across layers generally defeats the goal of following the same layer the spring forces help to maintain a constant spacing between points restraining them from moving independently these forces must all be balanced relative to one another which is done by trial and error we applied the computed offset iteratively to each particle resulting in a surface mesh sampled at a specific resolution relative to the time step of the particle system the user can tune the various parameters of this algorithm based on the data set and at any time during the segmentation process we also provided a feedback interface whereby a human user can reliably identify a failed segmentation and correct for mistakes in the segmentation process for each small segmentation we used spring force constants of and an  scaling factor of segmentation chain points had an initial separation of voxel this generally produced about six triangles per m in the segmented surface models when particles crossed surface boundaries because the structure tensor did not provide a valid estimate of the local surface normal the chain was manually corrected by the user two shapes were tested for the shape of the texturing neighborhood a spheroid and a line the line shape includes only those voxels that directly intersect along the surface normal when the surface normal is accurate and smoothly varying the line neighborhood allows parametric control for the texture calculation to incorporate voxels that are near but not on or within the surface the line neighborhood leads to faster processing times and less blurring on the surface although the spheroid neighborhood supports more generalized experimentation the line neighborhood is a degenerate spheroid we settled on bidirectional neighborhoods voxels in both the positive and negative direction using a line neighborhood with a primary axis length of voxels we filtered the neighborhood using a max filter because the average ink response density in the volume was much brighter than the ink response of the animal skin our flattening implementation makes use of bullet physics library s soft body simulation which uses position based dynamics to solve the soft body system points along one of the long edges of the segmentation were pinned in place while a gravity force along the x axis was applied to the rest of the body this roughly unfurled the wrapping and aligned the mesh with the xz plane a gravity force along the y axis was then applied to the entire body which pushed the mesh against a collision plane parallel to the xz plane this action flattened the curvature of the mesh against the plane a final expansion and relaxation step was applied to smooth out wrinkles in the internal mesh in total around small segmentations were generated during the segmentation process these segmentations were then mesh merged to produce seven larger segmentations a little over one for each wrap of the scroll each large segmentation was then flattened and textured individually the final set of seven texture images was then texture merged to produce the final master view imagery for this paper all merging steps for this work were performed by hand texture merges were performed in adobe photoshop and mesh merging was performed in meshlab an enhancement curve was uniformly applied to the merged master view image to enhance visual contrast between the text and substrate supplementary material for this article is available at http advances sciencemag org cgi content full e dc table s radiocarbon dating results of the en gedi scroll this is an open access article distributed under the terms of the creative commons attribution noncommercial license which permits use distribution and reproduction in any medium so long as the resultant use is not for commercial advantage and provided the original work is properly cited vol no september thank you for your interest in spreading the word about science advances note we only request your email address so that the person you are recommending the page to knows that you wanted them to see it and that it is not junk mail we do not capture any email address log in with your aaas login please log in to add an alert for this article log in with your aaas login by william brent seales clifford seth parker michael segal emanuel tov pnina shor yosef porath science advances sep e computational virtual unwrapping of the en gedi scroll reveals it to be the earliest pentateuchal book ever found in a holy ark by william brent seales clifford seth parker michael segal emanuel tov pnina shor yosef porath science advances sep e computational virtual unwrapping of the en gedi scroll reveals it to be the earliest pentateuchal book ever found in a holy ark vol issue table of contents american association for the advancement of science all rights reserved aaas is a partner of hinari agora oare chorus clockss crossref and counter science advances issn \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "can we tap ionized particles in the interstellar medium as a way of exchanging momentum for propulsion it is a concept with a lot of pluses if it can be made to work chief among them the fact that such a device would be propellantless looking at the topic today is drew brisbin a postdoctoral researcher in astronomy who received his phd from cornell university in dr brisbin has since gone on to work towards better understanding his field of specialization the study of galaxy evolution in the early universe he currently works at universidad diego portales in santiago chile where he collaborates closely with other researchers using some of the most sensitive telescopes in the world located in the mountainous chilean desert in addition to his formal work and outdoors oriented hobbies he also enjoys dreaming about the future of humanity one particular dream recently seemed to warrant some further investigation leading him to the ideas he explains today by drew brisbin this article represents a distillation of a work published in april in the journal of the british interplanetary society reference some technical details have been omitted for brevity but interested readers are encouraged to read the original publication which is freely available at https arxiv org abs additional commentary has been included to address what the author sees as a critical flaw in dr robert zubrin s dipole drive concept in particular light of the recent public excitement and ensuing disappointment regarding the exotic em drive it is worthwhile to point out that space travel without on board propellant is eminently possible based on well established physical principles here a new mode of transport is proposed which relies on electric field moderated momentum exchange with the ionized particles in the interstellar medium the application of this mechanism faces significant challenges requiring industrial scale exploitation of space but the technological roadblocks are different than those presented by light sails or particle beam powered craft this mode of space travel is well suited to energy efficient travel at velocities below about five percent the speed of light x c and compares exceptionally well to light sails on an energy expenditure basis it therefore represents an extremely attractive mode of transport for slow of order multi century long voyages carrying massive payloads to nearby stellar neighbors this will be a useful niche for missions that would otherwise be too energy intensive to carry out including initial forays into nearby stellar systems with observatory probes or long term transport of bulk materials as a precursor mission to set up colony infrastructure the tyranny of the rocket equation has long been recognized as an impediment to becoming a truly spacefaring species due to the exorbitant reaction mass required for traditional rockets in interstellar travel there has been considerable attention to methods of space travel that circumvent the rocket equation laser driven light sails are a prominent and long standing idea see for example and references therein while light sails are well established and also the engines of the widely publicized breakthrough starshot program and project dragonfly their thrust is fundamentally limited to n gw for comparison the three gorges dam the largest capacity power plant currently in operation has a capacity of about gw if this power was transmitted with perfect efficiency to a light sail it would provide thrust equivalent to the force required to lift a kg mass on earth scaling light sails up to larger than gram scale spacecraft therefore necessarily depends on humanity s ability to harness incredible power furthermore since light is only able to push it is very difficult for light sail spacecraft to slow down at their destination limiting missions to fly bys unless complicated reflecting infrastructure can be sent ahead of the craft alternatively direct sunlight could be used as a source of photon pressure unfortunately the material properties suggested to be necessary for a practical interstellar solar sail require materials with extremely low areal densities with  g m current state of the art reflective films developed for light sails reach areal densities of order g m or four orders of magnitude too dense even without including any support structure or payload so it is uncertain when if ever suitable materials will be developed for an interstellar solar sail another idea using external reaction mass is the particle beam powered spacecraft this hinges on a sail formed by an extended electric or magnetic field which is able to deflect a remotely beamed stream of charged particles since charged particles carry much more momentum per unit energy than photons this could have much lower power requirements than light sails this concept has its origins in the magsail a large loop of current carrying wire which deflects passing charged particles in the interstellar medium ism eliciting a drag force which could be used as a brake to slow spacecraft down to rest with respect to the ism after a high speed journey to provide acceleration one could simply supplant the ism with a beamed source of high velocity charged particles providing a long distance beam of charged particles is however quite difficult because of beam divergence due to particle thermal motion interaction with interplanetary or interstellar magnetic fields and electrostatic beam expansion in the case of non neutral particle beams andrews suggests that it would be necessary to construct a highway of beam generators at least every au or so along the route on which the craft accelerates the related concept of the electric sail instead uses an electric field generated by a positively charged grid of wires or wire spokes extending from a central hub to push against the outward streaming solar wind this concept has the near term potential to allow travel within our own stellar neighborhood with very low energy costs the electric sail like the magsail however ultimately relies on a drag force decelerating the spacecraft to rest with respect to the surrounding medium the outward moving solar wind in this case it is therefore unable to accelerate beyond the heliosphere nor can it accelerate directly inwards towards the sun while in the heliosphere it would be possible to overcome these obstacles by actively pushing against the charged particles of the ism rather than passively coming to rest with respect to the medium these spacecraft with interstellar medium momentum exchange reactions swimmers can accelerate with respect to the ism are significantly more energy efficient than light sails would be able to decelerate at their destination do not require pre established infrastructure along the route and are based on elementary physical principles recently dr robert zubrin discussed his independent work on a dipole drive concept which is similar to the swimmer concept described here although the two ideas are related and even share a similar geometry they were arrived at independently furthermore the dipole drive as described suffers from a flaw which prevents its successful acceleration in the stationary ism the work presented here concerns the conceptual mechanism which allows swimmers to accelerate through a stationary ism both the magsail and electric sail concepts rely on the fact that there is significant mass in the ism or the heliosphere which can interact with relatively low mass structures consisting of charged or current carrying wires how then could a spacecraft interact to accelerate rather than decelerate with respect to the surrounding medium quite generally this will require a time varying electric field which can do work on the surrounding particles of the ism as a thought experiment imagine a spacecraft consisting of a pair of conducting plates arranged in a parallel plate capacitor style configuration with a switchable power source connecting them and able to charge and discharge them at will the conducting plates rather than being solid are composed of a wire mesh with the vast majority of the area taken up by open space rather than metal such that particles are easily able to pass through the plate mesh without collision the spacecraft is moving face on through a stationary medium of charged particles like the interstellar medium as shown schematically in fig for the moment take the charged particles to be macroscopic and extremely dispersed so we can easily see individual particles and identify when they are in the vicinity of the spacecraft charged pebbles rather than atoms or elementary particles as the spacecraft moves through the field of charged particles we can strategically switch the power source on and off to create an electric field and push on charged particles as they pass between the conducting plates accelerating the particles backward and creating thrust to push the spacecraft forward fig this scenario is perfectly in line with conservation laws momentum is conserved since the particle gains momentum in the backward direction and the spacecraft gains momentum in the forward direction energy is conserved as the increase in kinetic energy of the particle and the spacecraft is drawn directly from the power source depleting whatever energy source is being used depleting a battery s chemical energy or converting beamed laser light for instance in practice of course the ism is not made of macroscopic easily separable pebbles but microscopic ions and electrons with tens of thousands per cubic meter or more so we cannot consider manually switching the charged plates based on the positions of individual particles the possibility of simply leaving the plates continuously charged front facing plate positive back plate negative may initially seem workable and this appears to be the scenario imagined in the dipole drive introductory electricity courses train us to think about parallel plate capacitors as having a strong field in one direction between a pair of charged plates and no field outside the plates so it is easy to initially imagine that a positively charged ion would approach the front plate while feeling absolutely no force feel a strong force backwards while between the plates and then feel absolutely no force again as it recedes beyond the back plate in fact however a set of finite parallel plates will indeed have electric fields outside of their gap which directly oppose the electric field inside the gap and would perfectly negate the thrust generated by particles transitioning the gap this is made more clear by considering the electrical voltage rather than electric fields fig top shows schematically the voltage through the center of an idealized infinite parallel plate capacitor charged to a potential difference of v positive charges will want to slide down the potential ramp located between the plates accelerating rightward for a finite sized set of parallel plates the voltage extends a bit to the left and right of the gap continuously decaying from the voltage at the plates to a voltage of zero at great distances as shown in fig middle note that these voltage ramps tilt in the other way than the region between the plates and will tend to accelerate positive charges in the opposite direction for a particle entering in from the left and making it all the way through to exit out the right side would it end up with more or less rightward velocity remember that voltage often referred to as electric potential is simply potential energy divided by charge in si units volt joule coulomb starting out very far on the left in fig the rightward traveling particle with charge q will have some kinetic energy ke and zero potential energy as it approaches the front plate it will begin slowing down as it rises up the potential ramp and converting kinetic energy to potential energy eventually reaching a peak potential energy of qv with a kinetic energy of ke qv as it traverses the gap it is accelerated rightward as it slides down the potential eventually reaching a potential energy of qv and a kinetic energy of ke qv it then exits the parallel plate gap and is again forced up a potential ramp converting kinetic energy back into potential energy until it finally reaches distances far away from the plates where the voltage is at which point it has potential energy and kinetic energy ke exactly the same as it started this is not a minor fluke of this particular geometry either any arrangement of charged plates so long as the voltage is finite and the plate volume is finite will leave the potential at infinity in zubrin has suggested that debye shielding the phenomenon of oppositely charged free particles in a plasma tending to cluster around charged objects and screen out the electric field would somehow ameliorate this issue but that is not the case the effect of debye shielding will be for the front facing positive plate to accumulate a cloud of electrons and the back plate to accumulate a cloud of positive ions making the voltage ramps just outside the plate pair more steeply return to zero as shown in fig bottom nonetheless the potential remains at large distances and in the end passing particles enter and leave the system with the exact same kinetic energy this does not preclude the particles from changing direction either being reflected back in the direction they came or deflected to the side if they interact with the parallel plates at an angle so such a configuration could certainly be used to either steer or decelerate with respect to the charged particle medium but such a system with constant voltages cannot do work on charged particles which begin and end far away and cannot accelerate with respect to them it could still be useful to accelerate up to the velocity of the solar wind inside our heliosphere much like the electric sail if the goal is to accelerate in the dead of the ism it is essential that a time varying electrical potential be used to do work on the passing particles there are multiple ways to do this but one simple implementation illustrated in fig could feature a pusher plate made of a large grid of wires moving face on through the ism much like the proposed geometry of a standard electric sail unlike a standard electric sail however the grid of wires would actually be two identical layers of wire sandwiching a strong insulator between them to keep the two layers physically apart and electrically isolated these wire grids or tethers could be made from very fine superconducting wire and the entire ensemble could be spun to create tension and keep the wire grids extended without heavy support structure the two faces of the pusher plate would be charged and discharged cyclically in the primer portion of the operation cycle the front layer in the pusher plate is raised to a positive potential and the back layer to an equal negative potential due to edge effects of the finite plates and the self shielding behavior of plasmas this results in a decaying electric potential of opposite sign on either side of the plates ions streaming towards the front positively charged layer slow down building up an overdense clump in front of the pusher plate while an underdensity forms at the immediate location of the pusher plate then in the pull stage of the cycle the potential difference across the layers is reversed and significantly increased the ion clump that was formed in front of the plate will be attracted to the negative front layer pulling the spacecraft forward as the clump approaches the pusher plate the potential difference is turned off and the clump is allowed to coast through the plate to the other side in the final push stage the same potential difference is applied and the clump is further pushed backwards by the positive back layer of the pusher plate the clump drifts away beyond the influence of the pusher plate and the cycle repeats fig shows the electric potential and ion density at various cycle stages for a simple model by intentionally setting up clumps in the oncoming ism we can interact with it much more like in our initial thought experiment with charged pebbles the spacecraft gains momentum by giving backward momentum to the ism pushing ion clumps to the right in fig the source of the potential difference does work in the primer stage when it sets up the positive potential raising the electrical potential energy of the ions in front and again in the push stage when it raises the ion clump to a higher potential electrons encountering the potential ramps will largely be reflected but this causes only a negligible momentum drag since they are far less massive than the protons and other positively charged ions in a real three dimensional case there will also be loss of efficiency due to particles which do not interact perfectly in one dimension but instead are pushed off to the side as they pass by the charged wires furthermore this qualitative conceptual analysis does not account for the self influencing behavior of plasmas this will undoubtedly strongly affect the ion and electron distributions and the extended electric potential detailed particle in cell simulations will be necessary to investigate the optimal tuning of cycle timings electrical potentials and even geometry of the charged plates as it may be advantageous in some cases to accelerate ion clumps across a series of potential differences to gain more thrust per ion at the expense of a more complex and massive pusher plate these simulations are beyond the scope of this work but will be a critical step in transitioning the concept from a thought experiment to a practical real world device while the effectiveness and geometry of a swimmer will ultimately need to be tested thoroughly by simulation it is straightforward to represent the force on an idealized system which is able to efficiently convert electrical power p into backwards acceleration of nearby particles the resulting force fswimmer is where mp is the particle mass of order the proton mass for the ism n is the density of particles v is the velocity of the spacecraft with respect to the stationary particle frame and a is the cross sectional area over which the system can interact with particles or equivalently v x a is the volume rate of particles swept out by the spacecraft through time the positive sign is used when accelerating with respect to the the stationary particle frame and the negative sign is used when decelerating the derivation of this relationship is shown in the power referred to throughout this work is the delivered electrical power thus far the source of power for a swimmer has been ignored there is no reason a swimmer could not use an onboard power source making it totally independent of external infrastructure this of course would require an exceptionally energy dense fuel source as well as a very efficient generator to achieve useful velocities for interstellar travel beaming power remotely to the swimmer is possibly a more viable strategy for interstellar travel which invites a direct comparison to light sails in this case an additional p c term is included in eq corresponding to the photon pressure of the beamed energy being absorbed by the spacecraft the total force is then where p c is either added or subtracted depending on if the beamed energy source is coming from the origin or the destination respectively it will also be useful to consider the ratio r of the force on a swimmer to the force on an ideal light sail with equal delivered power f p c this ratio can be written as where we have used the the positive signs in the swimmer and photon forces indicating the spacecraft is accelerating and beamed energy is coming from the origin as would be the case for an initial out bound journey to another star system in fig r is shown as a function of velocity for a few values of a p there is some uncertainty surrounding the structure and properties of the local ism but there is general consensus that a journey to cen a will involve passage through some combination of the local interstellar cloud the circum heliospheric interstellar medium and the g cloud therefore a conservatively low ion density of n cm consistent with the estimated densities in these clouds see for example is used in fig fig shows the force initially rising with velocity due to the fact that at higher velocities the swimmer plates are sweeping out larger volumes of the ism faster and able to interact with more particles per second the force peaks at some velocity and then decreases since it takes more and more energy to accelerate the passing ions to yet higher velocities to get the same momentum change due to this initial rise in force with velocity it may be useful to give swimmers an initial velocity boost through other means such as conventional rockets gravitational assists or particle beam assists to take advantage of the forces at higher velocities larger a p values give significantly better performance at lower velocities but trend together as velocity increases with the force approaching p v x c v c the ratio r approaches c v v shown by the red line in fig this high velocity limit implies an order of magnitude larger force for swimmers relative to light sails up to v c or about the speed of light to illustrate the potential of swimmers for interstellar travel it is helpful to consider a possible future mission further details and technical considerations are available in the published manuscript but here we simply assume the the engineering difficulties of beaming power to interstellar distances is solved and the onboard energy converter has a specific power capacity of kw kg every kw of delivered power requires an increase of kg in the mass of the power converter equipment the ism is assumed to be uniform with a density of cm a temperature of k and therefore an electron debye length d m a relatively lower mass swimmer mission might have the goal of transporting a modest space probe mpay kg to  cen a accelerating within our heliosphere and decelerating at the destination are possible and in fact relatively energy efficient but discussion of these will be left for the main publication for brevity a modest electrical power delivered to the swimmer of mw is assumed the pusher plate will be made up of several long tethers in practice these tethers will consist of very fine braided filaments to prevent failure due to micrometeoroid and interstellar dust collision as described for the electric sail from a material mass standpoint these are considered to be single wires with an effective diameter of m this is equivalent in material to eight filaments with diameters of about m given the pulsed nature of the swimmer electric field the wire tethers should be made out of superconducting materials a single charged wire will interact with charged particles passing within about d on either side of it the total cross sectional interaction area is given by where l is the summed length of all the tethers this cross sectional interaction area is somewhat of an idealization as the debye length does not represent a hard cut off where particles suddenly cease to be effected by an electric field and in regions where tethers intersect part of their cross sectional areas will overlap nonetheless it is a sufficient estimate for our rough calculations the mass devoted to this pusher plate will be mpusher  x l x  rwire where rwire represents the effective radius of the wire tether m in our case and  represents the density of the tether material which we will take to be kg m the density of the popular superconducting material magnesium diboride the total mass of the swimmer ship is comprised of mpay kg mpower kg given by the mw supplied electric power and a kw kg specific power and mpusher we will take kg as the mass of the pusher plate which provides for a total summed tether length of x m while this is seemingly a very long tether it does not in any way represent the spatial scale of the swimmer as the pusher plate will be made up of several thousand tethers possibly splitting off from each other at greater radial distances the summed length is merely a useful value for determining the total cross sectional area in plasmas of different temperatures and densities in this case from eq our swimmer tether length corresponds to a cross sectional interaction area of km about the size of uruguay or the state of washington we will begin our voyage as the swimmer enters the ism at au with a velocity of x m s the speed of light and consistent with typical velocities of the solar wind iteratively integrating using eq we find that after just less than years the spacecraft will be on the doorstep of alpha centauri after having travelled one parsec and achieving a final velocity of the speed of light including time to initially accelerate from rest within the heliosphere and decelerate at the destination marginally increases the trip time but we could also shorten the trip slightly by systematically shedding mass and reducing the size of the pusher plate enroute as fig shows at higher velocities larger plate areas provide diminishing returns so as the spacecraft reaches higher velocities the larger area of the pusher plate becomes dead weight a total journey of about years starting from rest in the solar system to being gravitationally captured by cen a is reasonable for the overall journey while years is a significant amount of time for a scientific endeavor there is good precedent for multi century science projects for worthwhile investigations c f furthermore the energy expense is a pittance compared to an equivalent mission using laser pushed light sails an equivalent kg probe pushed by mw of laser light incident on ideal light sails and starting with a velocity x m s would take about years to travel parsec and reach a final velocity of the speed of light to reduce the light sail travel time to years would require an average power consumption of nearly mw x higher swimmers represent a new mode of interstellar transport by disposing of onboard reaction mass they circumvent the rocket equation and by exchanging momentum with ions in the ism they improve by orders of magnitude over the energy efficiency of traditional light sails at relatively low velocities the key to this momentum exchange is the time varying electric field which allows swimmers to create inhomogeneities in the surrounding plasma and then push on these inhomogeneities to create thrust swimmers perform exceptionally well at lower velocities with their advantage over light sails diminishing quickly at v c furthermore by relying on the ambient ism as a momentum exchange medium they are quite versatile able to accelerate either away or towards a beamed energy source opening up myriad opportunities to serve as one way transport roundtrips or even statites remaining in stationary positions with respect to the sun and serving as useful waypoints with infrastructure for other potential space transportation networks the example discussed here only scratches the surface of the possible roles for swimmers in our spacefaring future their characteristics make them ideal for any mission with large masses in which relatively low velocities are acceptable they are unlikely to be the sole mode of space transport due to their diminishing advantages at high velocities and their structural complexity which requires onboard power conversion systems with significant mass they can play the role of the proverbial mack trucks of space transporting goods slowly and reliably at a low energy cost while more time sensitive cargo can make use of fast yet inefficient light sails the ferraris of space swimmers might for instance be well suited to aiding the construction of a fast interstellar highway by transporting massive particle beam stations along with their fuel supply out to stationary positions between us and our target destinations these particle beam stations could be used to swiftly carry low mass magsails along the path or augment the power of future swimmers by replacing the stationary ism with a corridor of fast moving beamed particles the mission analyzed here regards a one way interstellar trip while it does push the limits of current technology by assuming relatively high specific power electrical systems very thin mass produced super conducting wire and low mass electrical insulators which can resist large potential differences as well as very large laser array optics which are addressed in other works regarding light sails there is no obvious material or theoretical limits which would prevent such missions from realization future work in this vein will need to examine several issues ignored here areas of further investigation include the efficiency of the swimmer drive in three dimensions the electrical potential and cycle timings during the pulsed swimmer operation and how they effect the required current density of the tethers the expected impact of interstellar dust collisions and redundant tether configurations to avoid catastrophic damage from tether breakage and realistic limits on power conversion system capabilities as our understanding of interstellar travel develops we must face the realization that not only is it difficult but there is no one size fits all solution where swimmers excel in one metric other methods may excel in another ultimately our best strategy is to develop all possible methods in the hope that their synergy will provide a means to accomplish our goals d brisbin spacecraft with interstellar medium momentum exchange reactions the potential and limitations of propellantless interstellar travel jbis pp r l forward roundtrip interstellar travel using laser pushed lightsails j spacecraft and rockets pp p lubin a roadmap to interstellar flight jbis pp n perakis l e schrenk j gutsmiedl a kroop m j losekamm project dragonfly a feasibility study of interstellar travel using laser powered light sail propulsion acta astronautica pp r heller and m hippke deceleration of high velocity interstellar photon sails into bound orbits at centauri astrophysical journal letters pp l d spieth and r m zubrin ultra thin solar sails for interstellar travel phase i final report nasa institute for advanced concepts pioneer astronautics inc d g andrews and r m zubrin magnetic sails and interstellar travel jbis pp g a landis interstellar flight by particle beam in aip conference proceedings vol pp d g andrews interstellar transportation using today s physics conference proceedings american institute of aeronautics and astronautics p janhunen electric sail for spacecraft propulsion j of propulsion and power pp r zubrin dipole drive for space propulsion jbis pp r zubrin the dipole drive a new concept in space propulsion th international astronautical congress iac washington dc october stilfehler technique of strand braiding wikimedia commons file licensed for sharing and adaptation https commons wikimedia org wiki file _strand_braiding png last accessed on march cropped and edited for d effect i a crawford project icarus a review of local interstellar medium properties of relevance for space missions to the nearest stars acta astronautica pp a kivilaan and r s bandurski the one hundred year period for dr beal s seed viability experiment american journal of botany pp r johnston world s slowest moving drop caught on camera at last nature news c cockell the year microbiology experiment microbiology today pp may this is a fascinating idea for a star drive but it would benefit us to bring it down to earth can you make a device with brushes to inject small amounts of alternately positive and negative ions into moving air when then pushes on them with waves of alternating positive and negative voltage a smarter ionolifter in other words i have daydreamed of it as a means of aircraft propulsion but more importantly for replacing the appalling drone of the air blowers that appropriate the top floor and a half of modern buildings and the ears of all their neighbors beyond all sense or proportion there is another technology that we have had that will get us to that to of the speed of light project orion i was going through some old company documents about a dispute over depletion rates of mineral with the irs that was big in the fifties so was an idea by the pentagon called project orion in these documents is a wall street journal because of a front page tax article but on page is pentagon to review proposal to develop rocket propelled by small atomic blasts it was to cost billions then and be used to lift massive objects into orbit there has been a readability study by general dynamics funded by arpa they were going to do a follow up because there were some issues those issues were the fall of from earth launches gd came up with a rough design of course now if used for interstellar we could launch it from day the moon and avoid the main issue there was also a project pluto which was to design a nuclear powered craft the name of this project was orion of course other details were classified the date was wednesday july a few obvious issues that i see with the idea the ism has a distribution of particle sizes and charges just like the use of a mass spectrometer to separate charged molecular fragments by time of flight the electric fields of this device will pull and push the particles it interacts with a possibly too wide dispersion in space to allow this pulse engine approach to work iow rather than getting a reasonably tight distribution of charged particles as envisaged teh distribution will be far wider at the very least this would reduce the thrust power of the propuslion system the beamed power from the laser will still be subject to dispersion over interstellar distances being particular critical when power is needed to decelerate the craft this means that the laser power will need to be increased to compensate perhaps completely negating any thrust power advantage over photon sails that will get most of their beamed energy closer to the home system and coast much of the way to the target star system a minor point the reference design of km means a radius of km is the design deployable as a spinning device will any uneveness in the mass and distribution of the accelerated ions across the surface cause problems for the stability of the design what i would like to see with this idea as well as other theoretical ideas propped up by idealized equations is some experiments with small test devices a small mesh in a vacuum tube with charged ions being fired at the mesh should be able to demonstrate several features of the theoretical model do the ions bunch up in the priming phase do the ions leave the device with a higher net velocity than they started with when emitted it is the sort of experiment that an engineering or physics lab could set up and be run by students who have the time to test the effects of adjusting the electric field cycle rate vs the incoming ion stream velocity distribution once the theory is confirmed only then should effort be made to create a small test device and test it in space e g facing the incoming solar wind obviously a very very complicated subject but it seems to boil down to the concept of living off the land which is a highly desirable state of affairs from the standpoint of needing and utilizing whatever s available for your propulsion needs this reminds me of james woodward s epiphany that arose as a teenager when he watched the movie forbidden planet and he became motivated to devise a propulsion system which dependent upon back reaction from as he called it machs principal utilizing space time as a type of reaction mass i did not read the paper yet but i intend to as it is fairly complex but the immediate sense i get from scanning it is it utilizes the interstellar wind to catch a type of sail as a propulsive mechanism leave a comment name email website comment previous post amateur astronomers join hunt for exoplanets charter now reading recent posts on comments enter your email address to receive blog posts from centauri dreams via email delivered by feedburner advanced propulsion research exoplanet projects earth exoplanet projects space further astronomical and astronautical resources weblogs discussions commentaries archives meta this site rocks the classic responsive modded skin for thesis wp admin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************************************************************\n",
      "concept 10\n",
      "in addition to enhancing messaging on android with rich communication services rcs and bringing you helpful features with messages we also want to provide a safer messaging experience today we have two new updates to share on that front trustworthy business messages with verified sms sms messages help businesses share useful information with consumers things like one time passwords account alerts or appointment confirmations yet sometimes it can be difficult to trust the identity of these messages which are often sent from a random number some messages may even come from bad actors pretending to be from businesses you trust and ask for private information or link to dangerous websites this is called phishing verified sms for messages rolling out today in a number of countries will help you confirm the true identity of the business that is texting you the feature works by verifying on a per message basis that content is sent by a specific business when a message is verified which is done without sending your messages to google you ll see the business name and logo as well as a verification badge in the message thread verified sms is rolling out gradually on messages in nine countries starting in the u s india mexico brazil the u k france philippines spain and canada with more to come verified sms is just one of our efforts to improve your messages with businesses we also continue to work on enhancing the chats you have on messages with rich business messaging rbm real time spam detection in addition to verifying the businesses sending you messages we are working on protecting you from spam in messages spam protection which works with your message data while keeping your messages private has been available over the past year in a number of countries and is now rolling out broadly in the u s follow us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "google is seeking to address the growing problem of short messaging service sms phishing or smishing by allowing business senders to enroll in a verification service for texts text messages are often used for out of band authentication account alerts and appointments but it can be difficult to for recipients to verify the identity of the senders smishers can also use deceptive links in messages to trick customers to visit malicious sites that can compromise their devices and information on them now businesses can enroll in google s verified sms program allowing them to add their logos to messages messages will also show up with a verified sender badge and provide link previews for users verified sms requires google s messages client for android that supports the rich communications services rcs standard messages with verification do not have to go through google s servers however businesses in the united states india mexico brazil united kingdom france philippines spain and canada will be the first to get verified sms google said google did not reveal the cost of verified sms which is already being used by its indian payments service and three financial institutions worldwide as well as large flower delivery company and travel search engine kayak along with verified sms google said it had added spam detection for messages but only in the us market spam detetion in messages lets users block and report spammers and google said it will not see or store the content of the texts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "there is a special feeling i get on a sunday evening sunday is the weekend and i am home relaxing with my family after a well deserved break but sunday evening is the end of the weekend the end of that pleasant break i have to go back to work the next day this is the time i like to sit by the kitchen table by myself thinking about a better future one where i do not have to obey the traditional rules of work i do not want to put my life on pause on a weekly basis and dedicate the time to my employer for a modest fee instead i want to get up on a monday morning and go for a hike in the beautiful mountains of california i want to sit by the beach people watch breathe the ocean air on this particular sunday evening of september i was burnt out my job had beaten me to a pulp i could not tell exactly why i was exhausted at work it is not like i was doing anything physically taxing yet i found myself increasingly grinding my teeth i logged onto my computer and did something i never thought i was capable of doing i asked for help i went on to hacker news and started a thread i did not know what to say how to say it but my fingers typed on the keyboard therapeutically until the issue revealed itself on the screen this is what i wrote we may get fired and i do not know what to do i worked as a full stack developer and i had become comfortable with my job i was the go to guy i knew the ins and outs of all the projects in my department and then some in a company of a thousand everyone knew me by name i have created dozens of tools that are still used in the company to this day one day my manager called me for an impromptu one on one it was unusual since we already had our one on one that week when the door closed behind me and i sat in the closet sized room aptly named claw sets i thought i was in trouble she did not mince her words i am leaving next week we were a very close team and we knew each other well she had just lost a family member the decision could have been related i did not want to pry too much i accepted it so she left a month later it was like deja vu when the lead developer called me in the same closet and repeated the process he left the company hell broke loose it was complete chaos the business team ran right and left pulling their hair out trying to figure out what to do our department was the most profitable at the time and it was without leaders the company scrambled their resources to get them replaced as quick as possible my team and i voted for the most senior developer to take on the role of interim lead it was our secret ploy to get her to become the new lead in no time we got the situation back in control while she was a great lead we saved the managerial position for someone who was more suited for the job someone who could talk engineer and business all at the same time someone who has been groomed for the job a leader everyone knew and trusted so to say our ploy was foiled when the vp of tech promoted a manager into our team this one came from a department that was now defunct rumors were that he burnt it to the ground when he joined the team he wasted no time and brought in an outsider as the new lead the new manager and his new lead immediately took the department into a new direction a new era less than a month into his tenure the new lead called for a gathering he called it a power up meeting here he presented the new product he wanted us to work on at first i thought it was an honest mistake from a newcomer after all he had not had a chance to learn what we currently did when he finished his speech i raised my hand i told him that the project he had described did exist in fact it was the exact tool we had been building and improving for over two years he did not take it very well the meeting ended and over the next few weeks i started noticing something odd developers were dropping left and right out of our main projects a bunch of tickets where left unsolved and it fell to me to fix them when i asked the manager about it he literally ran away and then i started getting meeting requests from my disappearing coworkers one by one they started complaining to me they had been assigned to the new project i do not really have much power here i would tell them i do not really know what i can do to get you back on your projects but i will try i was assuming a position of leadership i was very popular yet i had no power my coworkers started requesting more and more time with me to complain about the new system it took a couple months for the business team to notice that there had been a sharp decline in revenue that is when i realized that this project was done in complete secrecy all eyes turned to me to do something what was i supposed to do i was the developer on the th floor that you call for technical issues that was about it my god we are all gonna lose our jobs someone said i did not want that to happen when no one was watching i sneaked out and went to hangout on the th floor where the business team was i accidentally bumped into the director of our department hey i have some insights about improving the shopping department let me know when you have a moment and we can discuss this yeah he replied we have a meeting with the managers after lunch why do not you join us i got myself a place at the table i told no one else in my team and went to attend the meeting after lunch my manager almost gasped when he saw me enter the room he was quick to interrupt me every time i started to talk at some point i stopped talking and showed everyone what the issue was i plugged my computer to the big screen and showed them one of the websites that was losing money on a particular page a form that was only supposed to be accessible to editors was available to all logged in user due to a bug with roles and permissions the text field was only hidden with css instead of being removed from the page a user noticed it and started spamming the entire website with inappropriate keywords google flagged the entire website as a result sinking our revenue i told the business team that it could be fixed in a matter of minutes the problem is that the current developer working on it is swamped with other tasks even though i think this should be a priority the manager found a way to turn this around dismissing me entirely our rate of decline is no longer negative he said he shared his screen where he showed a chart that showed our rate of decline on the left it was steep plunging head first into the abyss but then it curved and turned almost into a line parallel to the x axis now it is stable he promised that it was only a matter of time until it starts growing again adding an upward dotted projection into the future there is a disconnect between software development and business that we will hopefully address with our new lead he concluded i was never invited to a meeting again maybe it was me maybe i was not as competent at my job as i thought maybe i had received one too many compliment and my ego overflowed i asked my coworkers to send me the specs of the new project maybe i was not seeing the big picture there are no specs or documentation it is all in his head he said that way we do not get attached to things that do not work a coworker told me to work on the project the lead would come to your desk take over the keyboard create the necessary files then you would work on it i was not alone everyone thought he was crazy that sunday evening i turned to the internet and asked what to do at first i wrote out of rage but then i realized that it could backfire if i was identified so i removed things here and there then added a few things to throw off the reader i ended the message with these words maybe it is this ugly weather here in london that is causing me to rant but i had to share my frustration i am stuck and hn i need your advice the internet responded i really would not talk to upper management if the guy got moved to a profitable department it means he has friends there you are gonna lose and risk a bad reference which would make it difficult to get a new job this was insightful you can either resign silently or step up and go talk honestly with higher management with the risk of getting fired in any case you should not continue working like that this also made sense resign if you can this seems to me an organization with a toxic ecosystem that is neither healthy for you or your co workers the comments kept coming and it was becoming overwhelming an incompetent manager is moved to the most profitable division this indicates that the individual is highly connected in the company it was like people knew exactly what i was talking about the weather in london is not ugly right now just for the record he was right i should have googled that a day job is not supposed to be this political and stressful get the hell out of there that is how i felt while its personally satisfying to be a hero it will just get you fired that is what i feared stop being a lamb speak up or find a new job leave now i closed off my laptop and went to sleep monday morning i arrived at work sat on my chair and a message popped on my screen it was one of my coworkers the message had a link to the hacker news thread and these words lol that sounds just like us to which i replied lol totally does i read all the comments on the thread and made a decision if this was normal why would they be building it in secret i collected all the evidence i could get to prove my point and scheduled a meeting with the vp of tech every three months i met with him for a regular evaluation i knew him well enough and i thought he would be the right person to talk to who had influence on our department they are building it in total secret no one in the business team is even aware it exists the project is called hydra because of the many heads you know i said really in secret you say that is weird well thank you for bringing it to my attention he stood up and shook my hand and i left his office heading back to my desk i noticed that he was right behind me walking in the same direction we arrived together to our department with a whistle and a flick of the finger he called both the manager and the lead to his office they both got up and followed him i sent a message to each of my teammates telling them that i told the vp about our plight he will do something about it i typed not fifteen minutes later we heard people laughing hysterically in the aisles all three of them the vp of tech the manager and the lead they walked into our department and started talking out loud and laughing for a good five minutes there was only one thing in my mind i have made a huge mistake a minute later a coworker sent me a facebook link i was afraid to click on it as if i knew what was going to appear on my screen i clicked anyway and saw a picture there were people in it the vp the manager the lead and the cto casually partying at a bar a huge mistake i had denounced the two bandits to the leader of the pack nothing changed when it came to work but everything had changed there was a cold silence every time i was in the room i was asked to report the number of hours i worked and what i worked on any additional minute at lunch was scrutinized in the past there was no set time on when to come to work as long as you did your work now there was a strict am start of day a few other coworkers were caught in the fight what they did was come to me ask me if there was something i could do there was nothing i could do the whole team was now working on the new project full time everyone except me people started quitting next to our department was another hip group i was familiar with i often consulted with them for their front end needs their manager often joked that he would steal me from my team so i got up from my chair walked up to him and said so you guys have space for another developer i left the team i had spent years there learning growing making friends now i walked to the next row where they could still see me and abandoned them to my surprise i was still getting meeting requests not long after i left the company in the new team i still had to deal with the vp it was not comfortable anymore i hated it i left so i could wake up on a monday morning and drive on the pacific highway and hike in the beautiful mountains i moved on i felt that since the first manager and lead left there was no more room for me to grow in fact i found that the two had quit and secretly started a competing service the relationships i made in this company refused to let go of me i still am in constant contact with my ex coworkers some are at different jobs now but others are still there we still go to lunch i still recommend new graduates to my recruiter in fact i was invited some years ago for lunch in the building right at my old desk the elevator door opened and who was there to great me the manager and the lead oh my god what are you doing here the lead asked i joked around keeping no grudges about the past i had a good time reconnected with old buddies and scared the interns but the two bandits still held a grudge against me a couple days after my visit a coworker updated me that a new memo was in circulation it said employees are no longer allowed to invite outsiders into the facility they would first need written permission from a manager coincidence i do not think so i hold no grudge the department is now defunct they lost so much money and were never able to recover the project hydra was a complete failure other departments rose the company changed its business model and was bought for billion dollars by a private firm and then there was a restructuring both the manager and his lead have been fired just kidding they were not fired they were promoted the manager is now the vice president the lead now leads in the most profitable department the old vp is now chief product officer it is a happy ending i do not know if there is a lesson here but if you are bad at your job it pays to have good connections hello there if you like this story it is part my of unfinished book just fired in a country where your job becomes your identity what do you do when you get fired follow me through my journey of discovery in article padding px in article card border radius px box shadow px px rgba display flex background color fafafa text decoration none overflow hidden font family arial max width px margin auto transition background color s box shadow s in article card hover box shadow px px rgba background color fff in article card logo color fff background color ff text transform uppercase padding px font size px text align center in article card text padding px color in article card text span color did you like this article you can subscribe to read more awesome ones share your insightful comments here sign up for the newsletter waking up if you are like me it is one of the most difficult task of the day everyday i go to bed late thinking it will not be so hard to wake up the next morning but everyday i struggle when the first light of the day enters my bedroom it is only an excuse for me to tuck myself deeper into the blanket i cannot wake up and the apparent solution is the alarm clock if anything is clear since i wrote the last article about the machine firing me it is that most people wanted to read more they cruised my blog trying to find more to read but i seldom tell stories instead i talk about javascript cdns and php things that are more practical then interesting it seems accidental that i even wrote a story this time but it is not i have been working on story like format with more failures than successes introducing an exclusive extract of just fired my book currently in progress the million dollar job is part of a completed chapter feel free to subscribe to stay informed maged qwani a day ago thank you for your great article and experience although i work in a government job guaranteed for life in my country its the same experience over and over a well connected guy manage an active department with real results and services and burn it to the ground im curious about the tools wish if you can share them or link to your github ibrahim diallo a day ago hi maged thank you for reading unfortunately these tools are internal tools that ties many systems together although they could be used publicly they are now property of the organization skyler lewis a day ago that situation just sucked sorry you went through that is getting shoved out politically better or worse than getting fired by a machine ibrahima diallo a day ago skyler hahah well when the outcome is the same it is not that much different scorerror a day ago thanks for sharing it sucks i try to think there is no bad experience and any experience is needed and only pushes you forward i am glad you are out of that toxic place i am going through something more or less similar and i guess the best thing to do is just let it go and move on to something else i work in a small startup where i am probably one of the youngest employees and the least experienced one too i get anxious at times from how crappy the situation is and i it messes with my mental health as well it is always hard to let go when you care but seems like it is the right thing to do ibrahima diallo a day ago thanks scoreerror at the time of course it was hard to move on but over time there is so much to learn from a situation like this i can see the parts where i could have done better having conflicts at works makes you better prepared for the politics of a job sean a day ago at a certain point when you asked yourself why you continue to work in this toxic environment i assume the will to move on far outweighed the guilt of abandoning your co workers and projects but the way you handled the situation to me comes off as timid and uncalculated had you done your research on your upper management just as your qa department likely did you would have had a significantly better understanding of the social dynamics at play though you at least stood up for what you thought best for the team it ended up costing everyone instead if you had researched the social dynamics between the upper management you could have had real candid discussion with them what happens if the upper management tells you you have no authority then it is game on this is where you absolutely need to keep your frame keep your reality stronger than theirs do not back down and call out the situation do not point fingers at people but just talk about the problem with conviction just as you did in your first business meeting if that does not work then you missed something most likely a communication gap exists schedule time to speak to your management about the problems you are experiencing open up on your end and open a dialogue start communicating if i had to guess doing these things would have significantly helped you but i was not there and making assumptions really only serves to give us piece of mind in my experience a large portion of businesses fail because of a lack of communication and empathy sound like something we are already familiar with relationships next time you find yourself shoehorned in a social circle open a dialogue and listen cheers ibrahim a day ago thanks for your input sean yes i could have prepared better i may have left out hints that this occurred over a period of months there had been many talks many clashes that were too long for a blog post in the book i have expanded on the details but it does not change the fact that at the time at least i thought that this was my best course of action i learned loki a day ago do not concern about the company profit you are not the owner just do things make the boss happy and get paid hater friend a day ago this is life but bad karma will get back to them your future success will far exceed theirs ibrahim a day ago loki in those days i took ownership of everything i worked on it is only through experience that i realized that my concerns and those of my employer are not necessarily the same hater friend amen tim a day ago this is why i became a freelancer but to hear the phrase i hold no grudges irks me because i have been in these situations and the untold stress wasted time and in the end lost jobs and potentially ruined lives yes corporations often ruin lives is so great that we should truly find ways to root out such incompetence and nefarious corporate protocols for dozens or perhaps hundreds of people in your company having to suffer so the lead manager could glorify their egos is simply insane the more people to say eff this we are going above this guy and telling the truth about what is happening rather than looking for another guy like you with the most competence or connections to stand up for them then the more risk these corporate connected bullies would have in implementing such styles of work i hold plenty of grudges for people like that and i do so so that when i encounter them again as i sometimes due even in my freelance world i can quickly call them out and let them know what they are doing and that i will have none of it on a project i am working on ibrahim a day ago tim i definitely understand your point and at the time it was painful for me especially when i firmly believed i was doing the right thing but what i failed to understand at the time was that this was not a good versus evil situation it was a political game i saw the technical issues that we were up against in recreating a system from scratch while they saw an opportunity to create something that will promote them into their next role yes it was selfish and greedy and affected real people today i am better off having gone through this experience i learned it the hard way mark hours ago that is a vivid recap of your site situation and stuff i have experienced at a lesser level however the manager and the lead must be good at stuff in the eyes of others aside from bullshit politics and networking what is that stuff if you had to play devil s advocate and what was their background did they have strong technology experiences including academic joethefunspoiler hours ago i wonder how weird people that find this soup opera techno articles of any interest i mean first of all wow you scrolled all the way down that means you want to know more about me well here is a summary of who i am and what i do i started this blog because wait i have a link for that too hey have you heard about humans txt well i kinda liked the idea and did my own version you can find me on do not hesitate to say hi it is what keeps me going designed by yours truly copyright \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "dec attorney general bill barr is having an ongoing public debate with facebook over whether it is in the public interest for platform companies to be unable to read messages sent over their platform both players are missing two points yes we have laws enforcing atomic secrecy yes we had laws restricting the export of high grade encryption this is not that the technologies we are talking about have been open source for decades is the ag really talking about banning software in constant use on billions of devices even if he makes that the law and good luck both congress and the courts are friendlier to consumer privacy as the culture changes how would you ever prevent normal people from sharing encryption software with each other you would have to inspect every packet on the internet to make sure it was not using strong encryption keys you would have to ban every kind of decentralized network you would essentially have to ban any communication that is not subject to automatic monitoring by the government you would essentially have to ban privacy you would have to ban the open source movement maybe one might say you can create a distinction between encryption enabled by default by a platform vs encryption deployed by an expert user or corporation to protect their internal communication too many funny edge cases here do you then have to ban encryption plugins do you have to ban alternative chat clients that send encrypted messages over the api you would have to lock down too much stuff to draw this distinction if an individual consumer sends an encrypted message over a platform have they broken the law people in hong kong are using mesh networks every group of people who are subject to tracking and censorship will switch to decentralized technology it is a no brainer why does privacy matter if you live in hong kong is an easy question to answer why does privacy matter if you are not planning to break the law is a harder question to answer or it was until the last decade or so the answer now is that platforms and institutions have various laws that punish their users without much due process this is things like taking down user content blocking their access to the platform refusing refunds shadow banning also consider platforms behavior around competition amazon treats their rd party sellers as a business idea farm facebook has a line in their developer tos saying that they will probably use your stats to compete with you nike just dropped amazon because of shenanigans like this and also bootleg products tons of companies are using consumer demographic data for price discrimination tinder got in trouble for this in california and are probably still doing it in other states also there are so many data breaches by people who in a sane world would have a fiduciary obligation to protect what they collect these breaches directly expose consumers to the risk of crime the argument for privacy against the government has always been corruption tyranny protection against future laws and some people are not willing to debate those points because they view the government as axiomatically trustworthy but the argument for privacy against platforms and private institutions is very real and non hypothetical it requires zero imagination every platform user is living in an authoritarian state if you believe that platforms are state like entities legislators who do not get the importance of consumer privacy are on the wrong side of history and will lose their seats even the aarp crowd is super sensitive to fraud online they are a major target i am a nyc based scalability dev building a progress bar company https cloudprogress io\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#topic wise printing documents\n",
    "for idx,i in enumerate(u.columns):\n",
    "    con=zip(list(u['body']),list(u[i]))\n",
    "    sortedterms=sorted(con,key=lambda x: x[1],reverse=True)[0:4]\n",
    "    print('*'*120)\n",
    "    print('concept',idx)\n",
    "    for j in sortedterms:\n",
    "        print(j[0])\n",
    "        print('\\n')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
